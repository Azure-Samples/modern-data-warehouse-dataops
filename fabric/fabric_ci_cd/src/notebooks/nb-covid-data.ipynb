{"cells":[{"cell_type":"markdown","id":"58c809e3-b108-4671-84cb-837e00a4fceb","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## About this Notebook\n","\n","- This notebook performs a sample ETL operation using Microsoft Open datasets (Covid data).\n","- It **doesn't require** a default lakehouse attached and instead uses absolute paths to create/load managed tables.\n","- This notebook is part of a data pipeline.\n","- Can be run from any Fabric workspace as long as proper access to provided to write to the targets(workspace and lakehouse).\n"]},{"cell_type":"markdown","id":"6cac6841-be8c-4775-b3a4-b06d38fdf21b","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Libraries"]},{"cell_type":"code","execution_count":null,"id":"62ff373c-9871-473e-98fa-d26d66523910","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-03-21T02:12:16.8768105Z","execution_start_time":"2024-03-21T02:12:16.5632896Z","livy_statement_state":"available","parent_msg_id":"31679cda-72e7-4dc1-a98b-d37225391687","queued_time":"2024-03-21T02:12:16.1921654Z","session_id":"3e181d44-f0b6-406c-a2a1-e7eaf98267e8","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":13},"text/plain":["StatementMeta(, 3e181d44-f0b6-406c-a2a1-e7eaf98267e8, 13, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["import json\n","from pyspark.sql.functions import lit, to_utc_timestamp, unix_timestamp, avg, max, min, sum, count\n","from delta.tables import DeltaTable\n","from typing import Optional\n","from datetime import datetime, timezone"]},{"cell_type":"markdown","id":"8a4fa16b-7c9d-4068-8a09-5154f4189003","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## External parameters"]},{"cell_type":"code","execution_count":null,"id":"4dddcb9d-5004-4e91-9c0c-e765a1885f26","metadata":{"tags":["parameters"]},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-03-21T02:12:20.9770233Z","execution_start_time":"2024-03-21T02:12:20.6608936Z","livy_statement_state":"available","parent_msg_id":"ab08eeea-1eac-4824-954a-409a60732fec","queued_time":"2024-03-21T02:12:20.2724308Z","session_id":"3e181d44-f0b6-406c-a2a1-e7eaf98267e8","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":14},"text/plain":["StatementMeta(, 3e181d44-f0b6-406c-a2a1-e7eaf98267e8, 14, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["# Keep Only External parameters in this cell.\n","onelake_name = \"onelake\"\n","workspace_name = \"ws-fabric-cicd-dev\"\n","lakehouse_name = \"lh_main\""]},{"cell_type":"code","execution_count":null,"id":"c8c61585-8dd7-416a-bbac-604278b24219","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["print(f\"{onelake_name = }\")\n","print(f\"{workspace_name = }\")\n","print(f\"{lakehouse_name = }\")"]},{"cell_type":"markdown","id":"9713b413-1dc0-4689-b0a5-f0dad51e6fb4","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Local parameters"]},{"cell_type":"code","execution_count":null,"id":"f578b082-b72e-4951-8d9d-06e6384c50c9","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Complete paths - This way we are independent of local workspace - we can connect to any workspace and any lake house as long we have the proper access. \n","lakehouse_folder_name = f\"covid_data\"\n","\n","print(f\"{workspace_name = }\")\n","\n","\n","# Microsoft Open dataset - Safety data - Ref: https://learn.microsoft.com/en-us/azure/open-datasets/dataset-new-york-city-safety?tabs=pyspark\n","# Azure storage access info  \n","blob_account_name = \"pandemicdatalake\"\n","blob_container_name = \"public\"\n","blob_relative_path = \"curated/covid-19/\"\n","blob_sas_token = r\"\"\n","covid_data_sources = (\n","    (\"ECDC\", \"ecdc_cases/latest/ecdc_cases.parquet\"), \n","    (\"bing\", \"bing_covid-19_data/latest/bing_covid-19_data.parquet\"),\n","    (\"oxford\", \"covid_policy_tracker/latest/covid_policy_tracker.parquet\")  \n",")\n","\n","onelake_path = f\"abfss://{workspace_name}@{onelake_name}.dfs.fabric.microsoft.com/{lakehouse_name}.lakehouse\"\n","onelake_file_path = f\"{onelake_path}/Files\"\n","onelake_table_path = f\"{onelake_path}/Tables\"\n","\n","# Allow Spark remote read\n","wasbs_path = f\"wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/{blob_relative_path}\"\n","spark.conf.set( f\"fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net\", blob_sas_token)"]},{"cell_type":"markdown","id":"4d5307ad-74c5-4355-a207-01ea666f4208","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Check source and target locations"]},{"cell_type":"code","execution_count":null,"id":"932b39c2-1171-4b80-aa1a-f8e39d75f8af","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Check onelake existence - otherwise abort notebook execution\n","error_message = f\"Specfied lakehouse table path {onelake_file_path} doesn't exist. Ensure onelake={onelake_name}, workspace={workspace_name} and lakehouse={lakehouse_name} exist.\"\n","try:\n","    if not(mssparkutils.fs.exists(onelake_file_path)):\n","        raise ValueError(\"Encountered error while checking for Lakehouse table path specified.\")\n","except Exception as e:\n","    print(f\"Error message: {e}\")\n","    # no further execution but Session is still active\n","    mssparkutils.notebook.exit(error_message)\n","else:\n","    print(f\"Target folder path: {onelake_file_path} is valid and exists.\")\n","    print(\"Listing source data contents to check connectivity\")\n","    print(mssparkutils.fs.ls(wasbs_path))"]},{"cell_type":"markdown","id":"10221997-c1c8-4a52-afb4-e035207241d0","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Copy data from remote source to OneLake"]},{"cell_type":"code","execution_count":null,"id":"62e529d2-b5ee-4313-a03f-660efb327fce","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["current_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n","current_month = datetime.now(timezone.utc).strftime(\"%Y-%m\")\n","\n","\n","# Optionally delete existing contents\n","file_path_prefix = f\"{onelake_file_path}/raw_covid_data\"\n","\n","for source, source_path in covid_data_sources:\n","    source_file = f\"{wasbs_path}/{source_path}\"\n","    target_file = f\"{file_path_prefix}/{source}/{current_month}/{current_date}.parquet\"\n","    print(f\"Copying {source_file = } to {target_file =} for {source = }.\")\n","    # This is overwrite operation - and is okay as there is only one latest file for any given day.\n","    mssparkutils.fs.cp(source_file, target_file)\n","\n","print(f\"\\n=====\\nCovid data is copied to {file_path_prefix} for {current_date =}.\\n=====\")"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
