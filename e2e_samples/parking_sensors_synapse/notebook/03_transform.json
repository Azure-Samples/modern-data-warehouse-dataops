{
	"name": "03_transform",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "test",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "2eefb360-05b3-4799-a18e-356340b023d2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/b5b65b8b-b877-4089-8371-ebf7bf250451/resourceGroups/mdwdops-77-dev-rg/providers/Microsoft.Synapse/workspaces/sywsdev77/bigDataPools/test",
				"name": "test",
				"type": "Spark",
				"endpoint": "https://sywsdev77.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/test",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 1. Get dynamic pipeline parameters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true,
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# Get pipeline name\n",
					"pipelinename = 'pipeline_name'\n",
					"\n",
					"# Get pipeline run id\n",
					"loadid = ''\n",
					"\n",
					"# Get keyvault linked service name\n",
					"keyvaultlsname = 'Ls_KeyVault_01'"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 2. Transform and load Dimension tables\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import datetime\n",
					"import os\n",
					"from pyspark.sql.functions import col, lit\n",
					"import ddo_transform.transform as t\n",
					"import ddo_transform.util as util\n",
					"\n",
					"load_id = loadid\n",
					"loaded_on = datetime.datetime.now()\n",
					"\n",
					"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
					"\n",
					"# Primary storage info \n",
					"account_name = token_library.getSecretWithLS(keyvaultlsname,\"datalakeaccountname\")\n",
					"container_name = 'datalake' # fill in your container name \n",
					"relative_path = 'data/dw/' # fill in your relative folder path \n",
					"\n",
					"base_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
					"\n",
					"# Read interim cleansed data\n",
					"parkingbay_sdf = spark.read.table(\"interim.parking_bay\").filter(col('load_id') == lit(load_id))\n",
					"sensordata_sdf = spark.read.table(\"interim.sensor\").filter(col('load_id') == lit(load_id))\n",
					"\n",
					"# Read existing Dimensions\n",
					"dim_parkingbay_sdf = spark.read.table(\"dw.dim_parking_bay\")\n",
					"dim_location_sdf = spark.read.table(\"dw.dim_location\")\n",
					"dim_st_marker = spark.read.table(\"dw.dim_st_marker\")\n",
					"\n",
					"# Transform\n",
					"new_dim_parkingbay_sdf = t.process_dim_parking_bay(parkingbay_sdf, dim_parkingbay_sdf, load_id, loaded_on).cache()\n",
					"new_dim_location_sdf = t.process_dim_location(sensordata_sdf, dim_location_sdf, load_id, loaded_on).cache()\n",
					"new_dim_st_marker_sdf = t.process_dim_st_marker(sensordata_sdf, dim_st_marker, load_id, loaded_on).cache()\n",
					"\n",
					"# Load\n",
					"util.save_overwrite_unmanaged_table(spark, new_dim_parkingbay_sdf, table_name=\"dw.dim_parking_bay\", path=os.path.join(base_path, \"dim_parking_bay\"))\n",
					"util.save_overwrite_unmanaged_table(spark, new_dim_location_sdf, table_name=\"dw.dim_location\", path=os.path.join(base_path, \"dim_location\"))\n",
					"util.save_overwrite_unmanaged_table(spark, new_dim_st_marker_sdf, table_name=\"dw.dim_st_marker\", path=os.path.join(base_path, \"dim_st_marker\"))"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 3. Transform and load Fact tables"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# Read existing Dimensions\n",
					"dim_parkingbay_sdf = spark.read.table(\"dw.dim_parking_bay\")\n",
					"dim_location_sdf = spark.read.table(\"dw.dim_location\")\n",
					"dim_st_marker = spark.read.table(\"dw.dim_st_marker\")\n",
					"\n",
					"# Process\n",
					"new_fact_parking = t.process_fact_parking(sensordata_sdf, dim_parkingbay_sdf, dim_location_sdf, dim_st_marker, load_id, loaded_on)\n",
					"\n",
					"# Insert new rows\n",
					"new_fact_parking.write.mode(\"append\").insertInto(\"dw.fact_parking\")\n",
					"\n",
					"# Recording record counts for logging purpose\n",
					"new_dim_parkingbay_count = spark.read.table(\"dw.dim_parking_bay\").count()\n",
					"new_dim_location_count = spark.read.table(\"dw.dim_location\").count()\n",
					"new_dim_st_marker_count = spark.read.table(\"dw.dim_st_marker\").count()\n",
					"new_fact_parking_count = new_fact_parking.count()"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 4. Data Quality\r\n",
					"The following uses the [Great Expectations](https://greatexpectations.io/) library. See [Great Expectation Docs](https://docs.greatexpectations.io/docs/) for more info.\r\n",
					"\r\n",
					"**Note**: for simplication purposes, the [Expectation Suite](https://docs.greatexpectations.io/docs/terms/expectation_suite) is created inline. Generally this should be created prior to data pipeline execution, and only loaded during runtime and executed against a data [Batch](https://docs.greatexpectations.io/docs/terms/batch/) via [Checkpoint](https://docs.greatexpectations.io/docs/terms/checkpoint/).\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd\r\n",
					"# import os\r\n",
					"from ruamel import yaml\r\n",
					"from great_expectations.core.batch import RuntimeBatchRequest\r\n",
					"from great_expectations.data_context import BaseDataContext\r\n",
					"from great_expectations.data_context.types.base import (\r\n",
					"    DataContextConfig,\r\n",
					"    DatasourceConfig,\r\n",
					"    FilesystemStoreBackendDefaults,\r\n",
					")\r\n",
					"from pyspark.sql import SparkSession, Row\r\n",
					"\r\n",
					"# 0. Create mount point path for spark job\r\n",
					"job_id = mssparkutils.env.getJobId()\r\n",
					"root_directory=f\"/synfs/{job_id}/great_expectations\"\r\n",
					"\r\n",
					"# 1. Configure DataContext\r\n",
					"# https://docs.greatexpectations.io/docs/terms/data_context\r\n",
					"data_context_config = DataContextConfig(\r\n",
					"    datasources={\r\n",
					"        \"transformed_data_source\": DatasourceConfig(\r\n",
					"            class_name=\"Datasource\",\r\n",
					"            execution_engine={\"class_name\": \"SparkDFExecutionEngine\"},\r\n",
					"            data_connectors={\r\n",
					"                \"transformed_data_connector\": {\r\n",
					"                    \"module_name\": \"great_expectations.datasource.data_connector\",\r\n",
					"                    \"class_name\": \"RuntimeDataConnector\",\r\n",
					"                    \"batch_identifiers\": [\r\n",
					"                        \"environment\",\r\n",
					"                        \"pipeline_run_id\",\r\n",
					"                    ],\r\n",
					"                }\r\n",
					"            }\r\n",
					"        )\r\n",
					"    },\r\n",
					"    store_backend_defaults=FilesystemStoreBackendDefaults(root_directory=root_directory)\r\n",
					")\r\n",
					"context = BaseDataContext(project_config=data_context_config)\r\n",
					"\r\n",
					"# 2. Create a BatchRequest based on new_fact_parking dataframe.\r\n",
					"# https://docs.greatexpectations.io/docs/terms/batch\r\n",
					"batch_request = RuntimeBatchRequest(\r\n",
					"    datasource_name=\"transformed_data_source\",\r\n",
					"    data_connector_name=\"transformed_data_connector\",\r\n",
					"    data_asset_name=\"paringbaydataaset\",  # This can be anything that identifies this data_asset for you\r\n",
					"    batch_identifiers={\r\n",
					"        \"environment\": \"stage\",\r\n",
					"        \"pipeline_run_id\": \"pipeline_run_id\",\r\n",
					"    },\r\n",
					"    runtime_parameters={\"batch_data\": new_fact_parking},  # Your dataframe goes here\r\n",
					")\r\n",
					"\r\n",
					"\r\n",
					"# 3. Define Expecation Suite and corresponding Data Expectations\r\n",
					"# https://docs.greatexpectations.io/docs/terms/expectation_suite\r\n",
					"expectation_suite_name = \"Transfomed_data_exception_suite_basic\"\r\n",
					"context.add_or_update_expectation_suite(expectation_suite_name=expectation_suite_name)\r\n",
					"validator = context.get_validator(\r\n",
					"    batch_request=batch_request,\r\n",
					"    expectation_suite_name=expectation_suite_name,\r\n",
					")\r\n",
					"# Add Validatons to suite\r\n",
					"# Check available expectations: validator.list_available_expectation_types()\r\n",
					"# https://legacy.docs.greatexpectations.io/en/latest/autoapi/great_expectations/expectations/index.html\r\n",
					"# https://legacy.docs.greatexpectations.io/en/latest/reference/core_concepts/expectations/standard_arguments.html#meta\r\n",
					"validator.expect_column_values_to_not_be_null(column=\"status\")\r\n",
					"validator.expect_column_values_to_be_of_type(column=\"status\", type_=\"StringType\")\r\n",
					"validator.expect_column_values_to_not_be_null(column=\"dim_time_id\")\r\n",
					"validator.expect_column_values_to_be_of_type(column=\"dim_time_id\", type_=\"IntegerType\")\r\n",
					"validator.expect_column_values_to_not_be_null(column=\"dim_parking_bay_id\")\r\n",
					"validator.expect_column_values_to_be_of_type(column=\"dim_parking_bay_id\", type_=\"StringType\")\r\n",
					"#validator.validate() # To run run validations without checkpoint\r\n",
					"validator.save_expectation_suite(discard_failed_expectations=False)\r\n",
					"\r\n",
					"\r\n",
					"# 4. Configure a checkpoint and run Expectation suite using checkpoint\r\n",
					"# https://docs.greatexpectations.io/docs/terms/checkpoint\r\n",
					"my_checkpoint_name = \"Transformed Data\"\r\n",
					"checkpoint_config = {\r\n",
					"    \"name\": my_checkpoint_name,\r\n",
					"    \"config_version\": 1.0,\r\n",
					"    \"class_name\": \"SimpleCheckpoint\",\r\n",
					"    \"run_name_template\": \"%Y%m%d-%H%M%S-my-run-name-template\",\r\n",
					"}\r\n",
					"my_checkpoint = context.test_yaml_config(yaml.dump(checkpoint_config,default_flow_style=False))\r\n",
					"context.add_or_update_checkpoint(**checkpoint_config)\r\n",
					"# Run Checkpoint passing in expectation suite\r\n",
					"checkpoint_result = context.run_checkpoint(\r\n",
					"    checkpoint_name=my_checkpoint_name,\r\n",
					"    validations=[\r\n",
					"        {\r\n",
					"            \"batch_request\": batch_request,\r\n",
					"            \"expectation_suite_name\": expectation_suite_name,\r\n",
					"        }\r\n",
					"    ],\r\n",
					")"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 5. Observability: Logging to Azure Application Insights using OpenCensus Library"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"import logging\n",
					"import os\n",
					"from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
					"from opencensus.ext.azure.log_exporter import AzureEventHandler\n",
					"from datetime import datetime\n",
					"\n",
					"# Getting Application Insights instrumentation key\n",
					"appi_key = token_library.getSecretWithLS(keyvaultlsname,\"applicationInsightsKey\")\n",
					"\n",
					"# Enable App Insights\n",
					"aiLogger = logging.getLogger(__name__)\n",
					"aiLogger.addHandler(AzureEventHandler(connection_string = 'InstrumentationKey=' + appi_key))\n",
					"\n",
					"aiLogger.setLevel(logging.INFO)\n",
					"\n",
					"aiLogger.info(\"Transform (ai): Started at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
					"properties = {\"custom_dimensions\": {\"pipeline\": pipelinename, \"run_id\": loadid, \"new_parkingbay_count\": new_dim_parkingbay_count}}\n",
					"aiLogger.info(\"Transform (ai): Completed at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), extra=properties)\n",
					"\n",
					"# To query this log, go to the Azure Monitor and run the following kusto query (Scope: Application Insights instance):\n",
					"#customEvents\n",
					"#| order by timestamp desc\n",
					"#| project timestamp, appName, name,\n",
					"#    pipelineName             = customDimensions.pipeline,\n",
					"#    pipelineRunId            = customDimensions.run_id,\n",
					"#    parkingbayCount          = customDimensions.parkingbay_count,\n",
					"#    sensordataCount          = customDimensions.sensordata_count,\n",
					"#    parkingbayMalformedCount = customDimensions.parkingbay_malformed_count,\n",
					"#    sensordataMalformedCount = customDimensions.sensordata_malformed_count,\n",
					"#    dimParkingbayCount       = customDimensions.new_parkingbay_count"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 6. Observability: Logging to Log Analytics workspace using log4j"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"import logging\n",
					"import sys\n",
					"\n",
					"# Enable Log Analytics using log4j\n",
					"log4jLogger = sc._jvm.org.apache.log4j\n",
					"logger = log4jLogger.LogManager.getLogger(\"ParkingSensorLogs\")\n",
					"\n",
					"def log(msg = ''):\n",
					"    env = mssparkutils.env\n",
					"    formatted_msg = f'Transform (log4j): {msg}~{pipelinename}~{env.getJobId()}~{env.getPoolName()}~{env.getWorkspaceName()}~{env.getUserId()}'\n",
					"    logger.info(formatted_msg)\n",
					"\n",
					"log(\"Started at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
					"\n",
					"log(f'new_dim_parkingbay_count: {new_dim_parkingbay_count}')\n",
					"log(f'new_dim_location_count: {new_dim_location_count}')\n",
					"log(f'new_dim_st_marker_count: {new_dim_st_marker_count}')\n",
					"log(f'new_fact_parking_count: {new_fact_parking_count}')\n",
					"\n",
					"log(\"Completed at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
					"\n",
					"# To query this log, go to the log analytics workspace and run the following kusto query (Scope: Log Analytics Workspace):\n",
					"#SparkLoggingEvent_CL\n",
					"#| where logger_name_s == \"ParkingSensorLogs\"\n",
					"#| order by TimeGenerated desc\n",
					"#| project TimeGenerated, workspaceName_s, Level,\n",
					"#    message         = split(Message, '~', 0),\n",
					"#    pipelineName    = split(Message, '~', 1),\n",
					"#    jobId           = split(Message, '~', 2),\n",
					"#    SparkPoolName   = split(Message, '~', 3),\n",
					"#    UserId          = split(Message, '~', 5)"
				]
			}
		]
	}
}