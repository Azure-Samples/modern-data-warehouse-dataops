{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Get dynamic pipeline parameters"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get pipeline run id\r\n",
        "loadid = 0\r\n",
        "\r\n",
        "# Get pipeline name\r\n",
        "pipelinename = 'pipeline_name'\r\n",
        "\r\n",
        "# Get keyvault linked service name\r\n",
        "keyvaultlsname = 'Ls_KeyVault_01'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "synspdevdep70",
              "session_id": 39,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-13T11:21:06.3255817Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-13T11:21:06.5406041Z",
              "execution_finish_time": "2021-10-13T11:21:06.5408088Z"
            },
            "text/plain": "StatementMeta(synspdevdep70, 39, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true,
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Prepare observability mechanisms variables"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "sc = SparkSession.builder.getOrCreate()\r\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
        "\r\n",
        "env = mssparkutils.env\r\n",
        "pipelineruninfo = f'[{pipelinename}]::[{loadid}]::[{env.getJobId()}]::[{env.getPoolName}]::[{env.getWorkspaceName}]::[{env.getUserId()}]'\r\n",
        "\r\n",
        "# Needed to get App Insights Key\r\n",
        "appi_key = token_library.getSecretWithLS(keyvaultlsname,\"applicationInsightsKey\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "synspdevdep70",
              "session_id": 39,
              "statement_id": 29,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-13T11:54:37.2880417Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-13T11:54:37.3960971Z",
              "execution_finish_time": "2021-10-13T11:54:37.544946Z"
            },
            "text/plain": "StatementMeta(synspdevdep70, 39, 29, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "syndpdevdep70"
          ]
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Transform and load Dimension tables\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\r\n",
        "import os\r\n",
        "from pyspark.sql.functions import col, lit\r\n",
        "import ddo_transform.transform as t\r\n",
        "import ddo_transform.util as util\r\n",
        "\r\n",
        "load_id = loadid\r\n",
        "loaded_on = datetime.datetime.now()\r\n",
        "\r\n",
        "# Primary storage info \r\n",
        "account_name = token_library.getSecretWithLS(keyvaultlsname,\"datalakeaccountname\")\r\n",
        "container_name = 'datalake' # fill in your container name \r\n",
        "relative_path = 'data/dw/' # fill in your relative folder path \r\n",
        "\r\n",
        "base_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
        "\r\n",
        "# Read interim cleansed data\r\n",
        "parkingbay_sdf = spark.read.table(\"interim.parking_bay\").filter(col('load_id') == lit(load_id))\r\n",
        "sensordata_sdf = spark.read.table(\"interim.sensor\").filter(col('load_id') == lit(load_id))\r\n",
        "\r\n",
        "# Read existing Dimensions\r\n",
        "dim_parkingbay_sdf = spark.read.table(\"dw.dim_parking_bay\")\r\n",
        "dim_location_sdf = spark.read.table(\"dw.dim_location\")\r\n",
        "dim_st_marker = spark.read.table(\"dw.dim_st_marker\")\r\n",
        "\r\n",
        "# Transform\r\n",
        "new_dim_parkingbay_sdf = t.process_dim_parking_bay(parkingbay_sdf, dim_parkingbay_sdf, load_id, loaded_on).cache()\r\n",
        "new_dim_location_sdf = t.process_dim_location(sensordata_sdf, dim_location_sdf, load_id, loaded_on).cache()\r\n",
        "new_dim_st_marker_sdf = t.process_dim_st_marker(sensordata_sdf, dim_st_marker, load_id, loaded_on).cache()\r\n",
        "\r\n",
        "# Load\r\n",
        "util.save_overwrite_unmanaged_table(spark, new_dim_parkingbay_sdf, table_name=\"dw.dim_parking_bay\", path=os.path.join(base_path, \"dim_parking_bay\"))\r\n",
        "util.save_overwrite_unmanaged_table(spark, new_dim_location_sdf, table_name=\"dw.dim_location\", path=os.path.join(base_path, \"dim_location\"))\r\n",
        "util.save_overwrite_unmanaged_table(spark, new_dim_st_marker_sdf, table_name=\"dw.dim_st_marker\", path=os.path.join(base_path, \"dim_st_marker\"))\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "synspdevdep70",
              "session_id": 39,
              "statement_id": 43,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-13T12:13:14.0854264Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-13T12:13:14.1930594Z",
              "execution_finish_time": "2021-10-13T12:14:03.7517475Z"
            },
            "text/plain": "StatementMeta(synspdevdep70, 39, 43, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Transform and load Fact tables"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read existing Dimensions\r\n",
        "dim_parkingbay_sdf = spark.read.table(\"dw.dim_parking_bay\")\r\n",
        "dim_location_sdf = spark.read.table(\"dw.dim_location\")\r\n",
        "dim_st_marker = spark.read.table(\"dw.dim_st_marker\")\r\n",
        "\r\n",
        "# Process\r\n",
        "nr_fact_parking = t.process_fact_parking(sensordata_sdf, dim_parkingbay_sdf, dim_location_sdf, dim_st_marker, load_id, loaded_on)\r\n",
        "\r\n",
        "# Insert new rows\r\n",
        "nr_fact_parking.write.mode(\"append\").insertInto(\"dw.fact_parking\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "synspdevdep70",
              "session_id": 39,
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-13T11:33:54.485295Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-13T11:33:54.5931828Z",
              "execution_finish_time": "2021-10-13T11:33:55.1026213Z"
            },
            "text/plain": "StatementMeta(synspdevdep70, 39, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Table or view not found: `dw`.`dim_parking_bay`;;\n'UnresolvedRelation `dw`.`dim_parking_bay`\n",
          "traceback": [
            "AnalysisException: Table or view not found: `dw`.`dim_parking_bay`;;\n'UnresolvedRelation `dw`.`dim_parking_bay`\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 301, in table\n    return self._df(self._jreader.table(tableName))\n",
            "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 75, in deco\n    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
            "pyspark.sql.utils.AnalysisException: Table or view not found: `dw`.`dim_parking_bay`;;\n'UnresolvedRelation `dw`.`dim_parking_bay`\n\n"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.  Observability: create log messages"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_dim_parkingbay_count = spark.read.table(\"dw.dim_parking_bay\").count()\r\n",
        "new_dim_location_count = spark.read.table(\"dw.dim_location\").count()\r\n",
        "new_dim_st_marker_count = spark.read.table(\"dw.dim_st_marker\").count()\r\n",
        "nr_fact_parking_count = nr_fact_parking.count()\r\n",
        "\r\n",
        "\r\n",
        "final_message = f'Transform : Completed load::[new_dim_parkingbay_count::{new_dim_parkingbay_count}]::[new_dim_location_count:{new_dim_location_count}]::[new_dim_st_marker_count:{new_dim_st_marker_count}]::[nr_fact_parking_count:{nr_fact_parking_count}]'\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "synspdevdep70",
              "session_id": 39,
              "statement_id": 36,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-13T12:00:16.4152159Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-13T12:00:16.5550552Z",
              "execution_finish_time": "2021-10-13T12:00:19.3137304Z"
            },
            "text/plain": "StatementMeta(synspdevdep70, 39, 36, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nr_fact_parking' is not defined",
          "traceback": [
            "NameError: name 'nr_fact_parking' is not defined",
            "Traceback (most recent call last):\n",
            "NameError: name 'nr_fact_parking' is not defined\n"
          ]
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Observability: logging on App Insigths using OpenCensus Library"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\r\n",
        "import os\r\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\r\n",
        "from opencensus.ext.azure.log_exporter import AzureEventHandler\r\n",
        "from pyspark.sql.session import SparkSession\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "# Enable App Insights\r\n",
        "aiLogger = logging.getLogger(\"ParkingSensorLogs-Standardize\")\r\n",
        "aiLogger.addHandler(AzureEventHandler(connection_string = 'InstrumentationKey=' + appi_key))\r\n",
        "#logger.addHandler(AzureLogHandler(connection_string = 'InstrumentationKey=' + appi_key))\r\n",
        "\r\n",
        "\r\n",
        "aiLogger.warning(\"Starting at: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\r\n",
        "properties = {'custom_dimensions': {'pipeline': pipelinename, 'run_id': loadid, 'new parking count': new_dim_parkingbay_count}}\r\n",
        "aiLogger.warning(final_message, extra=properties)\r\n",
        "# To query this log go to the Azure Monitor and run the following kusto query (if you are using the EventHandler)\r\n",
        "#customEvents\r\n",
        "#|order by timestamp desc\r\n",
        "# To query this log go to the Azure Monitor and run the following kusto query (if you are using the LogHandler)\r\n",
        "# traces\r\n",
        "#|order by timestamp desc"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Observability: logging on Log Analytics workspace using Log4J"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\r\n",
        "import sys\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "sc = SparkSession.builder.getOrCreate()\r\n",
        "env = mssparkutils.env\r\n",
        "pipelineruninfo = f'[{pipelinename}]::[{loadid}]::[{env.getJobId()}]::[{env.getPoolName}]::[{env.getWorkspaceName}]::[{env.getUserId()}]'\r\n",
        "final_message = f'Standardize : Completed load::[new_dim_parkingbay_count:{new_dim_parkingbay_count}]::[new_dim_location_count:{new_dim_location_count}]::[new_dim_st_marker_count :{new_dim_st_marker_count }]::[nr_fact_parking_count:{nr_fact_parking_count}]'\r\n",
        "\r\n",
        "# Enable Log Analytics using Log4J\r\n",
        "log4jLogger = sc._jvm.org.apache.log4j\r\n",
        "logger = log4jLogger.LogManager.getLogger(\"ParkingSensorLogs-Standardize\")\r\n",
        "logger.info(final_message)\r\n",
        "# To query this log go to the log analytics workspace and run the following kusto query:\r\n",
        "# SparkLoggingEvent_CL\r\n",
        "# | where logger_name_s == \"ParkingSensorLogs-Standardize\"\r\n",
        "sc.stop"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}