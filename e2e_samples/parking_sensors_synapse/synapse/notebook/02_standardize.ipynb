{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 1. Get dynamic pipeline parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Get folder where the REST downloads were placed\n",
        "infilefolder = '2021_10_05_07_58_15/'\n",
        "\n",
        "# Get pipeline name\n",
        "pipelinename = 'P_Ingest_MelbParkingData'\n",
        "\n",
        "# Get pipeline run id\n",
        "loadid = ''\n",
        "\n",
        "# Get keyvault linked service name\n",
        "keyvaultlsname = 'Ls_KeyVault_01'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 2. Load file path variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "\n",
        "# Primary storage info \n",
        "account_name = token_library.getSecretWithLS( keyvaultlsname, \"datalakeaccountname\")\n",
        "container_name = 'datalake' # fill in your container name \n",
        "relative_path = 'data/lnd/' # fill in your relative folder path \n",
        "\n",
        "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
        "print('Primary storage account path: ' + adls_path) \n",
        "load_id = loadid\n",
        "loaded_on = datetime.datetime.now()\n",
        "base_path = os.path.join(adls_path, infilefolder)\n",
        "\n",
        "parkingbay_filepath = os.path.join(base_path, \"MelbParkingBayData.json\")\n",
        "print(parkingbay_filepath)\n",
        "sensors_filepath = os.path.join(base_path, \"MelbParkingSensorData.json\")\n",
        "print(sensors_filepath)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 3. Transform: Standardize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import ddo_transform.standardize as s\n",
        "\n",
        "# Retrieve schema\n",
        "parkingbay_schema = s.get_schema(\"in_parkingbay_schema\")\n",
        "sensordata_schema = s.get_schema(\"in_sensordata_schema\")\n",
        "\n",
        "# Read data\n",
        "parkingbay_sdf = spark.read\\\n",
        "  .schema(parkingbay_schema)\\\n",
        "  .option(\"badRecordsPath\", os.path.join(base_path, \"__corrupt\", \"MelbParkingBayData\"))\\\n",
        "  .json(parkingbay_filepath)\n",
        "sensordata_sdf = spark.read\\\n",
        "  .schema(sensordata_schema)\\\n",
        "  .option(\"badRecordsPath\", os.path.join(base_path, \"__corrupt\", \"MelbParkingSensorData\"))\\\n",
        "  .json(sensors_filepath)\n",
        "\n",
        "# Standardize\n",
        "t_parkingbay_sdf, t_parkingbay_malformed_sdf = s.standardize_parking_bay(parkingbay_sdf, load_id, loaded_on)\n",
        "t_sensordata_sdf, t_sensordata_malformed_sdf = s.standardize_sensordata(sensordata_sdf, load_id, loaded_on)\n",
        "\n",
        "# Insert new rows\n",
        "t_parkingbay_sdf.write.mode(\"append\").insertInto(\"interim.parking_bay\")\n",
        "t_sensordata_sdf.write.mode(\"append\").insertInto(\"interim.sensor\")\n",
        "\n",
        "# Insert bad rows\n",
        "t_parkingbay_malformed_sdf.write.mode(\"append\").insertInto(\"malformed.parking_bay\")\n",
        "t_sensordata_malformed_sdf.write.mode(\"append\").insertInto(\"malformed.sensor\")\n",
        "\n",
        "# Recording record counts for logging purpose\n",
        "parkingbay_count = t_parkingbay_sdf.count()\n",
        "sensordata_count = t_sensordata_sdf.count()\n",
        "parkingbay_malformed_count = t_parkingbay_malformed_sdf.count()\n",
        "sensordata_malformed_count = t_sensordata_malformed_sdf.count()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Data Quality\n",
        "The following uses the [Great Expectations](https://greatexpectations.io/) library. See [Great Expectation Docs](https://docs.greatexpectations.io/docs/) for more info.\n",
        "\n",
        "**Note**: for simplication purposes, the [Expectation Suite](https://docs.greatexpectations.io/docs/terms/expectation_suite) is created inline. Generally this should be created prior to data pipeline execution, and only loaded during runtime and executed against a data [Batch](https://docs.greatexpectations.io/docs/terms/batch/) via [Checkpoint](https://docs.greatexpectations.io/docs/terms/checkpoint/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from ruamel import yaml\n",
        "from great_expectations.core.batch import RuntimeBatchRequest\n",
        "from great_expectations.data_context import BaseDataContext\n",
        "from great_expectations.data_context.types.base import (\n",
        "    DataContextConfig,\n",
        "    DatasourceConfig,\n",
        "    FilesystemStoreBackendDefaults,\n",
        ")\n",
        "from pyspark.sql import SparkSession, Row\n",
        "\n",
        "# 0. Create mount point path for spark job\n",
        "job_id = mssparkutils.env.getJobId()\n",
        "root_directory=f\"/synfs/{job_id}/great_expectations\"\n",
        "\n",
        "\n",
        "# 1. Configure DataContext\n",
        "# https://docs.greatexpectations.io/docs/terms/data_context\n",
        "data_context_config = DataContextConfig(\n",
        "    datasources={\n",
        "        \"parkingbay_data_source\": DatasourceConfig(\n",
        "            class_name=\"Datasource\",\n",
        "            execution_engine={\"class_name\": \"SparkDFExecutionEngine\"},\n",
        "            data_connectors={\n",
        "                \"parkingbay_data_connector\": {\n",
        "                    \"module_name\": \"great_expectations.datasource.data_connector\",\n",
        "                    \"class_name\": \"RuntimeDataConnector\",\n",
        "                    \"batch_identifiers\": [\n",
        "                        \"environment\",\n",
        "                        \"pipeline_run_id\",\n",
        "                    ],\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "    },\n",
        "    store_backend_defaults=FilesystemStoreBackendDefaults(root_directory=root_directory)\n",
        ")\n",
        "context = BaseDataContext(project_config=data_context_config)\n",
        "\n",
        "\n",
        "# 2. Create a BatchRequest based on parkingbay_sdf dataframe.\n",
        "# https://docs.greatexpectations.io/docs/terms/batch\n",
        "batch_request = RuntimeBatchRequest(\n",
        "    datasource_name=\"parkingbay_data_source\",\n",
        "    data_connector_name=\"parkingbay_data_connector\",\n",
        "    data_asset_name=\"paringbaydataaset\",  # This can be anything that identifies this data_asset for you\n",
        "    batch_identifiers={\n",
        "        \"environment\": \"stage\",\n",
        "        \"pipeline_run_id\": \"pipeline_run_id\",\n",
        "    },\n",
        "    runtime_parameters={\"batch_data\": parkingbay_sdf},  # Your dataframe goes here\n",
        ")\n",
        "\n",
        "\n",
        "# 3. Define Expecation Suite and corresponding Data Expectations\n",
        "# https://docs.greatexpectations.io/docs/terms/expectation_suite\n",
        "expectation_suite_name = \"parkingbay_data_exception_suite_basic\"\n",
        "context.add_or_update_expectation_suite(expectation_suite_name=expectation_suite_name)\n",
        "validator = context.get_validator(\n",
        "    batch_request=batch_request,\n",
        "    expectation_suite_name=expectation_suite_name,\n",
        ")\n",
        "# Add Validatons to suite\n",
        "# Check available expectations: validator.list_available_expectation_types()\n",
        "# https://legacy.docs.greatexpectations.io/en/latest/autoapi/great_expectations/expectations/index.html\n",
        "# https://legacy.docs.greatexpectations.io/en/latest/reference/core_concepts/expectations/standard_arguments.html#meta\n",
        "validator.expect_column_values_to_not_be_null(column=\"meter_id\")\n",
        "validator.expect_column_values_to_not_be_null(column=\"marker_id\")\n",
        "validator.expect_column_values_to_be_of_type(column=\"rd_seg_dsc\", type_=\"StringType\")\n",
        "validator.expect_column_values_to_be_of_type(column=\"rd_seg_id\", type_=\"IntegerType\")\n",
        "# validator.validate() # To run run validations without checkpoint\n",
        "validator.save_expectation_suite(discard_failed_expectations=False)\n",
        "\n",
        "\n",
        "# 4. Configure a checkpoint and run Expectation suite using checkpoint\n",
        "# https://docs.greatexpectations.io/docs/terms/checkpoint\n",
        "my_checkpoint_name = \"Parkingbay Data DQ\"\n",
        "checkpoint_config = {\n",
        "    \"name\": my_checkpoint_name,\n",
        "    \"config_version\": 1.0,\n",
        "    \"class_name\": \"SimpleCheckpoint\",\n",
        "    \"run_name_template\": \"%Y%m%d-%H%M%S-my-run-name-template\",\n",
        "}\n",
        "my_checkpoint = context.test_yaml_config(yaml.dump(checkpoint_config))\n",
        "context.add_or_update_checkpoint(**checkpoint_config)\n",
        "# Run Checkpoint passing in expectation suite.\n",
        "checkpoint_result = context.run_checkpoint(\n",
        "    checkpoint_name=my_checkpoint_name,\n",
        "    validations=[\n",
        "        {\n",
        "            \"batch_request\": batch_request,\n",
        "            \"expectation_suite_name\": expectation_suite_name,\n",
        "        }\n",
        "    ],\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 5. Observability: Logging to Azure Application Insights using OpenCensus Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.log_exporter import AzureEventHandler\n",
        "from datetime import datetime\n",
        "\n",
        "# Getting Application Insights instrumentation key\n",
        "appi_key = token_library.getSecretWithLS(keyvaultlsname,\"applicationInsightsKey\")\n",
        "\n",
        "# Enable App Insights\n",
        "aiLogger = logging.getLogger(__name__)\n",
        "aiLogger.addHandler(AzureEventHandler(connection_string = 'InstrumentationKey=' + appi_key))\n",
        "\n",
        "aiLogger.setLevel(logging.INFO)\n",
        "\n",
        "aiLogger.info(\"Standardize (ai): Started at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "properties = {\"custom_dimensions\": {\"pipeline\": pipelinename, \"run_id\": loadid, \"parkingbay_count\": parkingbay_count, \"sensordata_count\": sensordata_count, \"parkingbay_malformed_count\": parkingbay_malformed_count, \"sensordata_malformed_count\": sensordata_malformed_count}}\n",
        "aiLogger.info(\"Standardize (ai): Completed at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), extra=properties)\n",
        "\n",
        "# To query this log, go to the Azure Monitor and run the following kusto query (Scope: Application Insights instance):\n",
        "#customEvents\n",
        "#| order by timestamp desc\n",
        "#| project timestamp, appName, name,\n",
        "#    pipelineName             = customDimensions.pipeline,\n",
        "#    pipelineRunId            = customDimensions.run_id,\n",
        "#    parkingbayCount          = customDimensions.parkingbay_count,\n",
        "#    sensordataCount          = customDimensions.sensordata_count,\n",
        "#    parkingbayMalformedCount = customDimensions.parkingbay_malformed_count,\n",
        "#    sensordataMalformedCount = customDimensions.sensordata_malformed_count,\n",
        "#    dimParkingbayCount       = customDimensions.new_parkingbay_count\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 6. Observability: Logging to Log Analytics workspace using log4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "# Enable Log Analytics using log4j\n",
        "log4jLogger = sc._jvm.org.apache.log4j\n",
        "logger = log4jLogger.LogManager.getLogger(\"ParkingSensorLogs\")\n",
        "\n",
        "def log(msg = ''):\n",
        "    env = mssparkutils.env\n",
        "    formatted_msg = f'Standardize (log4j): {msg}~{pipelinename}~{env.getJobId()}~{env.getPoolName()}~{env.getWorkspaceName()}~{env.getUserId()}'\n",
        "    logger.info(formatted_msg)\n",
        "\n",
        "log(\"Started at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "log(f'parkingbay_count: {parkingbay_count}')\n",
        "log(f'sensordata_count: {sensordata_count}')\n",
        "log(f'parkingbay_malformed_count: {parkingbay_malformed_count}')\n",
        "log(f'sensordata_malformed_count: {sensordata_malformed_count}')\n",
        "\n",
        "log(\"Completed at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "# To query this log, go to the log analytics workspace and run the following kusto query (Scope: Log Analytics Workspace):\n",
        "#SparkLoggingEvent_CL\n",
        "#| where logger_name_s == \"ParkingSensorLogs\"\n",
        "#| order by TimeGenerated desc\n",
        "#| project TimeGenerated, workspaceName_s, Level,\n",
        "#    message         = split(Message, '~', 0),\n",
        "#    pipelineName    = split(Message, '~', 1),\n",
        "#    jobId           = split(Message, '~', 2),\n",
        "#    SparkPoolName   = split(Message, '~', 3),\n",
        "#    UserId          = split(Message, '~', 5)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
