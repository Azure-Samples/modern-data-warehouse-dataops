{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Get dynamic pipeline parameters"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get folder where the REST downloads were placed\r\n",
        "infilefolder = '2021_10_05_07_58_15/'\r\n",
        "\r\n",
        "# Get pipeline name\r\n",
        "pipelinename = 'P_Ingest_MelbParkingData'\r\n",
        "\r\n",
        "# Get pipeline run id\r\n",
        "loadid = ''\r\n",
        "\r\n",
        "# Get keyvault linked service name\r\n",
        "keyvaultlsname = 'Ls_KeyVault_01'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true,
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load file path variables"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "import datetime\r\n",
        "\r\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
        "\r\n",
        "# Primary storage info \r\n",
        "account_name = token_library.getSecretWithLS( keyvaultlsname, \"datalakeaccountname\")\r\n",
        "container_name = 'datalake' # fill in your container name \r\n",
        "relative_path = 'data/lnd/' # fill in your relative folder path \r\n",
        "\r\n",
        "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
        "print('Primary storage account path: ' + adls_path) \r\n",
        "load_id = loadid\r\n",
        "loaded_on = datetime.datetime.now()\r\n",
        "base_path = os.path.join(adls_path, infilefolder)\r\n",
        "\r\n",
        "parkingbay_filepath = os.path.join(base_path, \"MelbParkingBayData.json\")\r\n",
        "print(parkingbay_filepath)\r\n",
        "sensors_filepath = os.path.join(base_path, \"MelbParkingSensorData.json\")\r\n",
        "print(sensors_filepath)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true,
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Transform: Standardize"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ddo_transform.standardize as s\r\n",
        "\r\n",
        "# Retrieve schema\r\n",
        "parkingbay_schema = s.get_schema(\"in_parkingbay_schema\")\r\n",
        "sensordata_schema = s.get_schema(\"in_sensordata_schema\")\r\n",
        "\r\n",
        "# Read data\r\n",
        "parkingbay_sdf = spark.read\\\r\n",
        "  .schema(parkingbay_schema)\\\r\n",
        "  .option(\"badRecordsPath\", os.path.join(base_path, \"__corrupt\", \"MelbParkingBayData\"))\\\r\n",
        "  .option(\"multiLine\", True)\\\r\n",
        "  .json(parkingbay_filepath)\r\n",
        "sensordata_sdf = spark.read\\\r\n",
        "  .schema(sensordata_schema)\\\r\n",
        "  .option(\"badRecordsPath\", os.path.join(base_path, \"__corrupt\", \"MelbParkingSensorData\"))\\\r\n",
        "  .option(\"multiLine\", True)\\\r\n",
        "  .json(sensors_filepath)\r\n",
        "\r\n",
        "# Standardize\r\n",
        "t_parkingbay_sdf, t_parkingbay_malformed_sdf = s.standardize_parking_bay(parkingbay_sdf, load_id, loaded_on)\r\n",
        "t_sensordata_sdf, t_sensordata_malformed_sdf = s.standardize_sensordata(sensordata_sdf, load_id, loaded_on)\r\n",
        "\r\n",
        "# Insert new rows\r\n",
        "t_parkingbay_sdf.write.mode(\"append\").insertInto(\"interim.parking_bay\")\r\n",
        "t_sensordata_sdf.write.mode(\"append\").insertInto(\"interim.sensor\")\r\n",
        "\r\n",
        "# Insert bad rows\r\n",
        "t_parkingbay_malformed_sdf.write.mode(\"append\").insertInto(\"malformed.parking_bay\")\r\n",
        "t_sensordata_malformed_sdf.write.mode(\"append\").insertInto(\"malformed.sensor\")\r\n",
        "\r\n",
        "# Recording record counts for logging purpose\r\n",
        "parkingbay_count = t_parkingbay_sdf.count()\r\n",
        "sensordata_count = t_sensordata_sdf.count()\r\n",
        "parkingbay_malformed_count = t_parkingbay_malformed_sdf.count()\r\n",
        "sensordata_malformed_count = t_sensordata_malformed_sdf.count()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Observability: Logging to Azure Application Insights using OpenCensus Library"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\r\n",
        "import os\r\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\r\n",
        "from opencensus.ext.azure.log_exporter import AzureEventHandler\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "# Getting Application Insights instrumentation key\r\n",
        "appi_key = token_library.getSecretWithLS(keyvaultlsname,\"applicationInsightsKey\")\r\n",
        "\r\n",
        "# Enable App Insights\r\n",
        "aiLogger = logging.getLogger(__name__)\r\n",
        "aiLogger.addHandler(AzureEventHandler(connection_string = 'InstrumentationKey=' + appi_key))\r\n",
        "#logger.addHandler(AzureLogHandler(connection_string = 'InstrumentationKey=' + appi_key))\r\n",
        "\r\n",
        "aiLogger.setLevel(logging.INFO)\r\n",
        "#aiLogger.setLevel(logging.WARNING)\r\n",
        "\r\n",
        "aiLogger.info(\"Standardize (ai): Started at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\r\n",
        "properties = {\"custom_dimensions\": {\"pipeline\": pipelinename, \"run_id\": loadid, \"parkingbay_count\": parkingbay_count, \"sensordata_count\": sensordata_count, \"parkingbay_malformed_count\": parkingbay_malformed_count, \"sensordata_malformed_count\": sensordata_malformed_count}}\r\n",
        "aiLogger.info(\"Standardize (ai): Completed at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), extra=properties)\r\n",
        "\r\n",
        "# To query this log go to the Azure Monitor and run the following kusto query (if you are using the EventHandler)\r\n",
        "#customEvents\r\n",
        "#|order by timestamp desc\r\n",
        "# To query this log go to the Azure Monitor and run the following kusto query (if you are using the LogHandler)\r\n",
        "# traces\r\n",
        "#|order by timestamp desc"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Observability: Logging to Log Analytics workspace using log4j"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\r\n",
        "import sys\r\n",
        "\r\n",
        "# Enable Log Analytics using log4j\r\n",
        "log4jLogger = sc._jvm.org.apache.log4j\r\n",
        "logger = log4jLogger.LogManager.getLogger(\"ParkingSensorLogs\")\r\n",
        "\r\n",
        "def log(msg = ''):\r\n",
        "    env = mssparkutils.env\r\n",
        "    formatted_msg = f'Standardize (log4j): {msg}~{pipelinename}~{env.getJobId()}~{env.getPoolName()}~{env.getWorkspaceName()}~{env.getUserId()}'\r\n",
        "    logger.info(formatted_msg)\r\n",
        "\r\n",
        "log(\"Started at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\r\n",
        "\r\n",
        "log(f'parkingbay_count: {parkingbay_count}')\r\n",
        "log(f'sensordata_count: {sensordata_count}')\r\n",
        "log(f'parkingbay_malformed_count: {parkingbay_malformed_count}')\r\n",
        "log(f'sensordata_malformed_count: {sensordata_malformed_count}')\r\n",
        "\r\n",
        "log(\"Completed at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\r\n",
        "# To query this log go to the log analytics workspace and run the following kusto query:\r\n",
        "# SparkLoggingEvent_CL\r\n",
        "# | where logger_name_s == \"ParkingSensorLogs\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}