parameters:
- name: environmentName
  type: string
- name: dbclusterid
  type: string

jobs:
- deployment: deploy_databricks
  displayName: 'Deploy to Databricks'
  pool:
    vmImage: 'ubuntu-latest'
  variables:
    pythonVersion: 3.8
    databricksFile: databricks_cli_0.240.0_linux_amd64
    databricksCLIVersion: 0.240.0
  environment: ${{ parameters.environmentName }}
  strategy:
    runOnce:
      deploy:
        steps:     
        - task: UsePythonVersion@0
          inputs:
            versionSpec: '$(pythonVersion)'
            addToPath: true
            architecture: 'x64'
          displayName: 'Use Python Version: $(pythonVersion)'

        - script: |
            # Change into temporary directory
            tmpdir="$(mktemp -d)"
            cd "$tmpdir"
            echo "tempdir: $tmpdir"
            
            # Download release archive.
            wget -q https://github.com/databricks/cli/releases/download/v$(databricksCLIVersion)/$(databricksFile).zip -O $tmpdir/$(databricksFile).zip
            
            # Verify the checksum
            cd $tmpdir && sha256sum $(databricksFile).zip > $tmpdir/$(databricksFile).zip.sha256
            cd $tmpdir 
            # Run the sha256sum check
            sha256sum -c $(databricksFile).zip.sha256
            
            # Check the exit status of the sha256sum command
            if [ $? -ne 0 ]; then
              echo "Checksum validation failed. Exiting."
              exit 1
            else
              echo "Checksum validation succeeded."
            fi
            
            # Unzip the downloaded file
            unzip -q $tmpdir/$(databricksFile).zip
            
            # Add databricks to path.
            chmod +x ./databricks
            cp ./databricks "/usr/local/bin"
            
            echo "Installed $("databricks" -v) at /usr/local/bin/databricks."
            
            # Clean up the zip file and checksum
            rm $tmpdir/$(databricksFile).zip $tmpdir/$(databricksFile).zip.sha256
            
            # Clean up temporary directory.
            cd "$OLDPWD"
            rm -rf "$tmpdir" || true
            echo "Cleaned up $tmpdir."
          displayName: 'Install Databricks CLI'
        - script: |
            packageWheelName=$(ls ${WHEEL_FILE_PATH} | grep '^ddo_transform.*.whl$' | head -n 1)
            echo "Package wheel name is: $packageWheelName"
            echo "##vso[task.setvariable variable=packageWheelName;isOutput=true]$packageWheelName"
          env:
            WHEEL_FILE_PATH: $(Pipeline.Workspace)/ciartifacts/dist
          name: setPackageWheelName
          displayName: 'Set packageWheelName variable'

        - script: |
            set -o errexit
            set -o pipefail
            set -o nounset

            # Check if release directory exists.
            databricks_release_path="/Workspace$DATABRICKS_NOTEBOOK_PATH"
            if databricks workspace list $databricks_release_path > /dev/null 2>&1; then
                echo "Folder '$databricks_release_path' already exists."
            else
                databricks workspace mkdirs $databricks_release_path
                echo "Folder '$databricks_release_path' created."
            fi

            echo "Creating folder in ADLS..."
            relative_path=${DBFS_LIBS_PATH#dbfs:/mnt/}
            fs_name="$(cut -d '/' -f 1 <<< $relative_path)"
            fs_folder_path="/$(cut -d '/' -f 2- <<< $relative_path)"
            stg_account_name=$(databricks secrets get-secret storage_scope datalakeAccountName | jq -r .value | base64 -d)
            stg_account_key=$(databricks secrets get-secret storage_scope datalakeKey | jq -r .value | base64 -d)
            az storage fs directory create -n "$fs_folder_path" -f $fs_name --account-name $stg_account_name --account-key $stg_account_key -o none

            echo "Uploading app libraries to DBFS..."
            echo "Uploading SOURCE whl file: ${WHEEL_FILE_PATH}/${WHEEL_FILE_NAME} to dbfs DESTINATION: ${DBFS_LIBS_PATH}/${WHEEL_FILE_NAME}"
            databricks fs cp --recursive --overwrite "${WHEEL_FILE_PATH}/${WHEEL_FILE_NAME}" "${DBFS_LIBS_PATH}/${WHEEL_FILE_NAME}"
            echo "Uploading notebooks at ${NOTEBOOKS_PATH} to workspace (${databricks_release_path})..."
            databricks workspace import "${databricks_release_path}/00_setup" --file "${NOTEBOOKS_PATH}/00_setup.py" --format SOURCE --language PYTHON --overwrite
            databricks workspace import "${databricks_release_path}/01_explore" --file "${NOTEBOOKS_PATH}/01_explore.py" --format SOURCE --language PYTHON --overwrite
            databricks workspace import "${databricks_release_path}/02_standardize" --file "${NOTEBOOKS_PATH}/02_standardize.py" --format SOURCE --language PYTHON --overwrite
            databricks workspace import "${databricks_release_path}/03_transform" --file "${NOTEBOOKS_PATH}/03_transform.py" --format SOURCE --language PYTHON --overwrite

            # Create JSON file for library installation
            temp_dir=$(mktemp -d)
            json_file="$temp_dir/libs.config.json"
            echo "$json_file"
            cat <<EOF > $json_file
            {
              "cluster_id": "${{ parameters.dbclusterid }}",
              "libraries": [
                {
                  "whl": "dbfs:/ddo_transform-localdev-py2.py3-none-any.whl"
                }
              ]
            }
            EOF
            
            # Install library on the cluster using the JSON file
            databricks libraries install --json @$json_file
          env:
            DATABRICKS_HOST: $(databricksDomain)
            DATABRICKS_TOKEN: $(databricksToken)
            DBFS_LIBS_PATH: $(databricksDbfsLibPath)
            WHEEL_FILE_PATH: $(Pipeline.Workspace)/ciartifacts/dist
            WHEEL_FILE_NAME: $(setPackageWheelName.packageWheelName)
            NOTEBOOKS_PATH: $(Pipeline.Workspace)/ciartifacts/databricks/notebooks
            DATABRICKS_NOTEBOOK_PATH: $(databricksNotebookPath)
          displayName: 'Deploy notebooks and packages'
 