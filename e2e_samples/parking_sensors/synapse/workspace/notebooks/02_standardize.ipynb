{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 1. Get dynamic pipeline parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "synspdevdep63",
              "session_id": 3,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-05T08:16:03.4397816Z",
              "session_start_time": "2021-10-05T08:16:03.4867645Z",
              "execution_start_time": "2021-10-05T08:18:46.4126947Z",
              "execution_finish_time": "2021-10-05T08:18:46.5707977Z"
            },
            "text/plain": "StatementMeta(synspdevdep63, 3, 1, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true,
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# Get folder where the REST downloads were placed\r\n",
        "infilefolder = '2021_10_05_07_58_15/'\r\n",
        "\r\n",
        "# Get pipeline name\r\n",
        "pipelinename = 'P_Ingest_MelbParkingData'\r\n",
        "\r\n",
        "# Get pipeline run id\r\n",
        "loadid = 'df2ddb82-9004-449f-84da-ae9484b446f4\"'\r\n",
        "\r\n",
        "# Get keyvault name\r\n",
        "keyvaultname = 'mdwdops-kv-dev-dep63'\r\n",
        "\r\n",
        "# Get keyvault linked service name\r\n",
        "keyvaultlsname = 'Ls_KeyVault_01'\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 2. Prepare observability mechanisms variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "synspdevdep63",
              "session_id": 3,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-05T08:16:11.9238137Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-05T08:18:46.6915887Z",
              "execution_finish_time": "2021-10-05T08:18:50.6592081Z"
            },
            "text/plain": "StatementMeta(synspdevdep63, 3, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "sc = SparkSession.builder.getOrCreate()\r\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
        "\r\n",
        "env = mssparkutils.env\r\n",
        "pipelineruninfo = f'[{pipelinename}]::[{loadid}]::[{env.getJobId()}]::[{env.getPoolName}]::[{env.getWorkspaceName}]::[{env.getUserId()}]'\r\n",
        "\r\n",
        "# Needed to get App Insights Key\r\n",
        "kv_connection_string = token_library.getConnectionString(keyvaultlsname)\r\n",
        "appi_key = token_library.getSecret(keyvaultname, \"applicationInsightsKey\", keyvaultlsname)\r\n",
        "#sc.stop\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 3. Load file path variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "synspdevdep63",
              "session_id": 3,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-05T08:23:19.9276528Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-05T08:23:20.1316847Z",
              "execution_finish_time": "2021-10-05T08:23:20.2792877Z"
            },
            "text/plain": "StatementMeta(synspdevdep63, 3, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primary storage account path: abfss://datalake@mdwdopsstdevdep63.dfs.core.windows.net/data/lnd/\nabfss://datalake@mdwdopsstdevdep63.dfs.core.windows.net/data/lnd/2021_10_05_07_58_15/MelbParkingBayData.json\nabfss://datalake@mdwdopsstdevdep63.dfs.core.windows.net/data/lnd/2021_10_05_07_58_15/MelbParkingSensorData.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x7f6ac9edf908>>"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "import os\r\n",
        "import datetime\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "sc = SparkSession.builder.getOrCreate()\r\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
        "\r\n",
        "# For testing\r\n",
        "#infilefolder = '2021_08_17_09_23_52/'\r\n",
        "\r\n",
        "# Primary storage info \r\n",
        "account_name = token_library.getSecret( keyvaultname, \"datalakeaccountname\", keyvaultlsname)\r\n",
        "container_name = 'datalake' # fill in your container name \r\n",
        "relative_path = 'data/lnd/' # fill in your relative folder path \r\n",
        "\r\n",
        "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
        "print('Primary storage account path: ' + adls_path) \r\n",
        "load_id = loadid\r\n",
        "loaded_on = datetime.datetime.now()\r\n",
        "base_path = os.path.join(adls_path, infilefolder)\r\n",
        "\r\n",
        "parkingbay_filepath = os.path.join(base_path, \"MelbParkingBayData.json\")\r\n",
        "print(parkingbay_filepath)\r\n",
        "sensors_filepath = os.path.join(base_path, \"MelbParkingSensorData.json\")\r\n",
        "print(sensors_filepath)\r\n",
        "sc.stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 4. Transform: Standardize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "synspdevdep63",
              "session_id": 3,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-05T08:23:29.320445Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-05T08:23:29.4634878Z",
              "execution_finish_time": "2021-10-05T08:23:31.2792401Z"
            },
            "text/plain": "StatementMeta(synspdevdep63, 3, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o218.json.\n: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://mdwdopsstdevdep63.dfs.core.windows.net/datalake/data/lnd/2021_10_05_07_58_15/MelbParkingBayData.json?upn=false&action=getStatus&timeout=90\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:166)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:414)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:551)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:430)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1627)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:556)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:544)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:544)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:358)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:212)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:397)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
          "traceback": [
            "Py4JJavaError: An error occurred while calling o218.json.\n: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://mdwdopsstdevdep63.dfs.core.windows.net/datalake/data/lnd/2021_10_05_07_58_15/MelbParkingBayData.json?upn=false&action=getStatus&timeout=90\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:166)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:414)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:551)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:430)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1627)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:556)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:544)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:544)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:358)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:212)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:397)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 274, in json\n    return self._df(self._jreader.json(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
            "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n    return f(*a, **kw)\n",
            "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o218.json.\n: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://mdwdopsstdevdep63.dfs.core.windows.net/datalake/data/lnd/2021_10_05_07_58_15/MelbParkingBayData.json?upn=false&action=getStatus&timeout=90\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:166)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:414)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:551)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:430)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1627)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:556)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:544)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:544)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:358)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:212)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:397)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "import ddo_transform.standardize as s\r\n",
        "\r\n",
        "# Retrieve schema\r\n",
        "parkingbay_schema = s.get_schema(\"in_parkingbay_schema\")\r\n",
        "sensordata_schema = s.get_schema(\"in_sensordata_schema\")\r\n",
        "\r\n",
        "# Read data\r\n",
        "parkingbay_sdf = spark.read\\\r\n",
        "  .schema(parkingbay_schema)\\\r\n",
        "  .option(\"badRecordsPath\", os.path.join(base_path, \"__corrupt\", \"MelbParkingBayData\"))\\\r\n",
        "  .option(\"multiLine\", True)\\\r\n",
        "  .json(parkingbay_filepath)\r\n",
        "sensordata_sdf = spark.read\\\r\n",
        "  .schema(sensordata_schema)\\\r\n",
        "  .option(\"badRecordsPath\", os.path.join(base_path, \"__corrupt\", \"MelbParkingSensorData\"))\\\r\n",
        "  .option(\"multiLine\", True)\\\r\n",
        "  .json(sensors_filepath)\r\n",
        "\r\n",
        "\r\n",
        "# Standardize\r\n",
        "t_parkingbay_sdf, t_parkingbay_malformed_sdf = s.standardize_parking_bay(parkingbay_sdf, load_id, loaded_on)\r\n",
        "t_sensordata_sdf, t_sensordata_malformed_sdf = s.standardize_sensordata(sensordata_sdf, load_id, loaded_on)\r\n",
        "\r\n",
        "# Insert new rows\r\n",
        "t_parkingbay_sdf.write.mode(\"append\").insertInto(\"interim.parking_bay\")\r\n",
        "t_sensordata_sdf.write.mode(\"append\").insertInto(\"interim.sensor\")\r\n",
        "\r\n",
        "# Insert bad rows\r\n",
        "t_parkingbay_malformed_sdf.write.mode(\"append\").insertInto(\"malformed.parking_bay\")\r\n",
        "t_sensordata_malformed_sdf.write.mode(\"append\").insertInto(\"malformed.sensor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 5. Observability: create log messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "parkingbay_count = t_parkingbay_sdf.count()\r\n",
        "sensordata_count = t_sensordata_sdf.count()\r\n",
        "parkingbay_malformed_count = t_parkingbay_malformed_sdf.count()\r\n",
        "sensordata_malformed_count = t_sensordata_malformed_sdf.count()\r\n",
        "\r\n",
        "\r\n",
        "final_message = f'Standardize : Completed load {pipelineruninfo}::[parkingbay_filepath::{parkingbay_filepath}]::[sensors_filepath:{sensors_filepath}]::[parkingbay_count:{parkingbay_count}]::[sensordata_count:{sensordata_count}]::[parkingbay_malformed_count:{parkingbay_malformed_count}]::[sensordata_malformed_count:{sensordata_malformed_count}]'\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 6. Observability: logging on App Insigths using OpenCensus Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "import logging\r\n",
        "import os\r\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\r\n",
        "from opencensus.ext.azure.log_exporter import AzureEventHandler\r\n",
        "#from pyspark.sql.session import SparkSession\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "# Enable App Insights\r\n",
        "aiLogger = logging.getLogger(\"ParkingSensorLogs-Standardize\")\r\n",
        "aiLogger.addHandler(AzureEventHandler(connection_string = 'InstrumentationKey=' + appi_key))\r\n",
        "#logger.addHandler(AzureLogHandler(connection_string = 'InstrumentationKey=' + appi_key))\r\n",
        "\r\n",
        "\r\n",
        "aiLogger.warning(\"Starting at: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\r\n",
        "properties = {'custom_dimensions': {'pipeline': pipelinename, 'run_id': loadid, 'parking count': parkingbay_count, 'sensor count': sensordata_count}}\r\n",
        "aiLogger.warning(final_message, extra=properties)\r\n",
        "# To query this log go to the Azure Monitor and run the following kusto query (if you are using the EventHandler)\r\n",
        "#customEvents\r\n",
        "#|order by timestamp desc\r\n",
        "# To query this log go to the Azure Monitor and run the following kusto query (if you are using the LogHandler)\r\n",
        "# traces\r\n",
        "#|order by timestamp desc\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 7. Observability logging on Log Analytics workspace using Log4J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "import logging\r\n",
        "import sys\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "sc = SparkSession.builder.getOrCreate()\r\n",
        "env = mssparkutils.env\r\n",
        "pipelineruninfo = f'[{pipelinename}]::[{loadid}]::[{env.getJobId()}]::[{env.getPoolName}]::[{env.getWorkspaceName}]::[{env.getUserId()}]'\r\n",
        "final_message = f'Standardize:Completed load::[parkingbay_filepath::{parkingbay_filepath}]::[sensors_filepath:{sensors_filepath}]::[parkingbay_count:{parkingbay_count}]::[sensor_count:{sensordata_count}]::[parkingbay_malformed_count:{parkingbay_malformed_count}]::[sensor_malformed_count:{sensordata_malformed_count}]'\r\n",
        "\r\n",
        "# Enable Log Analytics using Log4J\r\n",
        "log4jLogger = sc._jvm.org.apache.log4j\r\n",
        "logger = log4jLogger.LogManager.getLogger(\"ParkingSensorLogs-Standardize\")\r\n",
        "logger.info(pipelineruninfo)\r\n",
        "logger.info(final_message)\r\n",
        "\r\n",
        "# To query this log go to the log analytics workspace and run the following kusto query:\r\n",
        "# SparkLoggingEvent_CL\r\n",
        "# | where logger_name_s == \"ParkingSensorLogs-Standardize\"\r\n",
        "\r\n",
        "\r\n",
        "sc.stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": []
    }
  ]
}