{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848835c6-2bc3-4234-bc92-dfe7e5e27fc4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Set up for Parking Sensors\n",
    "\n",
    "TO DO: \n",
    "- Need to update the landing page references\n",
    "- Add unit test cases in a separate notebook\n",
    "\n",
    "About:\n",
    "\n",
    "- This notebook creates the database schemas and tables needed by parking sensors sample. See [parking sensors page](https://github.com/Azure-Samples/modern-data-warehouse-dataops/tree/feat/e2e-fabric-dataops-sample/e2e_samples/fabric_dataops_sample/README.md) for more details about Parking Sensor sample using Microsoft Fabric.\n",
    "\n",
    "Assumptions/Pre-requisites:\n",
    "\n",
    "- User must set appropriate flags in the environment in case \"case-sensitive\" names need to be maintained for tables and columns. \n",
    "    ```\n",
    "    spark.sql.caseSensitive: 'TRUE'\n",
    "    ```\n",
    "- Currently there is a known issue running cross workspace queries when workspace name has special characters. See [schema limitation](https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-schemas#public-preview-limitations) for more details. Avoid special characters if planning to query across workspaces with schema support. \n",
    "- The following assets are created by IaC step:\n",
    "    - The required files are staged under \"Files/\" of the lakehouse (to be added as a default lakehouse during runtime) as follows:\n",
    "        - Files/sc-adls-main/config/lakehouse_ddls.yaml \n",
    "        - Files/sc-adls-main/config/application.cfg (derived using application.cfg.template during ci/cd process). Ensure \"setup\" section is updated with the required parameters for this notebook.\n",
    "        - Files/sc-adls-main/reference/dim_date.csv\n",
    "        - Files/sc-adls-main/reference/dim_time.csv \n",
    "    - Environment with common library otel_monitor_invoker.py and its associated python dependencies\n",
    "    - Parking Sensor Lakehouse\n",
    "    - Datasource: ADLS made available as a shortcut in Parking Sensor Lakehouse\n",
    "    - Monitoring sink: AppInsights\n",
    "    - Secrets repo: Key vault to store AppInsights connection information\n",
    "\n",
    "- All Lakehouses have schema support enabled (in Public preview as of Nov, 2024).\n",
    "- Execution\n",
    "  - A default lakehouse is associated during runtime where the required files and data are already staged. Multiple ways of invoking:\n",
    "    - [Api call](https://learn.microsoft.com/fabric/data-engineering/notebook-public-api#run-a-notebook-on-demand)\n",
    "    - [Part of a data pipeline](https://learn.microsoft.com/fabric/data-engineering/author-execute-notebook#parameterized-session-configuration-from-a-pipeline)\n",
    "    - [Using `%run` from another notebook](https://learn.microsoft.com/fabric/data-engineering/author-execute-notebook#reference-run-a-notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba9a02-ac84-4173-b832-70ddb65544ee",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Parameters and Library imports\n",
    "\n",
    "### Reading parameters (external from Fabric pipeline or default values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb5192-d6c6-468d-8091-127ead10a32b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"defaultLakehouse\": {\n",
    "        \"name\": {\n",
    "            \"parameterName\": \"lakehouse_name\",\n",
    "            \"defaultValue\": \"{{ .lakehouse_name }}\"\n",
    "               } ,\n",
    "        \"id\": {\n",
    "            \"parameterName\": \"lakehouse_id\",\n",
    "            \"defaultValue\": \"{{ .lakehouse_id }}\"\n",
    "        } ,\n",
    "        \"workspaceId\": {\n",
    "            \"parameterName\": \"workspace_id\",\n",
    "            \"defaultValue\": \"{{ .workspace_id }}\"\n",
    "        }\n",
    "    },\n",
    "    \"mountPoints\": [\n",
    "        {\n",
    "            \"mountPoint\": \"/local_data\",\n",
    "            \"source\": {\n",
    "                \"parameterName\": \"local_mount\",\n",
    "                \"defaultValue\": \"abfss://{{ .workspace_id }}@onelake.dfs.fabric.microsoft.com/{{ .lakehouse_id }}/Files\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3feed-6727-484c-a437-02dcde99e165",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Unless `%%configure` is used to read external parameters - this cell should be the first one\n",
    "\n",
    "# This cell is tagged as Parameters cell. Parameters mentioned here are usually \\\n",
    "#    passed by the user at the time of notebook execution.\n",
    "# Ref: https://learn.microsoft.com/fabric/data-engineering/notebook-public-api#run-a-notebook-on-demand\n",
    "\n",
    "# Control how to run the notebook - \"all\" for entire notebook or \"module\" mode to use \\\n",
    "#      this notebook like a module (main execution will be skipped). Useful when performing\n",
    "#      testing using notebooks or functions from this notebook need to be called from another notebook.\n",
    "import configparser\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import otel_monitor_invoker as otel  # custom module part of env\n",
    "import yaml  # type: ignore\n",
    "from opentelemetry.trace import SpanKind\n",
    "from opentelemetry.trace.status import StatusCode\n",
    "\n",
    "# execution_mode = \"module\" will skip the execution of the main function. Use it for module like treatment\n",
    "#   \"all\" perform execution as well.\n",
    "execution_mode = \"all\"\n",
    "# Helpful if user wants to set a child process name etc. will be derived if not set by user\n",
    "job_exec_instance = \"\"\n",
    "# Helpful to derive any stage based globals\n",
    "env_stage = \"dev\"\n",
    "# Common config file path hosted on attached lakehouse - path relative to Files/\n",
    "config_file_path = \"sc-adls-main/config/application.cfg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3170fc-741f-4bfb-820c-43d9d26546f6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Validate input parameters\n",
    "in_errors = []\n",
    "if execution_mode not in [\"all\", \"module\"]:\n",
    "    in_errors.append(f\"Invalid value: {execution_mode = }. It must be either 'all' or 'module'.\")\n",
    "if not notebookutils.fs.exists(f\"Files/{config_file_path}\"):\n",
    "    in_errors.append(f\"Specified config - `Files/{config_file_path}` doesn't exist.\")\n",
    "\n",
    "if in_errors:\n",
    "    raise ValueError(f\"Input parameter validation failed. Errors are:\\n{in_errors}\")\n",
    "else:\n",
    "    print(\"Input parameter verification completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c196060-9e09-420d-93c7-809b4a2cd656",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Network mounts\n",
    "\n",
    "- Scope is set to Job/session - so these need to be run once per session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374da7d8-5ac9-48b4-88ce-6c563422882a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# -- Helps to read config files from onelake location\n",
    "runtime_context = notebookutils.runtime.context\n",
    "# Mount is configured already using %%configure.\n",
    "# notebookutils.fs.mount(\n",
    "#     source=f\"abfss://{runtime_context['currentWorkspaceId']}@onelake.dfs.fabric.microsoft.com/{runtime_context['defaultLakehouseId']}/Files\",\n",
    "#     mountPoint=\"/local_data\",\n",
    "#     extraConfigs={\"Scope\": \"job\"},\n",
    "# )\n",
    "local_data_mount_path = f'{notebookutils.fs.getMountPath(\"/local_data\")}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3fea57-3e3f-40cc-9fb5-59e595a11957",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Read user provided config values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c7bcb-0bce-4a1a-b69c-7ae4ecb4d19f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation())\n",
    "config.read(f\"{local_data_mount_path}/{config_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a353b5-92ec-4f24-81e4-7ff29e9a44f4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# When we config parser if the value is not present in the specified section, it will be\n",
    "#   read from \"DEFAULT\" section.\n",
    "config_section_name = \"setup\"\n",
    "process_name = config.get(config_section_name, \"process_name\")\n",
    "parking_ws = config.get(config_section_name, \"workspace_name\")\n",
    "parking_ws_id = config.get(config_section_name, \"workspace_id\")\n",
    "parking_lakehouse = config.get(config_section_name, \"lakehouse_name\")\n",
    "ddl_config_file = config.get(config_section_name, \"ddl_file\")  # relative path to Files/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631249bb-689d-4c69-ba5f-9ea139042ad1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Internal (derived) parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31711b30-031d-4790-9585-1833f6088217",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# default is micro-seconds, changing to milliseconds\n",
    "current_ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S%f\")[:-3]\n",
    "job_exec_instance = job_exec_instance if job_exec_instance else f\"{process_name}#{current_ts}\"\n",
    "execution_user_name = runtime_context[\"userName\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef283ac8-dd33-4568-9a56-e9a8aad5c4b2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Monitoring and observability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34deea-70bb-4316-85dc-08e38681da76",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### AppInsights connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ff164-80cc-44e3-b4af-f4809a57e77c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "connection_string = notebookutils.credentials.getSecret(\n",
    "    config.get(\"keyvault\", \"uri\"), config.get(\"otel\", \"appinsights_connection_name\")\n",
    ")\n",
    "otlp_exporter = otel.OpenTelemetryAppInsightsExporter(conn_string=connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8bc908-8c78-4335-ab06-13f545245374",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Populate resource information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4e588-f46f-40dd-806e-078c7c994e0a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Resource references\n",
    "# - Naming conventions: https://opentelemetry.io/docs/specs/semconv/general/attribute-naming/\n",
    "# - For a complete list of reserved ones: https://opentelemetry.io/docs/concepts/semantic-conventions/\n",
    "#  NOTE: service.namespace,service.name,service.instance.id triplet MUST be globally unique.\n",
    "#     The ID helps to distinguish instances of the same service that exist at the same time\n",
    "#     (e.g. instances of a horizontally scaled service)\n",
    "resource_attributes = {\n",
    "    # ---------- Reserved attribute names\n",
    "    \"service.name\": config.get(config_section_name, \"service_name\"),\n",
    "    \"service.version\": config.get(config_section_name, \"service_version\"),\n",
    "    \"service.namespace\": \"parking-sensor\",\n",
    "    \"service.instance.id\": notebookutils.runtime.context[\"activityId\"],\n",
    "    \"process.executable.name\": process_name,\n",
    "    \"deployment.environment\": env_stage,\n",
    "    # ---------- custom attributes - we can also add common attributes like appid, domain id etc\n",
    "    #     here or get them from process reference data using process name as the key.\n",
    "    # runtime context has a lot if useful info - adding it as is.\n",
    "    \"jobexec.context\": f\"{notebookutils.runtime.context}\",  # convert to string otherwise it will fail\n",
    "    \"jobexec.cluster.region\": spark.sparkContext.getConf().get(\"spark.cluster.region\"),\n",
    "    \"jobexec.app.name\": spark.sparkContext.getConf().get(\"spark.app.name\"),\n",
    "    \"jobexec.instance.name\": job_exec_instance,\n",
    "}\n",
    "\n",
    "# Typically, logging is performed within the context of a span.\n",
    "#   This allows log messages to be associated with trace information through the use of trace IDs and span IDs.\n",
    "#   As a result, it's generally not necessary to include resource information in log messages.\n",
    "# Note that trace IDs and span IDs will be null when logging is performed outside of a span context.\n",
    "log_attributes = {\"jobexec.instance.name\": job_exec_instance}\n",
    "trace_attributes = resource_attributes\n",
    "\n",
    "tracer = otlp_exporter.get_otel_tracer(trace_resource_attributes=trace_attributes, tracer_name=f\"tracer-{process_name}\")\n",
    "logger = otlp_exporter.get_otel_logger(\n",
    "    log_resource_attributes=log_attributes,\n",
    "    logger_name=f\"logger-{process_name}\",\n",
    "    add_console_handler=False,\n",
    ")\n",
    "logger.setLevel(\"INFO\")  # default is WARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e216da-6f45-4f0d-83fd-68fce17eb9ec",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b860f7-3ab4-40c4-a277-41e15c7d8a52",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Code functions\n",
    "\n",
    "- When using %run we can expose these functions to the calling notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd12d8-92cc-4f36-a1d5-097aab9ba693",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_lakehouse_details(lakehouse_name: str) -> dict:\n",
    "    logger.info(\"Performing lakehouse existence check.\")\n",
    "    try:\n",
    "        details = notebookutils.lakehouse.get(name=lakehouse_name)\n",
    "    except Exception:\n",
    "        logger.exception(f\"Specified lakehouse - {lakehouse_name} doesn't exist. Aborting..\")\n",
    "        raise\n",
    "    return details\n",
    "\n",
    "\n",
    "def create_schemas(schema_list: list) -> None:\n",
    "    errors = []\n",
    "    try:\n",
    "        for schema in schema_list:\n",
    "            logger.info(f\"Recreating schema: {parking_lakehouse}.{schema}\")\n",
    "            spark.sql(f\"create schema if not exists {parking_lakehouse}.{schema}\")\n",
    "    except Exception:\n",
    "        error_msg = f\"Failed to create schema{parking_lakehouse}.{schema}\"\n",
    "        errors.append(error_msg)\n",
    "        logger.exception(error_msg)\n",
    "    if errors:\n",
    "        raise ValueError(f\"There are errors:\\n{errors}\")\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_tables(ddl_dict: dict, lakehouse_table_path: str, lakehouse_file_path: str) -> None:\n",
    "    # Update in future - special characters in workspace are causing issues\n",
    "    #  use f\"{parking_ws}.{parking_lakehouse}\"  if possible\n",
    "    lake_house_prefix = parking_lakehouse\n",
    "    errors = []\n",
    "    for t in ddl_dict[\"table_types\"]:\n",
    "        logger.info(f\"\\nProcessing type: {t['type']}\")\n",
    "        for table in t[\"tables\"]:\n",
    "            table_name = table[\"table_name\"].strip()\n",
    "            table_fqn = f\"{lake_house_prefix}.{table_name}\"\n",
    "            logger.info(f\"\\t- Processing table:{table_fqn}\")\n",
    "            data_file = table.get(\"data_file\", None)\n",
    "            create_sql = table.get(\"create_sql\", None)\n",
    "            drop_sql = f\"DROP TABLE IF EXISTS {table_fqn}\"\n",
    "            try:\n",
    "                if data_file:\n",
    "                    data_file = f\"{lakehouse_file_path}/{data_file}\".strip()\n",
    "                    spark.read.csv(data_file, header=True).write.format(\"delta\").mode(\"overwrite\").save(\n",
    "                        f\"{lakehouse_table_path}/{table_name.replace('.', '/')}\"\n",
    "                    )\n",
    "                elif create_sql:\n",
    "                    create_sql = create_sql.format(workspace_name=parking_ws, lakehouse_name=parking_lakehouse)\n",
    "                    spark.sql(drop_sql)\n",
    "                    spark.sql(create_sql)\n",
    "                else:\n",
    "                    raise ValueError(\"Either `data_file` or `create_sql` must be defined.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.exception(f\"\\t\\t- Failed processing table:{table_fqn}\")\n",
    "                errors.append(\n",
    "                    {\n",
    "                        \"table_name\": table_fqn,\n",
    "                        \"data_file\": data_file,\n",
    "                        \"drop_sql\": drop_sql,\n",
    "                        \"create_sql\": create_sql,\n",
    "                        \"error\": e,\n",
    "                    }\n",
    "                )\n",
    "    if errors:\n",
    "        raise ValueError(\n",
    "            f\"Encountered errors while creating tables. Errors are\\n{json.dumps(errors, indent=2, default=str)}\"\n",
    "        )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b82efa2-4e54-46d6-bcb5-a96219dffa9f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    root_span_name = f\"root#{process_name}#{current_ts}\"\n",
    "\n",
    "    with tracer.start_as_current_span(root_span_name, kind=SpanKind.INTERNAL) as root_span:\n",
    "        try:\n",
    "            root_span.add_event(\n",
    "                name=\"010-verify-lakehouse\",\n",
    "                attributes={\"lakehouse_name\": parking_lakehouse},\n",
    "            )\n",
    "            lh_details = get_lakehouse_details(parking_lakehouse)\n",
    "            lh_table_path = f'{lh_details[\"properties\"][\"abfsPath\"]}/Tables'\n",
    "            lh_file_path = f'{lh_details[\"properties\"][\"abfsPath\"]}/Files'\n",
    "\n",
    "            ddl_file_path = f\"{local_data_mount_path}/{ddl_config_file}\"\n",
    "            root_span.add_event(name=\"020-read-ddls\", attributes={\"ddl_file\": ddl_file_path})\n",
    "            with open(ddl_file_path, \"r\") as d:\n",
    "                ddl_dict = yaml.safe_load(d.read())\n",
    "\n",
    "            root_span.add_event(name=\"030-schema-creation\", attributes={})\n",
    "            create_schemas([\"dw\", \"lnd\", \"interim\", \"malformed\"])\n",
    "\n",
    "            root_span.add_event(name=\"040-table-creation\", attributes={})\n",
    "            create_tables(ddl_dict, lh_table_path, lh_file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"{process_name} process failed with error {e}\"\n",
    "            logger.exception(error_message)\n",
    "            root_span.set_status(StatusCode.ERROR, error_message)\n",
    "            root_span.record_exception(e)\n",
    "            raise\n",
    "        else:\n",
    "            root_span.set_status(StatusCode.OK)\n",
    "            logger.info(f\"{process_name} process is successful.\")\n",
    "        finally:\n",
    "            logger.info(f\"\\n** {process_name} process is complete. Check the logs for execution status. **\\n\\n\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae060f3-e2d6-4440-95cd-72d2e08205f3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a30277-8696-4731-859a-c1b61615ea94",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Apply logic here incase this notebook need to be used as a library\n",
    "\n",
    "# dependencies\n",
    "# | where name hasprefix \"root#nb-020\"\n",
    "# //\n",
    "# exceptions\n",
    "# //\n",
    "# traces\n",
    "\n",
    "\n",
    "if execution_mode == \"all\":\n",
    "    print(f\"{execution_mode = }. Proceeding with the code execution.\")\n",
    "    main()\n",
    "else:\n",
    "    print(f\"Skipping the main function execution as {execution_mode = } and running it like a code module.\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "{{ .environment_id }}",
    "workspaceId": "{{ .workspace_id }}"
   },
   "lakehouse": {
    "default_lakehouse": "{{ .lakehouse_id }}",
    "default_lakehouse_name": "{{ .lakehouse_name }}",
    "default_lakehouse_workspace_id": "{{ .workspace_id }}"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {
    "59525826-9698-453b-8e35-917a9874cc31": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "count",
        "binsNumber": 10,
        "categoryFieldKeys": [
         "0"
        ],
        "chartType": "bar",
        "evaluatesOverAllRecords": false,
        "isStacked": false,
        "seriesFieldKeys": [
         "0"
        ],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "is_jupyter": false,
      "language": "scala",
      "table": {
       "rows": [
        {
         "0": "/synfs/notebook/f0f56cd7-a885-48fc-8627-485b268b35c7/local_data",
         "1": "/local_data",
         "2": "job",
         "3": "abfss://83c75fb6-942b-47bd-a10b-2c8107af44df@onelake.dfs.fabric.microsoft.com/d806621d-5151-4378-9297-4bc212333e91/Files",
         "4": "Lakehouse"
        }
       ],
       "schema": [
        {
         "key": "0",
         "name": "localPath",
         "type": "string"
        },
        {
         "key": "1",
         "name": "mountPoint",
         "type": "string"
        },
        {
         "key": "2",
         "name": "scope",
         "type": "string"
        },
        {
         "key": "3",
         "name": "source",
         "type": "string"
        },
        {
         "key": "4",
         "name": "storageType",
         "type": "string"
        }
       ],
       "truncated": false
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
