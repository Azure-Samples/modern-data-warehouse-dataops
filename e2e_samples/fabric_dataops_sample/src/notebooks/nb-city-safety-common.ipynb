{"cells":[{"cell_type":"markdown","id":"37f9db1d-b759-4db8-80e6-75cca15d7b3e","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# About this Notebook\n","\n","- This notebook demonstrates the common functions and their usage by the main ETL in [nb-city-safety.ipynb](./nb-city-safety.ipynb) notebook.\n","- Unit test cases for this notebook are created in another notebook - [test_nb-city-safety-common.ipynb](../../tests/test_nb-city-safety-common.ipynb).\n","\n","To DO:\n","- Ensure type formatting\n","- Addition of doc strings"]},{"cell_type":"markdown","id":"ffa8836d-829a-47cf-be9c-a0a0949804c3","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## External parameters"]},{"cell_type":"code","execution_count":null,"id":"99113c4a-f012-49bb-aef4-aa5a23e7acd0","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["common_execution_mode = \"testing\"  # Global for controlling the execution of notebook cells."]},{"cell_type":"code","execution_count":null,"id":"4438c8ec-2322-4650-a5ce-b17e81dfa4fe","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Enable the following for local testing\n","if common_execution_mode == \"testing\":\n","    onelake_table_path = \"dummy_path\"\n","    table_name = \"test_table\"\n","    onelake_name = \"onelake_name\"\n","    workspace_name = \"dummy_workspace\"\n","    lakehouse_name = \"dummy_lakehouse\"\n","    job_exec_instance = \"dummy_exec_id\"\n","    user_name = \"current_user\""]},{"cell_type":"markdown","id":"57381b1a-5cf4-4a67-80dd-add9f47db85a","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Formatting the Notebook - Run only when developing the Notebook\n","Reference: https://learn.microsoft.com/en-us/fabric/data-engineering/author-notebook-format-code#extend-fabric-notebooks \n"]},{"cell_type":"code","execution_count":null,"id":"e7b9b33b-d534-48b2-9aad-cc38d4785e76","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#  %load_ext jupyter_black\n","if common_execution_mode == \"testing\":\n","    import jupyter_black\n","\n","    jupyter_black.load()"]},{"cell_type":"markdown","id":"1d52552a-6461-411c-9d3a-1407d94e37ca","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Common functions\n","\n","- Note that there are some additional imports in the statements below - which are not used here but used by main code. This is *NOT* a good practice. Generally, imports should be performed where you are using them."]},{"cell_type":"code","execution_count":null,"id":"e2f061d7-814e-48a6-ae3b-2a1bfc47c63b","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import sys\n","from delta.tables import DeltaTable\n","import logging\n","from typing import Optional, Union\n","from opentelemetry import trace\n","from opentelemetry.trace.status import StatusCode\n","from opentelemetry.trace import SpanKind\n","import requests\n","import urllib.parse\n","import re\n","from pyspark.sql.functions import (\n","    lit,\n","    to_utc_timestamp,\n","    current_timestamp,\n","    unix_timestamp,\n","    avg,\n","    max,\n","    min,\n","    sum,\n","    count,\n",")"]},{"cell_type":"markdown","id":"02112623-95ac-4bb1-a0c8-37cc60e5ac39","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Common utility functions\n","\n","- NOTE: Consider moving this to a sepearate file"]},{"cell_type":"code","execution_count":null,"id":"27e48a43-f090-4301-8f1d-458a7f22417e","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def query_app_insights(\n","    log_workspace_id: str, query: str, timedelta_in_mins: Optional[int] = 15\n",") -> Union[None, int]:\n","    # time delta ensures we are only looking in to data from past few mins specified\n","    try:\n","        response = client.query_workspace(\n","            log_workspace_id, query, timespan=timedelta(minutes=timedelta_in_mins)\n","        )\n","        if response.status == LogsQueryStatus.SUCCESS:\n","            data = response.tables\n","            # print(data[0].rows)\n","            count = data[0].rows[0][0]\n","            column_names = data[0].columns\n","        else:\n","            # LogsQueryPartialResult - handle error here\n","            error = response.partial_error\n","            data = response.partial_data\n","            count = None\n","            print(error)\n","\n","    except HttpResponseError as err:\n","        print(\"something fatal happened\")\n","        print(err)\n","    else:\n","        return count\n","\n","\n","def store_unit_test_results(unit_tests_results: object) -> None:\n","    ansi_escape = re.compile(r\"\\x1b\\[[0-9;]*m\")\n","    cleaned_text = ansi_escape.sub(\n","        \"\", \" \".join(unit_tests_results.stdout.split(\"\\n\")[-3:])\n","    )\n","    cleaned_text = cleaned_text.replace(\"=\", \"\")\n","    errors = re.findall(r\"(\\d+)\\s+error\", cleaned_text)\n","    num_errors = int(errors[0]) if errors else 0\n","    passes = re.findall(r\"(\\d+)\\s+passed\", cleaned_text)\n","    num_passes = int(passes[0]) if passes else 0\n","    fails = re.findall(r\"(\\d+)\\s+failed\", cleaned_text)\n","    num_fails = int(fails[0]) if fails else 0\n","    runtime = re.findall(r\"\\s+in\\s+(.+)\\s+\", cleaned_text)\n","    runtime = runtime[0].strip() if runtime else \"N/A\"\n","\n","    # TO DO: Store these results somewhere\n","    # Details to include: workspace details, deployment identifierers release name, configs/params used, user information, test statuses etc\n","    print(f\"{num_errors =}, {num_passes =}, {num_fails =}, {runtime =}\")\n","\n","\n","# Getting workspace id - using Fabric REST APIS\n","def make_fabric_api_call(token: str, url: str, call_type: str, payload: str) -> object:\n","    headers = {\"Authorization\": f\"Bearer {token}\"}\n","    try:\n","        if call_type == \"get\":\n","            response = requests.get(url, data=payload, headers=headers)\n","        elif call_type == \"put\":\n","            response = requests.put(url, data=payload, headers=headers)\n","        else:\n","            raise ValueError(\n","                f\"Invalid {call_type = }. It must be either 'get' or 'put'.\"\n","            )\n","    except Exception as e:\n","        logger.error(f\"Failed with error {e}\")\n","        raise\n","    else:\n","        ## print(f\"{response.status_code = }\\n\\n{response.content = }\\n\\n{response.headers = }\\n\\n{response.json() = }\\n\\n{response.text =}\")\n","        return response\n","\n","\n","def verify_onelake_connection():\n","    cur_span = trace.get_current_span()\n","\n","    # Check onelake existence - otherwise abort notebook execution\n","    error_message = f\"Specfied lakehouse table path {onelake_table_path} doesn't exist. Ensure onelake={onelake_name}, workspace={workspace_name} and lakehouse={lakehouse_name} exist.\"\n","    try:\n","        if not (notebookutils.fs.exists(onelake_table_path)):\n","            raise ValueError(\n","                \"Encountered error while checking for Lakehouse table path specified.\"\n","            )\n","    except Exception as e:\n","        logger.exception(f\"Error message: {e}\")\n","        cur_span.record_exception(e)\n","        cur_span.set_status(StatusCode.ERROR, \"Onelake connection verification failed.\")\n","        # no further execution but Session is still active\n","        notebookutils.notebook.exit(error_message)\n","    else:\n","        cur_span.set_status(StatusCode.OK)\n","        logger.info(\n","            f\"Target table path: {onelake_table_path} is valid and exists.\\nListing source data contents to check connectivity\\n{notebookutils.fs.ls(wasbs_path)}\"\n","        )\n","\n","\n","def identify_table_load_mode(table_name: str, span_obj: object) -> bool:\n","\n","    # Preferred option - Assuming default lakehouse is not set, checking based on the delta path\n","    load_mode = (\n","        \"append\"\n","        if DeltaTable.isDeltaTable(spark, f\"{onelake_table_path}/{table_name}\")\n","        else \"overwrite\"\n","    )\n","\n","    # getting span object as an argument - as opposed to using trace.get_current_span() to find current span.\n","    span_obj.set_attribute(\"load_mode\", load_mode)\n","\n","    return load_mode\n","\n","\n","def delete_delta_table(table_name: str) -> bool:\n","\n","    delta_table_path = f\"{onelake_table_path}/{table_name}\"\n","\n","    if notebookutils.fs.exists(delta_table_path):\n","        logger.info(\n","            f\"Attempting to delete existing delta table with {delta_table_path = }....\"\n","        )\n","\n","        try:\n","            notebookutils.fs.rm(dir=delta_table_path, recurse=True)\n","        except Exception as e:\n","            logger.error(f\"Deletion failed with the error:\\n===={e}\\n=====\")\n","            raise\n","        else:\n","            logger.info(f\"Deleted existing delta table: {table_name}.\")\n","    else:\n","        logger.info(f\"The specified delta table doesn't exist. No need for deletion.\")"]},{"cell_type":"markdown","id":"8f277198-3df7-44b3-90c4-187938957f69","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Common functions specific to this business process\n","\n","NOTE: These functions assume all required parameters are set outside of this code. Note that `tracer` is used as a decorator for one of the functions. So, this must be already set before running these functions."]},{"cell_type":"code","execution_count":null,"id":"fb9a1d45-05a0-42e0-889c-bc796b6f4833","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def transform_data(city: str, data_frame: object) -> object:\n","\n","    # Need timezone to convert to UTC\n","    if city in (\"Boston\", \"NewYorkCity\"):\n","        timezone = \"America/New_York\"\n","    elif city in (\"Seattle\", \"SanFrancisco\"):\n","        timezone = \"America/Los_Angeles\"\n","    else:\n","        timezone = \"America/Chicago\"\n","\n","    data_frame = (\n","        data_frame.withColumn(\n","            \"dateTimeUTC\", to_utc_timestamp(data_frame.dateTime, timezone)\n","        )\n","        .withColumn(\"City\", lit(city))\n","        .withColumn(\"jobExecId\", lit(job_exec_instance))\n","        .withColumn(\n","            \"lastUpdateUTC\",\n","            to_utc_timestamp(\n","                current_timestamp(), spark.conf.get(\"spark.sql.session.timeZone\")\n","            ),\n","        )\n","        .withColumn(\"lastUpdateUser\", lit(user_name))\n","    )\n","\n","    return data_frame\n","\n","\n","def city_data_etl(table_name: str, cities: tuple, current_span: object):\n","\n","    delta_table_path = f\"{onelake_table_path}/{table_name}\"\n","\n","    for city in cities:\n","        current_span.add_event(name=f\"etl start for {city}\", attributes=None)\n","        logger.info(f\"ETL started for {city = }.\")\n","\n","        #  creating child-spans for each city using \"with\" context\n","        with tracer.start_as_current_span(f\"etl_steps_city-{city}\") as city_span:\n","\n","            city_span.set_attribute(\"city_name\", city)\n","            logger.info(f\"\\t Data Extraction in progress.\")\n","            city_calls_data_path = f\"{wasbs_path}/city={city}\"\n","            city_span.add_event(\n","                name=\"Data Extraction in progress.\",\n","                attributes={\"etl.city.source_path\": city_calls_data_path},\n","            )\n","            city_calls_df = spark.read.parquet(city_calls_data_path)\n","\n","            record_count = city_calls_df.count()\n","            logger.info(f\"\\t Read {record_count} records for {city = }.\")\n","            city_span.add_event(\n","                name=\"Data transformation in progress.\",\n","                attributes={\"record_count\": record_count},\n","            )\n","            city_calls_df = transform_data(city, city_calls_df)\n","\n","            delta_mode = identify_table_load_mode(table_name, city_span)\n","            logger.info(f\"\\t Data loading in inprogress using {delta_mode} mode.\")\n","            city_span.add_event(\n","                name=\"Data loading in progress.\", attributes={\"delta_mode\": delta_mode}\n","            )\n","            city_calls_df.write.format(\"delta\").mode(delta_mode).save(delta_table_path)\n","\n","            city_span.set_status(StatusCode.OK)\n","\n","        current_span.add_event(name=f\"etl end for {city}\", attributes=None)\n","        logger.info(f\"ETL completed for {city = }.\")\n","\n","\n","@tracer.start_as_current_span(f\"etl_steps\") # creating span using a decorator - `tracer` must be defined and globally available\n","def etl_steps(table_name: str, cities: list, cleanup: Optional[bool] = True) -> None:\n","\n","    current_span = trace.get_current_span()\n","    current_span.set_attributes({\"etl.table_name\": table_name, \"etl.cleanup\": cleanup})\n","\n","    # Optionally delete existing contents\n","    delta_table_path = f\"{onelake_table_path}/{table_name}\"\n","    if cleanup:\n","        delete_delta_table(table_name)\n","        logger.info(\n","            f\"A new delta table '{table_name}' will be created with {delta_table_path = }\"\n","        )\n","    else:\n","        logger.info(\"No request for cleanup. Proceeding to ETL steps.\")\n","\n","    city_data_etl(table_name, cities, current_span)\n","\n","    logger.info(\n","        f\"\\n=====\\nCity safety data is loaded into {table_name =} for {cities =}\\n=====\"\n","    )\n","    current_span.set_status(StatusCode.OK)\n","\n","    return None\n","\n","\n","def gather_city_level_metrics(table_name: str, counter: object) -> None:\n","\n","    delta_table = spark.read.format(\"delta\").load(f\"{onelake_table_path}/{table_name}\")\n","    logger.info(f\"Gathering metrics for: {table_name =} where {job_exec_instance = }\")\n","    city_metrics = (\n","        delta_table.filter(delta_table.jobExecId == job_exec_instance)\n","        .groupBy(\"city\")\n","        .agg(count(\"*\").alias(\"count\"))\n","    )\n","\n","    # add the metric\n","    counter.add(\n","        amount=1,\n","        attributes={\n","            \"record_count_total\": city_metrics.agg(sum(\"count\")).collect()[0][0]\n","        },\n","    )\n","    display(city_metrics)\n","    logger.info(f\"total:{city_metrics.agg(sum('count')).collect()[0][0]}\")\n","\n","    return None"]},{"cell_type":"markdown","id":"f54b9a93-5773-4cbd-8f01-b8c25c34a77f","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Control the execution flow\n","\n","- Control the behavior of the execution when using as notebook as opposed to a python script/module. The following code allows users to run only function definitions but not any other execution steps based on user arguments.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"6ac398d1-d36b-433f-bdf7-3fe4f58cc867","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["if __name__ == \"__main__\":\n","\n","    if common_execution_mode == \"testing\":\n","        run_mode1 = common_execution_mode  # setting a dummy parameter - this can be seen by the calling notebook\n","        # your testing related code goes here. These can be unit testcases if you are using same notebook for testing as well.\n","        print(f\"{common_execution_mode = }. '__main__' code will not be run.\")\n","    else:\n","        run_mode2 = common_execution_mode  # setting a dummy parameter - this can be seen by the calling notebook\n","        # your non-testing related goes here.\n","        print(f\"{common_execution_mode = }. '__main__' code will be run.\")"]}],"metadata":{"dependencies":{"environment":{"environmentId":"2b47b55d-9024-4737-a951-4a9ed5bff8a8","workspaceId":"91d4b7a6-b9b8-411c-bc27-e3be90555509"},"lakehouse":{"default_lakehouse":"f229e7c5-5cac-43fc-b794-91b23d484f6e","default_lakehouse_name":"lakehousefabrichack01","default_lakehouse_workspace_id":"91d4b7a6-b9b8-411c-bc27-e3be90555509","known_lakehouses":[{"id":"f229e7c5-5cac-43fc-b794-91b23d484f6e"}]}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
