{"cells":[{"cell_type":"markdown","id":"eeda8435-14f2-4a8e-96fa-3d396c7e48b3","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# City Safety Data - ETL - Sample"]},{"cell_type":"markdown","id":"8b7978a5","metadata":{},"source":["To DO:\n","- Ensure type formatting\n","- Addition of doc strings\n","-----------------------------------------------------------------------------\n","----------------------------------------------------------------------------"]},{"cell_type":"markdown","id":"4ed23f7c-61c0-4790-b749-46cebdfa7ca2","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## About this Notebook"]},{"cell_type":"markdown","id":"c79df08e-52c4-41b8-bd25-5bb4d3d4e6d7","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["Refer to [readme file](./README.md##sample---city-safety-data) for details on the processing logic, high level flow etc. This notebook shows examples of some key aspects to consider which are listed below while building Fabric notebook. Unit testcases written for this notebook are run as part of the Continuous Integration (CI) process before any code changes are merged with main branch code. \n","\n","<<<TO DO: Need to add link to Anuj's scripts here which explains the CI process.>>>\n","\n","- [Enabling formatting](https://learn.microsoft.com/fabric/data-engineering/author-notebook-format-code#extend-fabric-notebooks)\n","- [Making the code configuration driven](../../docs/ThingsToConsiderForDataProcesses.md#make-the-code-configuration-driven)\n","- [Organizing code](../../docs/ThingsToConsiderForDataProcesses.md#organize-code)\n","- [Making the system observable](../../docs/MonitoringAndObservabilityUsingOpenTelemetry.md)\n","- [Performing unit testing](../../docs/DataTesting.md)\n"]},{"cell_type":"markdown","id":"cd4dbe27","metadata":{},"source":["## Things to consider - general guidance and best practices"]},{"cell_type":"markdown","id":"e0c04e9c","metadata":{},"source":["Refer to [things to consider for data projects](../../docs/ThingsToConsiderForDataProcesses.md) for details on a few things to consider while building a data process/system. Please note that this is not an exhaustive list but enough for a good start of a data project. \n","\n","The following sections show sample steps on how to achieve what has been mentioned in the above link."]},{"cell_type":"markdown","id":"59ce5d3c","metadata":{},"source":["## Formatting the Notebook - Run only when developing the Notebook\n","\n","See [formatting notebook](https://learn.microsoft.com/fabric/data-engineering/author-notebook-format-code#extend-fabric-notebooks) for details.\n","\n","```python\n","import jupyter_black\n","jupyter_black.load()\n","```"]},{"cell_type":"markdown","id":"b2542d6b-42f4-4adc-ae74-58e60f133925","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Parameters\n","\n","See [making the code configuration driven](../../docs/ThingsToConsiderForDataProcesses.md#make-the-code-configuration-driven) for details.\n","\n","This example shows combination of two options: \n","1. Sending default lake house information using run item api call and \n","2. using parameters cell and using absolute paths."]},{"cell_type":"markdown","id":"01329d49-5f87-46bb-960f-1955e27ab94b","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["#### External parameters\n","\n","Note that below cell is marked as a [paramter cell](https://learn.microsoft.com/fabric/data-engineering/author-execute-notebook#designate-a-parameters-cell)."]},{"cell_type":"code","execution_count":null,"id":"03075e96-b60f-4561-92f2-d9c14ae13916","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"outputs":[],"source":["# ------------------- Keep Only External parameters in this cell. --------------------------------------------\n","execution_mode = \"all\"  # \"all\" to run including tests; \"normal\" to exclude tests; \"module\" to exclude execution of code\n","common_execution_mode = \"normal\"\n","config_file_path = f\"{notebookutils.nbResPath}/builtin/city_safety.cfg\"\n","env_stage = \"dev\"\n","process_name = None\n","user_name = None\n","cleanup_flag = False  # True for table re-creation otherwise append\n","otel_setup_type = \"monitor\"  # allowed values are \"monitor\" for AppInsights as target using OTEL SDK for Azure monitor; and \"collector\" for OTEL collector based setup with configureable target/s.\n","# This could be treated as a business friendly correlatation id.\n","job_exec_instance = None\n","# Use either override_config_file_path or param_override\n","override_config_file_path = None  # Related to Option 1 in param override\n","param_override_flag = True  # Related to Option 2 in param override"]},{"cell_type":"code","execution_count":null,"id":"483f98c3-e476-40d8-b37b-d134a6e075cc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import json\n","from datetime import datetime\n","import configparser\n","\n","config = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation())\n","# current_ts will act as a uniq execution tag when used in names\n","#  - Optionally bind with user input to identify child invocations\n","current_ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")"]},{"cell_type":"markdown","id":"c55900ac-30c7-45a2-b6f6-fe117fc9cdb8","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["#### Parameter overrides\n","\n","- For config values, you can use same config but write code to replace what values to use - pay attention to interpolation."]},{"cell_type":"code","execution_count":null,"id":"5ba22adb-7e10-40a8-a277-1ee8e2aa2535","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# ================= Overides for local testing  =================\n","# In this example - we are using both options together.\n","\n","# ---------- Option 1 - Using a local override file\n","if override_config_file_path:  # Avoid accidential usage of overrides\n","    config_files = [\n","        config_file_path,\n","        override_config_file_path,\n","    ]  # order is important - last file overwrites the previous one\n","else:\n","    config_files = config_file_path\n","\n","config.read(config_files)\n","# ---------- Option 2 - Directly override the values after reading the external config file\n","# Avoid accidential usage of overrides\n","if param_override_flag is not None and str(param_override_flag).lower() == \"true\":\n","    config[\"target\"][\"lakehouse_table_name\"] = \"tbl_city_safety_data_test\"\n","    # allowed values for cities (\"Boston\", \"Chicago\", \"NewYorkCity\", \"Seattle\", \"SanFrancisco\")\n","    config[\"source\"][\"cities\"] = json.dumps((\"Boston\",))\n","    config[\"otel\"][\"log_level\"] = \"INFO\""]},{"cell_type":"markdown","id":"24f9efc5-edff-4a1b-9fab-866314ce46f0","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["#### Additional paramater defaults\n","\n","- Handle the scenario where user hasn't provided all the parametes needed. The following parameters help us identifying the data related to an execution. Make sure those are set to some meaningful values."]},{"cell_type":"code","execution_count":null,"id":"f57bf6a6-3414-40a4-ba93-1ad9b63a79c5","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Set default values if not set in the config or by the user.\n","# - These will help us identifying data related to a run.\n","process_name = (\n","    process_name\n","    if process_name\n","    else config[\"process\"].get(\n","        \"process_name\", notebookutils.runtime.context[\"activityId\"]\n","    )\n",")\n","job_exec_instance = (\n","    job_exec_instance if job_exec_instance else f\"{process_name}#{current_ts}\"\n",")\n","# mssparkutils.env* will not work in runtime 1.3\n","user_name = user_name if user_name else mssparkutils.env.getUserName()"]},{"cell_type":"markdown","id":"f9920227-6dab-4dbe-b805-50524bbd4af0","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Static/reference data\n","\n","See [reference data](../../docs/ThingsToConsiderForDataProcesses.md#staticreference-data) section for details."]},{"cell_type":"markdown","id":"4209a9ea-ce58-421b-b1ea-9ce476f24654","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Monitoring and observability\n","\n","This example uses OpenTelemetry for observability. Either collector or SDK can be used based on the user input. If using collector option, ensure that OTEL collector is setup and running and this notebook is allowed to communicate with it.\n","\n","See [Observability in data projects](../../docs/MonitoringAndObservabilityUsingOpenTelemetry.md) for details on observability.\n","\n","See [OpenTelemetry implementation](../../docs/MonitoringAndObservabilityUsingOpenTelemetry.md#using-opentelemetry-for-monitoring-and-observability) for details on Opentelemetry implementation details."]},{"cell_type":"markdown","id":"a334a64d","metadata":{},"source":["**< TO DO >\n","Need to work with Mani on authentication and extensions for collector optin**"]},{"cell_type":"markdown","id":"b3c1b0d7-ec20-4bf1-980b-741dc6d09691","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["#### Open telemetry library"]},{"cell_type":"markdown","id":"37ddfb83","metadata":{},"source":["TO DO:\n","\n","Currently code assumes the scripts ([otel_collector_invoker](../../src/fabric_dataops_python_modules/src/otel_collector_invoker.py),[otel_monitor_invoker](../../src/fabric_dataops_python_modules/src/otel_monitor_invoker.py) ) are added as notebook resources. This need to be updated based on the final place fot these files."]},{"cell_type":"code","execution_count":null,"id":"9a7474c4-a9d4-478a-9647-0743e51c977d","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Run only once per environement - tracer, logger, meter should be global. Details:\n","# - Applications should generally use a single global TracerProvider, and\n","#   use either implicit or explicit context propagation consistently throughout.\n","#   See following sections to see different ways of creating traces.\n","#   Reference: https://opentelemetry-python.readthedocs.io/en/stable/api/trace.html\n","\n","if otel_setup_type == \"collector\":\n","    # OTEL Collector based version - Collector based impletation with configurable targets\n","    #  Ensure collector is setup, running and accepting requests from the Notebook.\n","    import builtin.otel_invoker as otel_invoker\n","\n","    otlp_exporter = otel_invoker.OpenTelemetryExporter(\n","        config[\"otel\"][\"port_type\"], config[\"otel\"][\"otel_collect_ip\"]\n","    )\n","elif otel_setup_type == \"monitor\":\n","    # OTEL Azure Monitor based version - used OpenTelemetry SDK based connection to Azure AppInsigths\n","    import builtin.otel_monitor_invoker as otel_invoker\n","\n","    connection_string = notebookutils.credentials.getSecret(\n","        config[\"keyvault\"][\"uri\"], config[\"otel\"][\"appinsights_connection_name\"]\n","    )\n","    otlp_exporter = otel_invoker.OpenTelemetryAppInsightsExporter(\n","        conn_string=connection_string\n","    )\n","else:\n","    raise ValueError(\n","        \"Given {otel_setup_type =} is not valid. Allowed values are 'monitor' and 'collector'.\"\n","    )"]},{"cell_type":"markdown","id":"98986135-9e8f-4c70-8a93-8fa8b4e9c6de","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Code implementation"]},{"cell_type":"markdown","id":"c3c7fd54-5478-4741-b3c8-4376a7012041","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Define OpenTelemetry resources using common telemetry module\n","#### Understand spans creation\n","\n","See [creating telemetry data](../../docs/MonitoringAndObservabilityUsingOpenTelemetry.md#create-telemetry-data) for details.\n","\n","#### Define resource attributes - using common schema\n","\n","see [sample common schema](../../docs/MonitoringAndObservabilityUsingOpenTelemetry.md#sample-schema) for details."]},{"cell_type":"code","execution_count":null,"id":"6976991d-aeed-43ed-8e60-076efa201fb7","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Resource references\n","# - Naming conventions: https://opentelemetry.io/docs/specs/semconv/general/attribute-naming/\n","# - For a complete list of reserved ones: https://opentelemetry.io/docs/concepts/semantic-conventions/\n","# NOTE: service.namespace,service.name,service.instance.id triplet MUST be globally unique.\n","#     The ID helps to distinguish instances of the same service that exist at the same time\n","#     (e.g. instances of a horizontally scaled service)\n","\n","# mssparkutils.env.* are not working in 3.5\n","# { k: v for k, v in spark.sparkContext.getConf().getAll() }\n","resource_attributes = {\n","    # ---------- Reserved attribute names\n","    \"service.name\": config[\"process\"][\"service_name\"],\n","    # service.name = opentelemetry.sdk.resources.SERVICE_NAME\n","    \"service.version\": config[\"process\"][\"service_version\"],\n","    \"service.namespace\": \"domain-info\",  # need to update this one - use process info\n","    \"service.instance.id\": notebookutils.runtime.context[\"activityId\"],\n","    \"process.executable.name\": process_name,\n","    \"deployment.environment\": env_stage,\n","    # ---------- custom attributes - we can also add common attributes like appid, domain id etc\n","    #     here or get them from process reference data using processname as the key.\n","    \"jobexec.fabric.capacityid\": spark.sparkContext.getConf().get(\n","        \"spark.hadoop.trident.capacity.id\"\n","    ),\n","    \"jobexec.cluster.id\": spark.sparkContext.getConf().get(\"spark.cluster.name\"),\n","    \"jobexec.cluster.name\": spark.sparkContext.getConf().get(\"spark.fabric.pool.name\"),\n","    \"jobexec.cluster.region\": spark.sparkContext.getConf().get(\"spark.cluster.region\"),\n","    \"jobexec.fabric.envdetails\": spark.sparkContext.getConf().get(\n","        \"spark.fabric.environmentDetails\"\n","    ),\n","    \"jobexec.app.name\": spark.sparkContext.getConf().get(\"spark.app.name\"),\n","    \"jobexec.instance.name\": job_exec_instance,\n","    \"jobexec.context\": f\"{notebookutils.runtime.context}\",  # convert to string otherwise it will fail\n","    \"jobexec.userid\": \"mssparkutils.env.getUserId()\",  # need to find a way to get this\n","    \"jobexec.username\": \"mssparkutils.env.getUserName()\",  # need to find a way to get this\n","    \"jobexec.workspaceid\": spark.sparkContext.getConf().get(\n","        \"spark.hadoop.trident.workspace.id\"\n","    ),\n","}"]},{"cell_type":"markdown","id":"92adf8d1-4f5f-4cd4-9af0-6db01f30793b","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["#### Create Opentelemetry providers"]},{"cell_type":"code","execution_count":null,"id":"945303c5-ede7-400c-9c50-6bca5fa69c18","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# - Usually logging is done with in the context of a Span. This means - each\n","#   log message can be joined with trace information using spanid and traceid.\n","#   For this reason we can choose to keep the resources information for logging\n","#   to a minimum (as they are already part of the span/trace) or keep custom resources which are\n","#   relavant to that log message.\n","#   Note that traceid and spanid will be null when logging is done outside of a span context.\n","log_attributes = {\"service.name\": \"otel-poc-vm-based\"}\n","\n","# For conveneience, in this example we are using same resource_attributes for traces and metrics.\n","trace_attributes = metrics_attributes = resource_attributes\n","\n","try:\n","    tracer\n","except NameError:\n","    print(\"Telemetry providers aren't set yet. Defining now..\")\n","    # Global providers for traces, logs and metrics - Only need to be run once per session\n","    tracer = otlp_exporter.get_otel_tracer(trace_attributes)\n","    logger = otlp_exporter.get_otel_logger(log_attributes)\n","    logger.setLevel(config[\"otel\"][\"log_level\"])  # deafult is WARN\n","    meter = otlp_exporter.get_otel_metrics(metrics_attributes)\n","    counter = meter.create_counter(\n","        name=\"city-level-metrics\", description=\"Counts at city level\", unit=\"1\"\n","    )\n","else:\n","    print(\n","        \"Telemetry providers are defined already. Process will continue with the same global providers.\"\n","    )"]},{"cell_type":"markdown","id":"5c6ce8e0-decc-4110-88ab-74eb1d6b9613","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Library functions"]},{"cell_type":"markdown","id":"6f367cba-c85b-441e-bee8-e6668efa178b","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["See [libary management in Fabric](../../docs/ThingsToConsiderForDataProcesses.md#common-libraries) and [using code from other notebooks](../../docs/ThingsToConsiderForDataProcesses.md#understand-making-calls-to-other-notebookscode-modules) for details.\n","\n","- The below command makes the functions available to local session. Only functions definitions and any other common params needed are defined in this [library notebook](./nb-city-safety-common.ipynb). Shown here with an optional parameter that can be used to skip some portion of the code inside a cell as `%run` will run the entire notebook.\n","  \n","- These functions incorporate Opentelemetry based traces and logs generation using the OpenTelemetry providers created in the previous step. See the code in nb_city_safety_common.ipynb for details.\n","\n","**WARNING**: If there are any variables with the same name then the notebook values that is being called will replace the values in the current notebook context. In the example below - `%run nb_city_safety_common {\"execution_mode\": \"testing\"}` will result in `execution_mode = \"testing\" ` in current context as well.\n"]},{"cell_type":"code","execution_count":null,"id":"b081c888-b17e-47c7-8b44-5866f979fe5f","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%run nb-city-safety-common { \"common_execution_mode\": \"normal\" }"]},{"cell_type":"markdown","id":"b212c971-ccaa-4875-82b7-96f71ea08b85","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### TO DO: DQ\n","\n","This is WIP. Refer to [DQ](../../docs/ThingsToConsiderForDataProcesses.md##value-proposition---dq)."]},{"cell_type":"markdown","id":"a245c06e-e724-450e-afeb-a4154db55b2e","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Main function - Set local parameters, create root span and run the process"]},{"cell_type":"markdown","id":"4ff1e43e","metadata":{},"source":["See [making the code configuration driven](../../docs/ThingsToConsiderForDataProcesses.md#make-the-code-configuration-driven) for details."]},{"cell_type":"code","execution_count":null,"id":"a6042244-97d5-44a1-9211-6594788eaf78","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# ================== Globals section ======================================\n","# During unit tests these are some of the values we will be overwriting with test values.\n","# mssparkutils.env.* are not working in 3.5\n","spark_major_version = float(\".\".join(spark.version.split(\".\")[:2]))\n","# this is a tuple so using json load to read it properly\n","cities = json.loads(config[\"source\"][\"cities\"])\n","# read configs is done in the very begining\n","workspace_name = (\n","    f'{config[\"target\"][\"workspace_name_prefix\"]}-{env_stage}'\n","    if env_stage\n","    else config[\"target\"][\"workspace_name_prefix\"]\n",")\n","# notebookutils.workspace.* aren't ready yet - so we need this code to obtain workspace id\n","ws_name_encoded = urllib.parse.quote(workspace_name)\n","ws_url = f\"https://api.fabric.microsoft.com/v1/admin/workspaces?name={ws_name_encoded}\"\n","token = notebookutils.credentials.getToken(\"pbi\")\n","response = make_fabric_api_call(token=token, url=ws_url, call_type=\"get\", payload={})\n","ws_status, ws_headers, ws_json = response.status_code, response.headers, response.json()\n","if ws_status == 200 and len(ws_json[\"workspaces\"]) > 0:\n","    workspace_id = ws_json[\"workspaces\"][0][\"id\"]\n","else:\n","    raise ValueError(f\"{workspace_name =} isn't returning a valid workspace_id.\")\n","    print(f\"{ws_status = }\\n\\n{ws_json =}\")\n","# Lakehouse name doesn't allow spaces - however if we are using workspace_id in adfss path then this also must be an id.\n","lakehouse_name = config[\"target\"][\"lakehouse_name\"]\n","lakehouse_id = notebookutils.lakehouse.get(\n","    name=lakehouse_name, workspaceId=workspace_id\n",")[\"id\"]\n","lakehouse_table_name = config[\"target\"][\"lakehouse_table_name\"]\n","onelake_name = config[\"target\"][\"onelake_name\"]\n","# Complete paths - allows us to connect to any workspace and any lake house as long we have the proper access.\n","# The following could cause issues if we have spaces in workspace name, so using ids instead.\n","#   onelake_path = f\"abfss://{workspace_name}@{onelake_name}.dfs.fabric.microsoft.com/{lakehouse_name}.lakehouse\"\n","onelake_path = (\n","    f\"abfss://{workspace_id}@{onelake_name}.dfs.fabric.microsoft.com/{lakehouse_id}\"\n",")\n","onelake_file_path = f\"{onelake_path}/Files\"\n","onelake_table_path = f\"{onelake_path}/Tables\"\n","\n","# ------------- Common globals from Config file ------------------------#\n","# Used these directly - ex: config[\"source\"][\"blob_container_name\"]\n","\n","# ------------- ENV/Stage spefic globals derived in the code ------------------------#\n","#  NOTE: When creating stage specific resources like ADLS (whose names are globally unique),\n","#      make sure we can create/access them ie that name is still available for us.\n","blob_account_name = (\n","    f'{config[\"source\"][\"blob_account_name_prefix\"]}{env_stage}'\n","    if env_stage\n","    else config[\"source\"][\"blob_account_name_prefix\"]\n",")\n","blob_sas_token = notebookutils.credentials.getSecret(\n","    config[\"keyvault\"][\"uri\"], f'{config[\"keyvault\"][\"secret_name_prefix\"]}-{env_stage}'\n",")\n","\n","# -------- Allow Spark remote read ------------------\n","# WARNING: NEVER EVER do this. We are changing how data reads work (WASBS VS ABFSS)\n","# - which means each environment will have different behavior,\n","#   and this will result in untested code/behavior in higher envs.\n","#   Correct way is to use same protocol/technology in all envs.\n","if env_stage == \"dev\":\n","    storage_type, storage_end_point_type = \"abfss\", \"dfs\"\n","else:\n","    storage_type, storage_end_point_type = \"wasbs\", \"blob\"\n","# --------------------------------------------\n","\n","wasbs_path = f'{storage_type}://{config[\"source\"][\"blob_container_name\"]}@{blob_account_name}.{storage_end_point_type}.core.windows.net/{config[\"source\"][\"blob_relative_path\"]}'\n","spark.conf.set(\n","    f'fs.azure.sas.{config[\"source\"][\"blob_container_name\"]}.{blob_account_name}.{storage_end_point_type}.core.windows.net',\n","    blob_sas_token,\n",")\n","\n","logger.info(f\"{wasbs_path =}\\n{blob_account_name =}\")"]},{"cell_type":"code","execution_count":null,"id":"72227768-dd8c-4b6b-816b-fce486e8ed0d","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Main function - Read Microsoft Open data sets and load the target table\n","# Telemetry reference: https://github.com/open-telemetry/opentelemetry-python/blob/stable/docs/examples/logs/example.py\n","def main():\n","\n","    with tracer.start_as_current_span(\n","        f\"root#nb-safety#{current_ts}\", kind=SpanKind.INTERNAL\n","    ) as root_span:\n","\n","        try:\n","            root_span.set_attributes(\n","                {\n","                    \"etl.workspace_name\": workspace_name,\n","                    \"etl.cities\": cities,\n","                    \"etl.lakehouse_table_name\": lakehouse_table_name,\n","                }\n","            )\n","            logger.info(\n","                f\"{workspace_name = }\\n{wasbs_path =}\\n{blob_account_name =}\\n{cities = }\\n{lakehouse_table_name = }\"\n","            )\n","\n","            verify_onelake_connection()\n","            etl_steps(\n","                table_name=lakehouse_table_name, cities=cities, cleanup=cleanup_flag\n","            )\n","            # TO DO: ADD some DQ checks\n","\n","            # Prepare for metrics publish using OpenTelemetry\n","            gather_city_level_metrics(table_name=lakehouse_table_name, counter=counter)\n","\n","        except Exception as e:\n","            logger.exception(f\"ETL step failed with error {e}\")\n","            root_span.set_status(StatusCode.ERROR, f\"ETL step failed with error {e}\")\n","            root_span.record_exception(e)\n","            raise\n","        else:\n","            root_span.set_status(StatusCode.OK)\n","        finally:\n","            logger.info(\"City safety processing is complete.\")"]},{"cell_type":"code","execution_count":null,"id":"ef5b1530-73b6-4b76-b9cd-4e15c8112ee6","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["if execution_mode in (\"normal\", \"all\"):\n","    logger.info(\"Execution main function now...\")\n","    main()\n","else:\n","    logger.info(f\"Skipping execution based on the input {execution_mode =}\")"]},{"cell_type":"markdown","id":"413dd708-f74a-49e2-8283-39e782a9fd51","metadata":{"jp-MarkdownHeadingCollapsed":true,"nteract":{"transient":{"deleting":false}}},"source":["## Visualize data\n","\n","See [data reporting](../../docs/DataReporting.md) for details. This is a *WIP*."]},{"cell_type":"markdown","id":"028134d0-d555-4b23-af86-0db15f240e4c","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Unit tests\n","\n","Refer to [data testing](../../docs/DataTesting.md) on how to plan, create and run unit testcases. This write-up also discusses different types (from their focus perspective) of unit testcases as well.\n","\n","Also, see [MDW Parking sensor sample](https://github.com/Azure-Samples/modern-data-warehouse-dataops/tree/main/e2e_samples/parking_sensors) for more [unit testing and integration testing](https://github.com/Azure-Samples/modern-data-warehouse-dataops/tree/main/e2e_samples/parking_sensors#testing) examples."]},{"cell_type":"markdown","id":"cdbcd24b","metadata":{},"source":["### Unit tests - code focus"]},{"cell_type":"markdown","id":"e6774478","metadata":{},"source":["**TO DO**: \n","\n","Elena's feedback: Keep the good case here (ie using another notebook for testcases). \n","    - Need to move this and refactor and demonstrate the use of conditional execution."]},{"cell_type":"markdown","id":"30ad809f","metadata":{},"source":["This example shows Unit testing code hosted in the same notebook as the main code. Note the usage of `execution_mode` to control the testcase runs. We are also setting `raise_on_error=False` so that failure of unit testcases will not cause a notebook failure. Instead these are captured in later steps and let the user decide on how to proceed further."]},{"cell_type":"code","execution_count":null,"id":"923a9bc3-5def-4536-ba39-a8695f253bcd","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["if execution_mode in (\"testing\", \"all\"):\n","    import pytest\n","    import ipytest\n","    from unittest.mock import MagicMock, patch, call\n","\n","    ipytest.autoconfig(\n","        raise_on_error=False\n","    )  # **NOTE** True will result in notebook failure; False will not cause notebook failure. We can then use our capture output to do any custom error handling.\n","\n","    @patch(\"__main__.gather_city_level_metrics\")\n","    @patch(\"__main__.etl_steps\")\n","    @patch(\"__main__.verify_onelake_connection\")\n","    @patch(\"__main__.tracer\")\n","    def test_main(\n","        mock_tracer, mock_conn_check, mock_etl_steps, mock_metrics_gather, caplog\n","    ):\n","        caplog.clear()\n","        caplog.set_level(logging.INFO)\n","\n","        exp_log_output = [\n","            f\"{workspace_name = }\",\n","            f\"{cities = }\",\n","            f\"{lakehouse_table_name = }\",\n","            \"City safety processing is complete.\",\n","        ]\n","\n","        mock_conn_check.return_value = True\n","        mock_etl_steps.return_value = True\n","        mock_metrics_gather.return_value = True\n","\n","        main()\n","\n","        mock_span = (\n","            mock_tracer.start_as_current_span.return_value.__enter__.return_value\n","        )\n","        print(caplog.text)\n","        assert all(log in caplog.text for log in exp_log_output)\n","        # print(mock_tracer.mock_calls)\n","        assert (\n","            call.start_as_current_span().__enter__() in mock_tracer.mock_calls\n","        )  # <----trace (with) context call\n","        assert (\n","            mock_tracer.start_as_current_span.return_value.__enter__.call_count == 1\n","        )  # <----trace (with) context-start\n","        mock_span.set_attributes.assert_called_once_with(\n","            {\n","                \"etl.workspace_name\": workspace_name,\n","                \"etl.cities\": cities,\n","                \"etl.lakehouse_table_name\": lakehouse_table_name,\n","            }\n","        )\n","        assert mock_span.set_status.call_count == 1\n","        assert (\n","            mock_tracer.start_as_current_span.return_value.__exit__.call_count == 1\n","        )  # <--- trace (with) context-end\n","\n","        # Check exception condition\n","        error_message = \"Dummy failure on metrics gathering.\"\n","        mock_metrics_gather.side_effect = Exception(error_message)\n","\n","        with pytest.raises(Exception) as exc:\n","            main()\n","\n","        assert f\"ETL step failed with error {error_message}\" in caplog.text\n","        mock_span.set_status.call_count == 2\n","        assert exc.type == Exception\n","        assert str(exc.value) == error_message"]},{"cell_type":"code","execution_count":null,"id":"f6a26d17-3d45-4176-9676-d4cf45c06ee6","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%%capture unit_tests_results\n","# %%ipytest - will run the test cases using the magic - instead we are using ipytest.run()\n","print(f\"at the testing cell. {execution_mode = }\")\n","if execution_mode in (\"testing\", \"all\"):\n","    print(f\"running the test cases now. {execution_mode = }\")\n","    ipytest.run()\n","else:\n","    logger.info(\"Skipping code based unit tests\")"]},{"cell_type":"code","execution_count":null,"id":"c129618d-fecf-4bec-bb99-9de3eb71c9f4","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["if execution_mode in (\"testing\", \"all\"):\n","    store_unit_test_results(unit_tests_results) # as of now we are only printing the results\n","    # TO DO - how to sureface these results for the CI process"]},{"cell_type":"markdown","id":"92a87d3e-9c68-418f-af01-7e4b1dfc5517","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Unit testing - data focus"]},{"cell_type":"markdown","id":"36808d51","metadata":{},"source":["This example shows Unit testing code hosted in [another notebook](../../tests/test_nb-city-safety.ipynb)."]},{"cell_type":"markdown","id":"41108789-950c-4d55-824c-10cc27ea3c7b","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Maintenace Utilities\n","\n"]},{"cell_type":"markdown","id":"bb2355c3-b726-452d-b39f-de503e5b7aab","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["It is important the system is maintained properly and optimized for resource utililization. \n","\n","See [maintenance utilities](../../docs/MaintenanceUtilities.md) for details on this topic. < This is WIP>"]}],"metadata":{"dependencies":{"environment":{"environmentId":"2b47b55d-9024-4737-a951-4a9ed5bff8a8","workspaceId":"91d4b7a6-b9b8-411c-bc27-e3be90555509"},"lakehouse":{}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{"a4617076-a9e3-4965-92d7-f5646c25bbc7":{"persist_state":{"view":{"chartOptions":{"aggregationType":"count","binsNumber":10,"categoryFieldKeys":["0"],"chartType":"bar","evaluatesOverAllRecords":false,"isStacked":false,"seriesFieldKeys":["0"],"wordFrequency":"-1"},"tableOptions":{},"type":"details"}},"sync_state":{"isSummary":false,"language":"scala","table":{"rows":[{"0":"Boston","1":"407","index":1}],"schema":[{"key":"0","name":"city","type":"string"},{"key":"1","name":"count","type":"bigint"}],"truncated":false}},"type":"Synapse.DataFrame"}},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
