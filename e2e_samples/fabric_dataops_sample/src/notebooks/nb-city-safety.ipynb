{"cells":[{"cell_type":"markdown","id":"eeda8435-14f2-4a8e-96fa-3d396c7e48b3","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# City Safety Data - ETL - Sample"]},{"cell_type":"markdown","id":"c79df08e-52c4-41b8-bd25-5bb4d3d4e6d7","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## About this Notebook\n","\n","- This notebook performs a sample ETL operation using Microsoft Open datasets (city safety data).\n","- It **doesn't require** a default lakehouse attached and instead uses absolute paths to create/load managed tables.\n","- Can be run from any Fabric workspace as long as proper access to provided to write to the targets(workspace and lakehouse).\n","- Data can be loaded:\n","    - into a new table \n","    - into an existing table in append mode  (by setting - cleanup = False)\n","    - into an existing table in overwrite mode (by setting - cleanup = True)"]},{"cell_type":"markdown","id":"1f72ca89-dae6-4fb2-9cd5-bc44b2e67207","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Libraries"]},{"cell_type":"code","execution_count":null,"id":"74f1155a-a3f7-4abd-b8b4-e0901fab839e","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import json\n","from pyspark.sql.functions import lit, to_utc_timestamp, unix_timestamp, avg, max, min, sum, count\n","from delta.tables import DeltaTable\n","from typing import Optional"]},{"cell_type":"markdown","id":"5c6ce8e0-decc-4110-88ab-74eb1d6b9613","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Configuration"]},{"cell_type":"markdown","id":"e33d5f25-3aa0-4ee3-b78e-114d64ceed92","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Set External parameters"]},{"cell_type":"code","execution_count":null,"id":"03075e96-b60f-4561-92f2-d9c14ae13916","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"outputs":[],"source":["# Keep Only External parameters in this cell.\n","onelake_name = \"onelake\"\n","workspace_name = \"ws-fabric-cicd-dev\"\n","lakehouse_name = \"lh_main\""]},{"cell_type":"code","execution_count":null,"id":"15993285-991e-480f-906d-276a0c5399f0","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["print(f\"{onelake_name = }\")\n","print(f\"{workspace_name = }\")\n","print(f\"{lakehouse_name = }\")"]},{"cell_type":"markdown","id":"aeab0b3a-f6f7-479a-a95b-aa4572ba1ed7","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Set local parameters"]},{"cell_type":"code","execution_count":null,"id":"d033bce7-f2f1-45d0-8124-6b02a2627f76","metadata":{},"outputs":[],"source":["# Complete paths - This way we are independent of local workspace - we can connect to any workspace and any lake house as long we have the proper access. \n","lakehouse_table_name = \"tbl_city_safety_data\"\n","cities = (\"Boston\", \"Chicago\", \"NewYorkCity\", \"Seattle\", \"SanFrancisco\") # (\"Boston\", )\n","\n","print(f\"{workspace_name = }\")\n","print(f\"{cities = }\")\n","print(f\"{lakehouse_table_name = }\")\n","\n","# Microsoft Open dataset - Safety data - Ref: https://learn.microsoft.com/en-us/azure/open-datasets/dataset-new-york-city-safety?tabs=pyspark\n","# Azure storage access info  \n","blob_account_name = \"azureopendatastorage\"\n","blob_container_name = \"citydatacontainer\"\n","blob_relative_path = \"Safety/Release/\"\n","blob_sas_token = r\"\"\n","\n","onelake_path = f\"abfss://{workspace_name}@{onelake_name}.dfs.fabric.microsoft.com/{lakehouse_name}.lakehouse\"\n","onelake_file_path = f\"{onelake_path}/Files\"\n","onelake_table_path = f\"{onelake_path}/Tables\"\n","\n","# Allow Spark remote read\n","wasbs_path = f\"wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/{blob_relative_path}\"\n","spark.conf.set( f\"fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net\", blob_sas_token)"]},{"cell_type":"code","execution_count":null,"id":"177e3c18-ef40-4acc-984e-eae7aa132275","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Check onelake existence - otherwise abort notebook execution\n","error_message = f\"Specfied lakehouse table path {onelake_table_path} doesn't exist. Ensure onelake={onelake_name}, workspace={workspace_name} and lakehouse={lakehouse_name} exist.\"\n","try:\n","    if not(mssparkutils.fs.exists(onelake_table_path)):\n","        raise ValueError(\"Encountered error while checking for Lakehouse table path specified.\")\n","except Exception as e:\n","    print(f\"Error message: {e}\")\n","    # no further execution but Session is still active\n","    mssparkutils.notebook.exit(error_message)\n","else:\n","    print(f\"Target table path: {onelake_table_path} is valid and exists.\")\n","    print(\"Listing source data contents to check connectivity\")\n","    print(mssparkutils.fs.ls(wasbs_path))"]},{"cell_type":"markdown","id":"4a87eda6-c9a5-433e-a449-32c5bd2ec10d","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Create data extraction and load functions"]},{"cell_type":"code","execution_count":null,"id":"7e8556ac-447b-4e06-8547-b21c34708df1","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def identify_table_load_mode(table_name: str) -> bool:\n","\n","    # Not so preferred option - works when we have a default lakehouse attached\n","    # load_mode = \"append\" if spark.catalog.tableExists(table_name) else \"overwrite\"\n","\n","    # Preferred option - Assuming default lakehouse is not set, checking based on the delta path\n","    load_mode = \"append\" if DeltaTable.isDeltaTable(spark, f\"{onelake_table_path}/{table_name}\") else \"overwrite\"\n","    \n","    return  load_mode\n","\n","def delete_delta_table(table_name: str) -> bool:\n","\n","    delta_table_path = f\"{onelake_table_path}/{table_name}\"\n","\n","    if mssparkutils.fs.exists(delta_table_path):\n","        print(f\"Attempting to delete exisitng delta table with {delta_table_path = }....\")\n","\n","        try:\n","            mssparkutils.fs.rm(dir=delta_table_path, recurse=True)   \n","        except Exception as e:\n","            print(f\"Deletion failed with the error:\\n===={e}\\n=====\")\n","            raise\n","        else:\n","            print(f\"Deleted exisitng delta table: {table_name}.\")\n","    else:\n","        print(f\"The specified delta table doesn't exist. No need for deletion.\")\n","\n","def transform_data(city: str, data_frame: object) -> object:\n","\n","    # Need timezone to convert to UTC\n","    if city in (\"Boston\", \"NewYorkCity\"):\n","        timezone = \"America/New_York\"\n","    elif city in (\"Seattle\", \"SanFrancisco\"):\n","        timezone = \"America/Los_Angeles\"\n","    else:\n","        timezone = \"America/Chicago\"\n","\n","    data_frame = data_frame\\\n","        .withColumn(\"dateTimeUTC\", to_utc_timestamp(data_frame.dateTime, timezone)) \\\n","        .withColumn(\"City\", lit(city))\n","\n","    return data_frame\n","\n","def etl_steps(table_name: str, cleanup: Optional[bool] = True) -> None:\n","\n","    # Optionally delete existing contents\n","    delta_table_path = f\"{onelake_table_path}/{table_name}\"\n","    if cleanup:\n","        delete_delta_table(table_name)\n","        print(f\"A new delta table '{table_name}' will be created with {delta_table_path = }\")\n","    else:\n","        print(\"No request for cleanup. Proceeding to ETL steps.\")\n","    \n","    for city in cities:\n","        print(f\"ETL started for {city = }.\")\n","\n","        print(f\"\\t Data Extraction in progress.\")\n","        city_calls_data_path = f\"{wasbs_path}/city={city}\"\n","        city_calls_df = spark.read.parquet(city_calls_data_path)\n","\n","        print(f\"\\t Read {city_calls_df.count()} records for {city = }.\")\n","        print(f\"\\t Data transformation in progress.\")\n","        city_calls_df = transform_data(city, city_calls_df)\n","\n","        delta_mode = identify_table_load_mode(table_name)\n","        print(f\"\\t Data loading in inprogress using {delta_mode} mode.\")\n","        city_calls_df.write.format(\"delta\").mode(delta_mode).save(delta_table_path) \n","    \n","    print(f\"\\n=====\\nCity safety data is loaded into {table_name =} for {cities =}\\n=====\")\n","\n","    return None\n","\n","def gather_city_level_metrics(table_name: str) -> None:\n","\n","    delta_table = spark.read.format(\"delta\").load(f\"{onelake_table_path}/{table_name}\")\n","    city_metrics = delta_table.groupBy(\"city\").agg(  \n","        count(\"*\").alias(\"count\")  \n","        # avg(\"metric1\").alias(\"avg_metric1\"),  \n","        # max(\"metric2\").alias(\"max_metric2\"),  \n","        # min(\"metric3\").alias(\"min_metric3\"),  \n","        # sum(\"metric4\").alias(\"sum_metric4\")  \n","    )  \n","  \n","    display(city_metrics)\n","\n","    return None\n","\n","\n"]},{"cell_type":"markdown","id":"b374c6d8-8dcd-4131-a86d-7743cf01b823","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Read Microsoft Open data sets and load the target table"]},{"cell_type":"code","execution_count":null,"id":"72227768-dd8c-4b6b-816b-fce486e8ed0d","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# main function\n","try:\n","    etl_steps(table_name=lakehouse_table_name, cleanup=True)\n","except Exception as e:\n","    print(f\"ETL step failed with error {e}\")\n","    raise\n","else:\n","    gather_city_level_metrics(table_name=lakehouse_table_name)\n","finally:\n","    print(\"City safety processing is complete.\")\n"]},{"cell_type":"markdown","id":"413dd708-f74a-49e2-8283-39e782a9fd51","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Visualize data"]},{"cell_type":"code","execution_count":null,"id":"b4ac6fb8-864b-4157-a3d0-e2dfad5d102a","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# WIP"]}],"metadata":{"dependencies":{"lakehouse":{}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
