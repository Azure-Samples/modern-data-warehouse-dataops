{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "a80203f1-c063-4176-92e7-494827da9a18",
    "default_lakehouse_name": "lh_fqntestf01",
    "default_lakehouse_workspace_id": "ca36df59-223e-44e3-9e86-cb810f3f4d75"
   },
   "environment": {
    "environmentId": "1200b883-7c0c-40c7-a0ae-1050003064f0",
    "workspaceId": "e4bfc481-1e36-4673-99da-5342cc7eb134"
   }
  },
  "language_info": {
   "name": "python"
  },
  "kernel_info": {
   "name": "synapse_pyspark",
   "jupyter_kernel_name": null
  },
  "a365ComputeOptions": null,
  "sessionKeepAliveTimeout": 0
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": null,
   "source": [
    "# Transform data for Parking Sensors\n",
    "\n",
    "About:\n",
    "\n",
    "- This notebook ingests data from the source needed by parking sensors sample. It then performs cleanup and standardization step. See [parking sensors page](https://github.com/Azure-Samples/modern-data-warehouse-dataops/tree/feat/e2e-fabric-dataops-sample/e2e_samples/fabric_dataops_sample/README.md) for more details about Parking Sensor sample using Microsoft Fabric.\n",
    "\n",
    "Assumptions/Pre-requisites:\n",
    "\n",
    "- Currently there is a known issue running cross workspace queries when workspace name has special characters. See [schema limitation](https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-schemas#public-preview-limitations) for more details. Avoid special characters if planning to query across workspaces with schema support. \n",
    "- All the assets needed are created by IaC step during migration.\n",
    "    - Config file needed: Files/sc-adls-main/config/application.cfg (derived using application.cfg.template during ci/cd process). Ensure \"transform\" section is updated with the required parameters for this notebook.\n",
    "- All the required lakehouse schemas and tables are created by the `nb-setup` notebook.\n",
    "- Input data standardization is completed by `nb-standardize` notebook.\n",
    "- Environment with common library otel_monitor_invoker.py and its associated python dependencies\n",
    "- Parking Sensor Lakehouse\n",
    "- Datasource: ADLS made available as a shortcut in Parking Sensor Lakehouse or Direct access to REST APIs.\n",
    "- Monitoring sink: AppInsights\n",
    "- Secrets repo: Key vault to store AppInsights connection information\n",
    "\n",
    "- All Lakehouses have schema support enabled (in Public preview as of Nov, 2024).\n",
    "- Execution\n",
    "  - A default lakehouse is associated during runtime where the required files and data are already staged. Multiple ways of invoking:\n",
    "    - [Api call](https://learn.microsoft.com/fabric/data-engineering/notebook-public-api#run-a-notebook-on-demand)\n",
    "    - [Part of a data pipeline](https://learn.microsoft.com/fabric/data-engineering/author-execute-notebook#parameterized-session-configuration-from-a-pipeline)\n",
    "    - [Using `%run` from another notebook](https://learn.microsoft.com/fabric/data-engineering/author-execute-notebook#reference-run-a-notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": null,
   "source": [
    "## Parameters and Library imports\n",
    "\n",
    "### Reading parameters (external from Fabric pipeline or default values)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "import configparser\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Any\n",
    "\n",
    "import ddo_transform_transform as t\n",
    "import otel_monitor_invoker as otel  # custom module part of env\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "from great_expectations.data_context import BaseDataContext\n",
    "from great_expectations.data_context.types.base import (\n",
    "    DataContextConfig,\n",
    "    DatasourceConfig,\n",
    "    FilesystemStoreBackendDefaults,\n",
    ")\n",
    "from opentelemetry.trace import SpanKind\n",
    "from opentelemetry.trace.status import StatusCode\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit\n",
    "from ruamel import yaml"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "parameters"
    ],
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# Unless `%%configure` is used to read external parameters - this cell should be the first one after imports\n",
    "\n",
    "# This cell is tagged as Parameters cell. Parameters mentioned here are usually \\\n",
    "#    passed by the user at the time of notebook execution.\n",
    "# Ref: https://learn.microsoft.com/fabric/data-engineering/notebook-public-api#run-a-notebook-on-demand\n",
    "\n",
    "# Control how to run the notebook - \"all\" for entire notebook or \"module\" mode to use \\\n",
    "#      this notebook like a module (main execution will be skipped). Useful when performing\n",
    "#      testing using notebooks or functions from this notebook need to be called from another notebook.\n",
    "# execution_mode = \"module\" will skip the execution of the main function. Use it for module like treatment\n",
    "#   \"all\" perform execution as well.\n",
    "execution_mode = \"all\"\n",
    "# Helpful if user wants to set a child process name etc. will be derived if not set by user\n",
    "job_exec_instance = \"\"\n",
    "# Helpful to derive any stage based globals\n",
    "env_stage = \"dev\"\n",
    "# Common config file path hosted on attached lakehouse - path relative to Files/\n",
    "config_file_path = \"sc-adls-main/config/application.cfg\"\n",
    "\n",
    "# Parameters from the pipeline\n",
    "infilefolder = \"2024_12_17_11_46_14\"\n",
    "load_id = \"e8099e5c-16f6-4104-9009-814b75587d99\"\n",
    "# For local execution, it derives values from the runtime context.\n",
    "# Or value passed from parameters from the pipeline during execution\n",
    "runtime_context = notebookutils.runtime.context\n",
    "workspace_id = runtime_context[\"currentWorkspaceId\"]\n",
    "lakehouse_id = runtime_context[\"defaultLakehouseId\"]\n",
    "workspace_name = runtime_context[\"currentWorkspaceName\"]\n",
    "lakehouse_name = runtime_context[\"defaultLakehouseName\"]\n",
    "local_mount_name = \"/local_data\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# # only need to run when developing the notebook to format the code\n",
    "# import jupyter_black\n",
    "\n",
    "# jupyter_black.load()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# Validate input parameters\n",
    "in_errors = []\n",
    "if execution_mode not in [\"all\", \"module\"]:\n",
    "    in_errors.append(f\"Invalid value: {execution_mode = }. It must be either 'all' or 'module'.\")\n",
    "if not notebookutils.fs.exists(f\"Files/{config_file_path}\"):\n",
    "    in_errors.append(f\"Specified config - `Files/{config_file_path}` doesn't exist.\")\n",
    "\n",
    "if in_errors:\n",
    "    raise ValueError(f\"Input parameter validation failed. Errors are:\\n{in_errors}\")\n",
    "else:\n",
    "    print(\"Input parameter verification completed successfully.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": null,
   "source": [
    "### File mounts\n",
    "\n",
    "- Scope is set to Job/session - so these need to be run once per session\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# Helps to read config files from onelake location\n",
    "# Optionally this can be done in using %%configure.\n",
    "local_mount = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Files\"\n",
    "notebookutils.fs.mount(\n",
    "    source=local_mount,\n",
    "    mountPoint=local_mount_name,\n",
    "    extraConfigs={\"Scope\": \"job\"},\n",
    ")\n",
    "local_data_mount_path = notebookutils.fs.getMountPath(local_mount_name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "## Temporary workaround. Remove when issue with OneLake authentication in Notebook is resolved.\n",
    "file_path = f\"{local_data_mount_path}/{config_file_path}\"\n",
    "os.listdir(f\"{local_data_mount_path}/\")\n",
    "os.listdir(f\"{local_data_mount_path}/sc-adls-main\")\n",
    "os.listdir(f\"{local_data_mount_path}/sc-adls-main/config\")\n",
    "with open(file_path, \"r\") as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "# print(file_content)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": null,
   "source": [
    "### Read user provided config values\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "config = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation())\n",
    "config.read(f\"{local_data_mount_path}/{config_file_path}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# When we config parser if the value is not present in the specified section, it will be\n",
    "#   read from \"DEFAULT\" section.\n",
    "config_section_name = \"transform\"\n",
    "process_name = config.get(config_section_name, \"process_name\")\n",
    "parking_ws = config.get(config_section_name, \"workspace_name\")\n",
    "parking_ws_id = config.get(config_section_name, \"workspace_id\")\n",
    "parking_lakehouse = config.get(config_section_name, \"lakehouse_name\")\n",
    "\n",
    "# Add any other parameters that need to be read"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": null,
   "source": [
    "### Internal (derived) parameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# default is micro-seconds, changing to milli-seconds\n",
    "current_ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S%f\")[:-3]\n",
    "job_exec_instance = job_exec_instance if job_exec_instance else f\"{process_name}#{current_ts}\"\n",
    "execution_user_name = runtime_context[\"userName\"]\n",
    "\n",
    "# Add any other parameters needed by the process\n",
    "loaded_on = datetime.now()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": null,
   "source": [
    "## Monitoring and observability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": null,
   "source": [
    "### AppInsights connection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "connection_string = notebookutils.credentials.getSecret(\n",
    "    config.get(\"keyvault\", \"uri\"), config.get(\"otel\", \"appinsights_connection_name\")\n",
    ")\n",
    "otlp_exporter = otel.OpenTelemetryAppInsightsExporter(conn_string=connection_string)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": null,
   "source": [
    "### Populate resource information"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# Resource references\n",
    "# - Naming conventions: https://opentelemetry.io/docs/specs/semconv/general/attribute-naming/\n",
    "# - For a complete list of reserved ones: https://opentelemetry.io/docs/concepts/semantic-conventions/\n",
    "#  NOTE: service.namespace,service.name,service.instance.id triplet MUST be globally unique.\n",
    "#     The ID helps to distinguish instances of the same service that exist at the same time\n",
    "#     (e.g. instances of a horizontally scaled service)\n",
    "resource_attributes = {\n",
    "    # ---------- Reserved attribute names\n",
    "    \"service.name\": config.get(config_section_name, \"service_name\"),\n",
    "    \"service.version\": config.get(config_section_name, \"service_version\"),\n",
    "    \"service.namespace\": \"parking-sensor\",\n",
    "    \"service.instance.id\": notebookutils.runtime.context[\"activityId\"],\n",
    "    \"process.executable.name\": process_name,\n",
    "    \"deployment.environment\": env_stage,\n",
    "    # ---------- custom attributes - we can also add common attributes like appid, domain id etc\n",
    "    #     here or get them from process reference data using processname as the key.\n",
    "    # runtime context has a lot if useful info - adding it as is.\n",
    "    \"jobexec.context\": f\"{notebookutils.runtime.context}\",  # convert to string otherwise it will fail\n",
    "    \"jobexec.cluster.region\": spark.sparkContext.getConf().get(\"spark.cluster.region\"),\n",
    "    \"jobexec.app.name\": spark.sparkContext.getConf().get(\"spark.app.name\"),\n",
    "    \"jobexec.instance.name\": job_exec_instance,\n",
    "}\n",
    "\n",
    "# Typically, logging is performed within the context of a span.\n",
    "#   This allows log messages to be associated with trace information through the use of trace IDs and span IDs.\n",
    "#   As a result, it's generally not necessary to include resource information in log messages.\n",
    "# Note that trace IDs and span IDs will be null when logging is performed outside of a span context.\n",
    "log_attributes = {\"jobexec.instance.name\": job_exec_instance}\n",
    "trace_attributes = resource_attributes\n",
    "\n",
    "tracer = otlp_exporter.get_otel_tracer(trace_resource_attributes=trace_attributes, tracer_name=f\"tracer-{process_name}\")\n",
    "logger = otlp_exporter.get_otel_logger(\n",
    "    log_resource_attributes=log_attributes,\n",
    "    logger_name=f\"logger-{process_name}\",\n",
    "    add_console_handler=False,\n",
    ")\n",
    "logger.setLevel(\"INFO\")  # deafult is WARN"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": null,
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": null,
   "source": [
    "### Code functions\n",
    "\n",
    "- When using %run we can expose these functions to the calling notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "def get_lakehouse_details(lakehouse_name: str) -> dict:\n",
    "    logger.info(\"Performing lakehouse existence check.\")\n",
    "    try:\n",
    "        # Checks only current workspace\n",
    "        details = notebookutils.lakehouse.get(name=lakehouse_name)\n",
    "    except Exception:\n",
    "        logger.exception(f\"Specified lakehouse - {lakehouse_name} doesn't exist. Aborting..\")\n",
    "        raise\n",
    "    return details"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform and load Dimension tables\n",
    "\n",
    "\n",
    "def transform_load_dimension(\n",
    "    parkingbay_sdf: DataFrame, sensordata_sdf: DataFrame, load_id: str, loaded_on: datetime\n",
    ") -> None:\n",
    "    # Read existing Dimensions\n",
    "    dim_parkingbay_sdf = spark.read.table(f\"`{workspace_name}`.{lakehouse_name}.dw.dim_parking_bay\")\n",
    "    dim_location_sdf = spark.read.table(f\"`{workspace_name}`.{lakehouse_name}.dw.dim_location\")\n",
    "    dim_st_marker = spark.read.table(f\"`{workspace_name}`.{lakehouse_name}.dw.dim_st_marker\")\n",
    "\n",
    "    # Transform\n",
    "    new_dim_parkingbay_sdf = t.process_dim_parking_bay(parkingbay_sdf, dim_parkingbay_sdf, load_id, loaded_on).cache()\n",
    "    new_dim_location_sdf = t.process_dim_location(sensordata_sdf, dim_location_sdf, load_id, loaded_on).cache()\n",
    "    new_dim_st_marker_sdf = t.process_dim_st_marker(sensordata_sdf, dim_st_marker, load_id, loaded_on).cache()\n",
    "\n",
    "    # Insert new rows into Dimension tables\n",
    "    new_dim_parkingbay_sdf.write.insertInto(f\"`{workspace_name}`.{lakehouse_name}.dw.dim_parking_bay\", overwrite=True)\n",
    "    new_dim_location_sdf.write.insertInto(f\"`{workspace_name}`.{lakehouse_name}.dw.dim_location\", overwrite=True)\n",
    "    new_dim_st_marker_sdf.write.insertInto(f\"`{workspace_name}`.{lakehouse_name}.dw.dim_st_marker\", overwrite=True)\n",
    "\n",
    "\n",
    "### Transform and load Fact tables\n",
    "def transform_load_fact_table(sensordata_sdf: DataFrame, load_id: str, loaded_on: datetime) -> DataFrame:\n",
    "    # Read existing Dimensions\n",
    "    dim_parkingbay_sdf = spark.read.table(f\"`{workspace_name}`.{lakehouse_name}.dw.dim_parking_bay\")\n",
    "    dim_location_sdf = spark.read.table(f\"`{workspace_name}`.{lakehouse_name}.dw.dim_location\")\n",
    "    dim_st_marker = spark.read.table(f\"`{workspace_name}`.{lakehouse_name}.dw.dim_st_marker\")\n",
    "\n",
    "    # Process\n",
    "    nr_fact_parking = t.process_fact_parking(\n",
    "        sensordata_sdf, dim_parkingbay_sdf, dim_location_sdf, dim_st_marker, load_id, loaded_on\n",
    "    )\n",
    "\n",
    "    # Insert new rows\n",
    "    nr_fact_parking.write.mode(\"append\").insertInto(f\"`{workspace_name}`.{lakehouse_name}.dw.fact_parking\")\n",
    "\n",
    "    return nr_fact_parking"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "def validate_fact_parking(fact_parking_df: DataFrame) -> Any:\n",
    "\n",
    "    root_directory = f\"{local_data_mount_path}/transform_validation\"\n",
    "\n",
    "    # 1. Configure DataContext\n",
    "    # https://docs.greatexpectations.io/docs/terms/data_context\n",
    "    data_context_config = DataContextConfig(\n",
    "        datasources={\n",
    "            \"transformed_data_source\": DatasourceConfig(\n",
    "                class_name=\"Datasource\",\n",
    "                execution_engine={\"class_name\": \"SparkDFExecutionEngine\", \"force_reuse_spark_context\": True},\n",
    "                data_connectors={\n",
    "                    \"transformed_data_connector\": {\n",
    "                        \"module_name\": \"great_expectations.datasource.data_connector\",\n",
    "                        \"class_name\": \"RuntimeDataConnector\",\n",
    "                        \"batch_identifiers\": [\n",
    "                            \"environment\",\n",
    "                            \"pipeline_run_id\",\n",
    "                        ],\n",
    "                    }\n",
    "                },\n",
    "            )\n",
    "        },\n",
    "        store_backend_defaults=FilesystemStoreBackendDefaults(root_directory=root_directory),\n",
    "    )\n",
    "    context = BaseDataContext(project_config=data_context_config)\n",
    "\n",
    "    # 2. Create a BatchRequest based on parkingbay_sdf dataframe.\n",
    "    # https://docs.greatexpectations.io/docs/terms/batch\n",
    "    batch_request = RuntimeBatchRequest(\n",
    "        datasource_name=\"transformed_data_source\",\n",
    "        data_connector_name=\"transformed_data_connector\",\n",
    "        data_asset_name=\"paringbaydataaset\",  # This can be anything that identifies this data_asset for you\n",
    "        batch_identifiers={\n",
    "            \"environment\": \"stage\",\n",
    "            \"pipeline_run_id\": \"pipeline_run_id\",\n",
    "        },\n",
    "        runtime_parameters={\"batch_data\": fact_parking_df},  # Your dataframe goes here\n",
    "    )\n",
    "\n",
    "    # 3. Define Expectation Suite and corresponding Data Expectations\n",
    "    # https://docs.greatexpectations.io/docs/terms/expectation_suite\n",
    "    expectation_suite_name = \"Transfomed_data_exception_suite_basic\"\n",
    "    context.create_expectation_suite(expectation_suite_name=expectation_suite_name, overwrite_existing=True)\n",
    "    validator = context.get_validator(\n",
    "        batch_request=batch_request,\n",
    "        expectation_suite_name=expectation_suite_name,\n",
    "    )\n",
    "    # Add Validations to suite\n",
    "    # Check available expectations: validator.list_available_expectation_types()\n",
    "    # https://legacy.docs.greatexpectations.io/en/latest/autoapi/great_expectations/expectations/index.html\n",
    "    # https://legacy.docs.greatexpectations.io/en/latest/reference/core_concepts/expectations/standard_arguments.html#meta\n",
    "    validator.expect_column_values_to_not_be_null(column=\"status\")\n",
    "    validator.expect_column_values_to_be_of_type(column=\"status\", type_=\"StringType\")\n",
    "    validator.expect_column_values_to_not_be_null(column=\"dim_time_id\")\n",
    "    validator.expect_column_values_to_be_of_type(column=\"dim_time_id\", type_=\"IntegerType\")\n",
    "    validator.expect_column_values_to_not_be_null(column=\"dim_parking_bay_id\")\n",
    "    validator.expect_column_values_to_be_of_type(column=\"dim_parking_bay_id\", type_=\"StringType\")\n",
    "    # validator.validate() # To run run validations without checkpoint\n",
    "    validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "\n",
    "    # 4. Configure a checkpoint and run Expectation suite using checkpoint\n",
    "    # https://docs.greatexpectations.io/docs/terms/checkpoint\n",
    "    my_checkpoint_name = \"Transformed Data\"\n",
    "    checkpoint_config = {\n",
    "        \"name\": my_checkpoint_name,\n",
    "        \"config_version\": 1.0,\n",
    "        \"class_name\": \"SimpleCheckpoint\",\n",
    "        \"run_name_template\": \"%Y%m%d-%H%M%S-my-run-name-template\",\n",
    "    }\n",
    "    context.test_yaml_config(yaml.dump(checkpoint_config, default_flow_style=False))\n",
    "    context.add_checkpoint(**checkpoint_config)\n",
    "    # Run Checkpoint passing in expectation suite\n",
    "    checkpoint_result = context.run_checkpoint(\n",
    "        checkpoint_name=my_checkpoint_name,\n",
    "        validations=[\n",
    "            {\n",
    "                \"batch_request\": batch_request,\n",
    "                \"expectation_suite_name\": expectation_suite_name,\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return checkpoint_result"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": null,
   "source": [
    "### Data Quality Metric Reporting\n",
    "This parses the results of the checkpoint and sends it to AppInsights / Azure Monitor for reporting."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "def data_quality_metric_reporting(checkpoint_result: Any, load_id: str) -> None:\n",
    "    ## Report Data Quality Metrics to Azure Monitor using python Azure Monitor open-census exporter\n",
    "    result_dic = checkpoint_result.to_json_dict()\n",
    "    key_name = [key for key in result_dic[\"run_results\"].keys()][0]\n",
    "    results = result_dic[\"run_results\"][key_name][\"validation_result\"][\"results\"]\n",
    "\n",
    "    checks = {\"check_name\": checkpoint_result[\"checkpoint_config\"][\"name\"], \"pipelinerunid\": load_id}\n",
    "    for i in range(len(results)):\n",
    "        validation_name = (\n",
    "            results[i][\"expectation_config\"][\"expectation_type\"]\n",
    "            + \"_on_\"\n",
    "            + results[i][\"expectation_config\"][\"kwargs\"][\"column\"]\n",
    "        )\n",
    "        checks[validation_name] = results[i][\"success\"]\n",
    "\n",
    "    properties = {\"custom_dimensions\": str(checks)}\n",
    "\n",
    "    if checkpoint_result.success is True:\n",
    "        logger.setLevel(logging.INFO)\n",
    "        logger.info(\"verifychecks\", extra=properties)\n",
    "    else:\n",
    "        logger.setLevel(logging.ERROR)\n",
    "        logger.error(\"verifychecks\", extra=properties)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "def main() -> None:\n",
    "    root_span_name = f\"root#{process_name}#{current_ts}\"\n",
    "\n",
    "    with tracer.start_as_current_span(root_span_name, kind=SpanKind.INTERNAL) as root_span:\n",
    "        try:\n",
    "            root_span.add_event(\n",
    "                name=\"010-verify-lakehouse\",\n",
    "                attributes={\"lakehouse_name\": parking_lakehouse},\n",
    "            )\n",
    "            # Read interim cleansed data\n",
    "            parkingbay_sdf = spark.read.table(f\"`{workspace_name}`.{lakehouse_name}.interim.parking_bay\").filter(\n",
    "                col(\"load_id\") == lit(load_id)\n",
    "            )\n",
    "            sensordata_sdf = spark.read.table(f\"`{workspace_name}`.{lakehouse_name}.interim.sensor\").filter(\n",
    "                col(\"load_id\") == lit(load_id)\n",
    "            )\n",
    "\n",
    "            transform_load_dimension(parkingbay_sdf, sensordata_sdf, load_id, loaded_on)\n",
    "            nr_fact_parking = transform_load_fact_table(sensordata_sdf, load_id, loaded_on)\n",
    "\n",
    "            # validate the parking fact dataframe\n",
    "            checkpoint_result = validate_fact_parking(nr_fact_parking)\n",
    "            # data quality report\n",
    "            data_quality_metric_reporting(checkpoint_result, load_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"{process_name} process failed with error {e}\"\n",
    "            logger.exception(error_message)\n",
    "            root_span.set_status(StatusCode.ERROR, error_message)\n",
    "            root_span.record_exception(e)\n",
    "            raise\n",
    "        else:\n",
    "            root_span.set_status(StatusCode.OK)\n",
    "            logger.info(f\"{process_name} process is successful.\")\n",
    "        finally:\n",
    "            logger.info(f\"\\n** {process_name} process is complete. Check the logs for execution status. **\\n\\n\")\n",
    "\n",
    "    return None"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": null,
   "source": [
    "### Code execution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# Apply logic here incase this notebook need to be used as a library\n",
    "if execution_mode == \"all\":\n",
    "    print(f\"{execution_mode = }. Proceeding with the code execution.\")\n",
    "    main()\n",
    "else:\n",
    "    print(f\"Skipping the main function execution as {execution_mode = } and running it like a code module.\")"
   ],
   "outputs": []
  }
 ]
}