{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848835c6-2bc3-4234-bc92-dfe7e5e27fc4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Transform data for Parking Sensors\n",
    "\n",
    "TO DO: \n",
    "- Update the code logic using [03_transform.py](https://github.com/Azure-Samples/modern-data-warehouse-dataops/blob/feat/e2e-fabric-dataops-sample/e2e_samples/parking_sensors/databricks/notebooks/03_transform.py) as the reference.\n",
    "- Need to update the landing page references.\n",
    "- Add unit test cases in a separate notebook.\n",
    "\n",
    "About:\n",
    "\n",
    "- This notebook ingests data from the source needed by parking sensors sample. It then performs cleanup and standardization step. See [parking sensors page](https://github.com/Azure-Samples/modern-data-warehouse-dataops/tree/feat/e2e-fabric-dataops-sample/e2e_samples/fabric_dataops_sample/README.md) for more details about Parking Sensor sample using Microsoft Fabric.\n",
    "\n",
    "Assumptions/Pre-requisites:\n",
    "\n",
    "- Currently there is a known issue running cross workspace queries when workspace name has special characters. See [schema limitation](https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-schemas#public-preview-limitations) for more details. Avoid special characters if planning to query across workspaces with schema support. \n",
    "- All the assets needed are created by IaC step during migration.\n",
    "    - Config file needed: Files/sc-adls-main/config/application.cfg (derived using application.cfg.template during ci/cd process). Ensure \"transform\" section is updated with the required parameters for this notebook.\n",
    "- All the required lakehouse schemas and tables are created by the `nb-setup` notebook.\n",
    "- Input data standardization is completed by `nb-standardize` notebook.\n",
    "- Environment with common library otel_monitor_invoker.py and its associated python dependencies\n",
    "- Parking Sensor Lakehouse\n",
    "- Datasource: ADLS made available as a shortcut in Parking Sensor Lakehouse or Direct access to REST APIs.\n",
    "- Monitoring sink: AppInsights\n",
    "- Secrets repo: Key vault to store AppInsights connection information\n",
    "\n",
    "- All Lakehouses have schema support enabled (in Public preview as of Nov, 2024).\n",
    "- Execution\n",
    "  - A default lakehouse is associated during runtime where the required files and data are already staged. Multiple ways of invoking:\n",
    "    - [Api call](https://learn.microsoft.com/fabric/data-engineering/notebook-public-api#run-a-notebook-on-demand)\n",
    "    - [Part of a data pipeline](https://learn.microsoft.com/fabric/data-engineering/author-execute-notebook#parameterized-session-configuration-from-a-pipeline)\n",
    "    - [Using `%run` from another notebook](https://learn.microsoft.com/fabric/data-engineering/author-execute-notebook#reference-run-a-notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba9a02-ac84-4173-b832-70ddb65544ee",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Parameters and Library imports\n",
    "\n",
    "### Reading parameters (external from Fabric pipeline or default values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ed39d-6ad2-4c0e-8b18-8354c411027a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"defaultLakehouse\": {\n",
    "        \"name\": {\n",
    "            \"parameterName\": \"lakehouse_name\",\n",
    "            \"defaultValue\": \"{{ .lakehouse_name }}\"\n",
    "               } ,\n",
    "        \"id\": { \n",
    "            \"parameterName\": \"lakehouse_id\",\n",
    "            \"defaultValue\": \"{{ .lakehouse_id }}\"\n",
    "        } ,\n",
    "        \"workspaceId\": {\n",
    "            \"parameterName\": \"workspace_id\",\n",
    "            \"defaultValue\": \"{{ .workspace_id }}\"\n",
    "        }\n",
    "    },\n",
    "    \"mountPoints\": [\n",
    "        {\n",
    "            \"mountPoint\": \"/local_data\",\n",
    "            \"source\": {\n",
    "                \"parameterName\": \"local_mount\",\n",
    "                \"defaultValue\": \"abfss://{{ .workspace_id }}@onelake.dfs.fabric.microsoft.com/{{ .lakehouse_id }}/Files\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3feed-6727-484c-a437-02dcde99e165",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Unless `%%configure` is used to read external parameters - this cell should be the first one\n",
    "\n",
    "# This cell is tagged as Parameters cell. Parameters mentioned here are usually \\\n",
    "#    passed by the user at the time of notebook execution.\n",
    "# Ref: https://learn.microsoft.com/fabric/data-engineering/notebook-public-api#run-a-notebook-on-demand\n",
    "\n",
    "# Control how to run the notebook - \"all\" for entire notebook or \"module\" mode to use \\\n",
    "#      this notebook like a module (main execution will be skipped). Useful when performing\n",
    "#      testing using notebooks or funcitons from this notebook need to be called from another notebook.\n",
    "import configparser\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import otel_monitor_invoker as otel  # custom module part of env\n",
    "from opentelemetry.trace import SpanKind\n",
    "from opentelemetry.trace.status import StatusCode\n",
    "\n",
    "import os\n",
    "import ddo_transform.transform as t\n",
    "import ddo_transform.util as util\n",
    "\n",
    "from ruamel import yaml\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "\n",
    "from great_expectations.data_context import BaseDataContext\n",
    "from great_expectations.data_context.types.base import (\n",
    "    DataContextConfig,\n",
    "    DatasourceConfig,\n",
    "    FilesystemStoreBackendDefaults,\n",
    ")\n",
    "\n",
    "# execution_mode = \"module\" will skip the execution of the main fucntion. Use it for module like treatment\n",
    "#   \"all\" perform execution as well.\n",
    "execution_mode = \"all\"\n",
    "# Helpful if user wants to set a child process name etc. will be derived if not set by user\n",
    "job_exec_instance = \"\"\n",
    "# Helpful to derive any stage based globals\n",
    "env_stage = \"dev\"\n",
    "# Common config file path hosted on attached lakehouse - path relative to Files/\n",
    "config_file_path = \"sc-adls-main/config/application.cfg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6fd6c-d786-47d4-a075-b5362056d5ca",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# # only need to run when developing the notebook to format the code\n",
    "# import jupyter_black\n",
    "\n",
    "# jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3170fc-741f-4bfb-820c-43d9d26546f6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Validate input parameters\n",
    "in_errors = []\n",
    "if execution_mode not in [\"all\", \"module\"]:\n",
    "    in_errors.append(f\"Invalid value: {execution_mode = }. It must be either 'all' or 'module'.\")\n",
    "if not notebookutils.fs.exists(f\"Files/{config_file_path}\"):\n",
    "    in_errors.append(f\"Specified config - `Files/{config_file_path}` doesn't exist.\")\n",
    "\n",
    "if in_errors:\n",
    "    raise ValueError(f\"Input parameter valiadtion failed. Erros are:\\n{in_errors}\")\n",
    "else:\n",
    "    print(\"Input parameter verification completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c196060-9e09-420d-93c7-809b4a2cd656",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Network mounts\n",
    "\n",
    "- Scope is set to Job/session - so these need to be run once per session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374da7d8-5ac9-48b4-88ce-6c563422882a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# -- Helps to read config files from onelake location\n",
    "runtime_context = notebookutils.runtime.context\n",
    "local_data_mount_path = f'{notebookutils.fs.getMountPath(\"/local_data\")}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "## Temporary workaround. Remove when issue with OneLake authentication in Notebook is resolved.\n",
    "file_path = f\"{local_data_mount_path}/{config_file_path}\"\n",
    "os.listdir(f\"{local_data_mount_path}/\")\n",
    "os.listdir(f\"{local_data_mount_path}/sc-adls-main\")\n",
    "os.listdir(f\"{local_data_mount_path}/sc-adls-main/config\")\n",
    "with open(file_path, \"r\") as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "print(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3fea57-3e3f-40cc-9fb5-59e595a11957",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Read user provided config values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c7bcb-0bce-4a1a-b69c-7ae4ecb4d19f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation())\n",
    "config.read(f\"{local_data_mount_path}/{config_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a353b5-92ec-4f24-81e4-7ff29e9a44f4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# When we config parser if the value is not present in the specified section, it will be\n",
    "#   read from \"DEFAULT\" section.\n",
    "config_section_name = \"transform\"\n",
    "process_name = config.get(config_section_name, \"process_name\")\n",
    "parking_ws = config.get(config_section_name, \"workspace_name\")\n",
    "parking_ws_id = config.get(config_section_name, \"workspace_id\")\n",
    "parking_lakehouse = config.get(config_section_name, \"lakehouse_name\")\n",
    "\n",
    "# Add any other parameters that need to be read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631249bb-689d-4c69-ba5f-9ea139042ad1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Internal (derived) parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31711b30-031d-4790-9585-1833f6088217",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# default is micro-seconds, changing to milli-seconds\n",
    "current_ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S%f\")[:-3]\n",
    "job_exec_instance = job_exec_instance if job_exec_instance else f\"{process_name}#{current_ts}\"\n",
    "execution_user_name = runtime_context[\"userName\"]\n",
    "\n",
    "# Add any other parameters needed by the process\n",
    "loaded_on = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef283ac8-dd33-4568-9a56-e9a8aad5c4b2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Monitoring and observability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34deea-70bb-4316-85dc-08e38681da76",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### AppInsights connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ff164-80cc-44e3-b4af-f4809a57e77c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "connection_string = notebookutils.credentials.getSecret(\n",
    "    config.get(\"keyvault\", \"uri\"), config.get(\"otel\", \"appinsights_connection_name\")\n",
    ")\n",
    "otlp_exporter = otel.OpenTelemetryAppInsightsExporter(conn_string=connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8bc908-8c78-4335-ab06-13f545245374",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Populate resource information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4e588-f46f-40dd-806e-078c7c994e0a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Resource references\n",
    "# - Naming conventions: https://opentelemetry.io/docs/specs/semconv/general/attribute-naming/\n",
    "# - For a complete list of reserved ones: https://opentelemetry.io/docs/concepts/semantic-conventions/\n",
    "#  NOTE: service.namespace,service.name,service.instance.id triplet MUST be globally unique.\n",
    "#     The ID helps to distinguish instances of the same service that exist at the same time\n",
    "#     (e.g. instances of a horizontally scaled service)\n",
    "resource_attributes = {\n",
    "    # ---------- Reserved attribute names\n",
    "    \"service.name\": config.get(config_section_name, \"service_name\"),\n",
    "    \"service.version\": config.get(config_section_name, \"service_version\"),\n",
    "    \"service.namespace\": \"parking-sensor\",\n",
    "    \"service.instance.id\": notebookutils.runtime.context[\"activityId\"],\n",
    "    \"process.executable.name\": process_name,\n",
    "    \"deployment.environment\": env_stage,\n",
    "    # ---------- custom attributes - we can also add common attributes like appid, domain id etc\n",
    "    #     here or get them from process reference data using processname as the key.\n",
    "    # runtime context has a lot if useful info - adding it as is.\n",
    "    \"jobexec.context\": f\"{notebookutils.runtime.context}\",  # convert to string otherwise it will fail\n",
    "    \"jobexec.cluster.region\": spark.sparkContext.getConf().get(\"spark.cluster.region\"),\n",
    "    \"jobexec.app.name\": spark.sparkContext.getConf().get(\"spark.app.name\"),\n",
    "    \"jobexec.instance.name\": job_exec_instance,\n",
    "}\n",
    "\n",
    "# Typically, logging is performed within the context of a span.\n",
    "#   This allows log messages to be associated with trace information through the use of trace IDs and span IDs.\n",
    "#   As a result, it's generally not necessary to include resource information in log messages.\n",
    "# Note that trace IDs and span IDs will be null when logging is performed outside of a span context.\n",
    "log_attributes = {\"jobexec.instance.name\": job_exec_instance}\n",
    "trace_attributes = resource_attributes\n",
    "\n",
    "tracer = otlp_exporter.get_otel_tracer(trace_resource_attributes=trace_attributes, tracer_name=f\"tracer-{process_name}\")\n",
    "logger = otlp_exporter.get_otel_logger(\n",
    "    log_resource_attributes=log_attributes,\n",
    "    logger_name=f\"logger-{process_name}\",\n",
    "    add_console_handler=False,\n",
    ")\n",
    "logger.setLevel(\"INFO\")  # deafult is WARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e216da-6f45-4f0d-83fd-68fce17eb9ec",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b860f7-3ab4-40c4-a277-41e15c7d8a52",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Code functions\n",
    "\n",
    "- When using %run we can expose these functions to the calling notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd12d8-92cc-4f36-a1d5-097aab9ba693",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_lakehouse_details(lakehouse_name: str) -> dict:\n",
    "    logger.info(\"Performing lakehouse existence check.\")\n",
    "    try:\n",
    "        details = notebookutils.lakehouse.get(name=lakehouse_name)\n",
    "    except Exception:\n",
    "        logger.exception(f\"Specified lakehouse - {lakehouse_name} doesn't exist. Aborting..\")\n",
    "        raise\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b2890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (https://github.com/Azure-Samples/modern-data-warehouse-dataops/blob/main/e2e_samples/parking_sensors/databricks/notebooks/03_transform.py)\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "### Transform and load Dimension tables\n",
    "def transform_load_dimension(parkingbay_sdf, sensordata_sdf, base_path, load_id, loaded_on) -> None:\n",
    "    # Read existing Dimensions\n",
    "    dim_parkingbay_sdf = spark.read.table(\"dw.dim_parking_bay\")\n",
    "    dim_location_sdf = spark.read.table(\"dw.dim_location\")\n",
    "    dim_st_marker = spark.read.table(\"dw.dim_st_marker\")\n",
    "\n",
    "    # Transform\n",
    "    new_dim_parkingbay_sdf = t.process_dim_parking_bay(parkingbay_sdf, dim_parkingbay_sdf, load_id, loaded_on).cache()\n",
    "    new_dim_location_sdf = t.process_dim_location(sensordata_sdf, dim_location_sdf, load_id, loaded_on).cache()\n",
    "    new_dim_st_marker_sdf = t.process_dim_st_marker(sensordata_sdf, dim_st_marker, load_id, loaded_on).cache()\n",
    "\n",
    "    # Insert new rows into Dimension tables\n",
    "    new_dim_parkingbay_sdf.write.insertInto(\"dw.dim_parking_bay\", overwrite=True)\n",
    "    new_dim_location_sdf.write.insertInto(\"dw.dim_location\", overwrite=True)\n",
    "    new_dim_st_marker_sdf.write.insertInto(\"dw.dim_st_marker\", overwrite=True)\n",
    "\n",
    "\n",
    "### Transform and load Fact tables\n",
    "def transform_load_fact_table(sensordata_sdf, load_id, loaded_on) -> DataFrame:\n",
    "    # Read existing Dimensions\n",
    "    dim_parkingbay_sdf = spark.read.table(\"dw.dim_parking_bay\")\n",
    "    dim_location_sdf = spark.read.table(\"dw.dim_location\")\n",
    "    dim_st_marker = spark.read.table(\"dw.dim_st_marker\")\n",
    "\n",
    "    # Process\n",
    "    nr_fact_parking = t.process_fact_parking(sensordata_sdf, dim_parkingbay_sdf, dim_location_sdf, dim_st_marker, load_id, loaded_on)\n",
    "\n",
    "    # Insert new rows\n",
    "    nr_fact_parking.write.mode(\"append\").insertInto(\"dw.fact_parking\")   \n",
    "\n",
    "    return nr_fact_parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf058a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_fact_parking(fact_parking_df):\n",
    "    \n",
    "    root_directory = \"/lakehouse/default/Files/transform_validation\"\n",
    "\n",
    "    # 1. Configure DataContext\n",
    "    # https://docs.greatexpectations.io/docs/terms/data_context\n",
    "    data_context_config = DataContextConfig(\n",
    "        datasources={\n",
    "            \"transformed_data_source\": DatasourceConfig(\n",
    "                class_name=\"Datasource\",\n",
    "                execution_engine={\"class_name\": \"SparkDFExecutionEngine\",\n",
    "                \"force_reuse_spark_context\": True},\n",
    "                data_connectors={\n",
    "                    \"transformed_data_connector\": {\n",
    "                        \"module_name\": \"great_expectations.datasource.data_connector\",\n",
    "                        \"class_name\": \"RuntimeDataConnector\",\n",
    "                        \"batch_identifiers\": [\n",
    "                            \"environment\",\n",
    "                            \"pipeline_run_id\",\n",
    "                        ],\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        },\n",
    "        store_backend_defaults=FilesystemStoreBackendDefaults(root_directory=root_directory)\n",
    "    )\n",
    "    context = BaseDataContext(project_config=data_context_config)\n",
    "\n",
    "\n",
    "    # 2. Create a BatchRequest based on parkingbay_sdf dataframe.\n",
    "    # https://docs.greatexpectations.io/docs/terms/batch\n",
    "    batch_request = RuntimeBatchRequest(\n",
    "        datasource_name=\"transformed_data_source\",\n",
    "        data_connector_name=\"transformed_data_connector\",\n",
    "        data_asset_name=\"paringbaydataaset\",  # This can be anything that identifies this data_asset for you\n",
    "        batch_identifiers={\n",
    "            \"environment\": \"stage\",\n",
    "            \"pipeline_run_id\": \"pipeline_run_id\",\n",
    "        },\n",
    "        runtime_parameters={\"batch_data\": fact_parking_df},  # Your dataframe goes here\n",
    "    )\n",
    "\n",
    "\n",
    "    # 3. Define Expecation Suite and corresponding Data Expectations\n",
    "    # https://docs.greatexpectations.io/docs/terms/expectation_suite\n",
    "    expectation_suite_name = \"Transfomed_data_exception_suite_basic\"\n",
    "    context.create_expectation_suite(expectation_suite_name=expectation_suite_name, overwrite_existing=True)\n",
    "    validator = context.get_validator(\n",
    "        batch_request=batch_request,\n",
    "        expectation_suite_name=expectation_suite_name,\n",
    "    )\n",
    "    # Add Validatons to suite\n",
    "    # Check available expectations: validator.list_available_expectation_types()\n",
    "    # https://legacy.docs.greatexpectations.io/en/latest/autoapi/great_expectations/expectations/index.html\n",
    "    # https://legacy.docs.greatexpectations.io/en/latest/reference/core_concepts/expectations/standard_arguments.html#meta\n",
    "    validator.expect_column_values_to_not_be_null(column=\"status\")\n",
    "    validator.expect_column_values_to_be_of_type(column=\"status\", type_=\"StringType\")\n",
    "    validator.expect_column_values_to_not_be_null(column=\"dim_time_id\")\n",
    "    validator.expect_column_values_to_be_of_type(column=\"dim_time_id\", type_=\"IntegerType\")\n",
    "    validator.expect_column_values_to_not_be_null(column=\"dim_parking_bay_id\")\n",
    "    validator.expect_column_values_to_be_of_type(column=\"dim_parking_bay_id\", type_=\"StringType\")\n",
    "    #validator.validate() # To run run validations without checkpoint\n",
    "    validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "\n",
    "\n",
    "    # 4. Configure a checkpoint and run Expectation suite using checkpoint\n",
    "    # https://docs.greatexpectations.io/docs/terms/checkpoint\n",
    "    my_checkpoint_name = \"Transformed Data\"\n",
    "    checkpoint_config = {\n",
    "        \"name\": my_checkpoint_name,\n",
    "        \"config_version\": 1.0,\n",
    "        \"class_name\": \"SimpleCheckpoint\",\n",
    "        \"run_name_template\": \"%Y%m%d-%H%M%S-my-run-name-template\",\n",
    "    }\n",
    "    my_checkpoint = context.test_yaml_config(yaml.dump(checkpoint_config,default_flow_style=False))\n",
    "    context.add_checkpoint(**checkpoint_config)\n",
    "    # Run Checkpoint passing in expectation suite\n",
    "    checkpoint_result = context.run_checkpoint(\n",
    "        checkpoint_name=my_checkpoint_name,\n",
    "        validations=[\n",
    "            {\n",
    "                \"batch_request\": batch_request,\n",
    "                \"expectation_suite_name\": expectation_suite_name,\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return checkpoint_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c63bcb",
   "metadata": {},
   "source": [
    "### Data Quality Metric Reporting\n",
    "This parses the results of the checkpoint and sends it to AppInsights / Azure Monitor for reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d20302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_metric_reporting(checkpoint_result, load_id):\n",
    "    ## Report Data Quality Metrics to Azure Monitor using python Azure Monitor open-census exporter \n",
    "    result_dic = checkpoint_result.to_json_dict()\n",
    "    key_name=[key for key in result_dic['run_results'].keys()][0]\n",
    "    results = result_dic['run_results'][key_name]['validation_result']['results']\n",
    "\n",
    "    checks = {'check_name':checkpoint_result['checkpoint_config']['name'],'pipelinerunid':load_id}\n",
    "    for i in range(len(results)):\n",
    "        validation_name= results[i]['expectation_config']['expectation_type'] + \"_on_\" + results[i]['expectation_config']['kwargs']['column']\n",
    "        checks[validation_name]=results[i]['success']\n",
    "        \n",
    "    properties = {'custom_dimensions': checks}\n",
    "\n",
    "    if checkpoint_result.success is True:\n",
    "        logger.setLevel(logging.INFO)\n",
    "        logger.info('verifychecks', extra=properties)\n",
    "    else:\n",
    "        logger.setLevel(logging.ERROR)\n",
    "        logger.error('verifychecks', extra=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b82efa2-4e54-46d6-bcb5-a96219dffa9f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "def main() -> None:\n",
    "    root_span_name = f\"root#{process_name}#{current_ts}\"\n",
    "\n",
    "    with tracer.start_as_current_span(root_span_name, kind=SpanKind.INTERNAL) as root_span:\n",
    "        try:\n",
    "            root_span.add_event(\n",
    "                name=\"010-verify-lakehouse\",\n",
    "                attributes={\"lakehouse_name\": parking_lakehouse},\n",
    "            )\n",
    "            # Read interim cleansed data\n",
    "            parkingbay_sdf = spark.read.table(\"interim.parking_bay\").filter(col('load_id') == lit(load_id))\n",
    "            sensordata_sdf = spark.read.table(\"interim.sensor\").filter(col('load_id') == lit(load_id))\n",
    "\n",
    "            base_path = \"\"\n",
    "            transform_load_dimension(parkingbay_sdf, sensordata_sdf, base_path, load_id, loaded_on)\n",
    "            nr_fact_parking = transform_load_fact_table(sensordata_sdf, load_id, loaded_on)\n",
    "\n",
    "            # validate the parking fact dataframe\n",
    "            checkpoint_result = validate_fact_parking(nr_fact_parking)\n",
    "            # data quality report\n",
    "            data_quality_metric_reporting(checkpoint_result, load_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"{process_name} process failed with error {e}\"\n",
    "            logger.exception(error_message)\n",
    "            root_span.set_status(StatusCode.ERROR, error_message)\n",
    "            root_span.record_exception(e)\n",
    "            raise\n",
    "        else:\n",
    "            root_span.set_status(StatusCode.OK)\n",
    "            logger.info(f\"{process_name} process is successful.\")\n",
    "        finally:\n",
    "            logger.info(f\"\\n** {process_name} process is complete. Check the logs for execution status. **\\n\\n\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae060f3-e2d6-4440-95cd-72d2e08205f3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a30277-8696-4731-859a-c1b61615ea94",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Apply logic here incase this notebook need to be used as a library\n",
    "\n",
    "# Log into AppInsights and verify the following for telemetry events produced by OpenTelemetry.\n",
    "# dependencies\n",
    "# | where name hasprefix \"root#nb-030\"\n",
    "# //\n",
    "# exceptions\n",
    "# //\n",
    "# traces\n",
    "\n",
    "\n",
    "if execution_mode == \"all\":\n",
    "    print(f\"{execution_mode = }. Proceeding with the code execution.\")\n",
    "    main()\n",
    "else:\n",
    "    print(f\"Skipping the main funciton execution as {execution_mode = } and running it like a code module.\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "{{ .environment_id }}",
    "workspaceId": "{{ .workspace_id }}"
   },
   "lakehouse": {
    "default_lakehouse": "{{ .lakehouse_id }}",
    "default_lakehouse_name": "{{ .lakehouse_name }}",
    "default_lakehouse_workspace_id": "{{ .workspace_id }}"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
