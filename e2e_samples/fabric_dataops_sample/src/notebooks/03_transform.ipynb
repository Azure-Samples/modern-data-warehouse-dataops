{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848835c6-2bc3-4234-bc92-dfe7e5e27fc4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Transform data for Parking Sensors\n",
    "\n",
    "TO DO: \n",
    "- Update the code logic using [03_transform.py](https://github.com/Azure-Samples/modern-data-warehouse-dataops/blob/feat/e2e-fabric-dataops-sample/e2e_samples/parking_sensors/databricks/notebooks/03_transform.py) as the reference.\n",
    "- Need to update the landing page references.\n",
    "- Add unit test cases in a separate notebook.\n",
    "\n",
    "About:\n",
    "\n",
    "- This notebook ingests data from the source needed by parking sensors sample. It then performs cleanup and standardization step. See [parking sensors page](https://github.com/Azure-Samples/modern-data-warehouse-dataops/tree/feat/e2e-fabric-dataops-sample/e2e_samples/fabric_dataops_sample/README.md) for more details about Parking Sensor sample using Microsoft Fabric.\n",
    "\n",
    "Assumptions/Pre-requisites:\n",
    "\n",
    "- Currently there is a known issue running cross workspace queries when workspace name has special characters. See [schema limitation](https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-schemas#public-preview-limitations) for more details. Avoid special characters if planning to query across workspaces with schema support. \n",
    "- All the assets needed are created by IaC step during migration.\n",
    "    - Config file needed: Files/sc-adls-main/config/application.cfg (derived using application.cfg.template during ci/cd process). Ensure \"transform\" section is updated with the required parameters for this notebook.\n",
    "- All the required lakehouse schemas and tables are created by the `nb-setup` notebook.\n",
    "- Input data standardization is completed by `nb-standardize` notebook.\n",
    "- Environment with common library otel_monitor_invoker.py and its associated python dependencies\n",
    "- Parking Sensor Lakehouse\n",
    "- Datasource: ADLS made available as a shortcut in Parking Sensor Lakehouse or Direct access to REST APIs.\n",
    "- Monitoring sink: AppInsights\n",
    "- Secrets repo: Key vault to store AppInsights connection information\n",
    "\n",
    "- All Lakehouses have schema support enabled (in Public preview as of Nov, 2024).\n",
    "- Execution\n",
    "  - A default lakehouse is associated during runtime where the required files and data are already staged. Multiple ways of invoking:\n",
    "    - [Api call](https://learn.microsoft.com/fabric/data-engineering/notebook-public-api#run-a-notebook-on-demand)\n",
    "    - [Part of a data pipeline](https://learn.microsoft.com/fabric/data-engineering/author-execute-notebook#parameterized-session-configuration-from-a-pipeline)\n",
    "    - [Using `%run` from another notebook](https://learn.microsoft.com/fabric/data-engineering/author-execute-notebook#reference-run-a-notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba9a02-ac84-4173-b832-70ddb65544ee",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Parameters and Library imports\n",
    "\n",
    "### Reading parameters (external from Fabric pipeline or default values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ed39d-6ad2-4c0e-8b18-8354c411027a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"defaultLakehouse\": {\n",
    "        \"name\": {\n",
    "            \"parameterName\": \"lakehouse_name\",\n",
    "            \"defaultValue\": \"{{ .lakehouse_name }}\"\n",
    "               } ,\n",
    "        \"id\": { \n",
    "            \"parameterName\": \"lakehouse_id\",\n",
    "            \"defaultValue\": \"{{ .lakehouse_id }}\"\n",
    "        } ,\n",
    "        \"workspaceId\": {\n",
    "            \"parameterName\": \"workspace_id\",\n",
    "            \"defaultValue\": \"{{ .workspace_id }}\"\n",
    "        }\n",
    "    },\n",
    "    \"mountPoints\": [\n",
    "        {\n",
    "            \"mountPoint\": \"/local_data\",\n",
    "            \"source\": {\n",
    "                \"parameterName\": \"local_mount\",\n",
    "                \"defaultValue\": \"abfss://{{ .workspace_id }}@onelake.dfs.fabric.microsoft.com/{{ .lakehouse_id }}/Files\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3feed-6727-484c-a437-02dcde99e165",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Unless `%%configure` is used to read external parameters - this cell should be the first one\n",
    "\n",
    "# This cell is tagged as Parameters cell. Parameters mentioned here are usually \\\n",
    "#    passed by the user at the time of notebook execution.\n",
    "# Ref: https://learn.microsoft.com/fabric/data-engineering/notebook-public-api#run-a-notebook-on-demand\n",
    "\n",
    "# Control how to run the notebook - \"all\" for entire notebook or \"module\" mode to use \\\n",
    "#      this notebook like a module (main execution will be skipped). Useful when performing\n",
    "#      testing using notebooks or funcitons from this notebook need to be called from another notebook.\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "\n",
    "import otel_monitor_invoker as otel  # custom module part of env\n",
    "from opentelemetry.trace import SpanKind\n",
    "from opentelemetry.trace.status import StatusCode\n",
    "\n",
    "# execution_mode = \"module\" will skip the execution of the main fucntion. Use it for module like treatment\n",
    "#   \"all\" perform execution as well.\n",
    "execution_mode = \"all\"\n",
    "# Helpful if user wants to set a child process name etc. will be derived if not set by user\n",
    "job_exec_instance = \"\"\n",
    "# Helpful to derive any stage based globals\n",
    "env_stage = \"dev\"\n",
    "# Common config file path hosted on attached lakehouse - path relative to Files/\n",
    "config_file_path = \"sc-adls-main/config/application.cfg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6fd6c-d786-47d4-a075-b5362056d5ca",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# # only need to run when developing the notebook to format the code\n",
    "# import jupyter_black\n",
    "\n",
    "# jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3170fc-741f-4bfb-820c-43d9d26546f6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Validate input parameters\n",
    "in_errors = []\n",
    "if execution_mode not in [\"all\", \"module\"]:\n",
    "    in_errors.append(f\"Invalid value: {execution_mode = }. It must be either 'all' or 'module'.\")\n",
    "if not notebookutils.fs.exists(f\"Files/{config_file_path}\"):\n",
    "    in_errors.append(f\"Specified config - `Files/{config_file_path}` doesn't exist.\")\n",
    "\n",
    "if in_errors:\n",
    "    raise ValueError(f\"Input parameter valiadtion failed. Erros are:\\n{in_errors}\")\n",
    "else:\n",
    "    print(\"Input parameter verification completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c196060-9e09-420d-93c7-809b4a2cd656",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Network mounts\n",
    "\n",
    "- Scope is set to Job/session - so these need to be run once per session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374da7d8-5ac9-48b4-88ce-6c563422882a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# -- Helps to read config files from onelake location\n",
    "runtime_context = notebookutils.runtime.context\n",
    "local_data_mount_path = f'{notebookutils.fs.getMountPath(\"/local_data\")}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3fea57-3e3f-40cc-9fb5-59e595a11957",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Read user provided config values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c7bcb-0bce-4a1a-b69c-7ae4ecb4d19f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation())\n",
    "config.read(f\"{local_data_mount_path}/{config_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a353b5-92ec-4f24-81e4-7ff29e9a44f4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# When we config parser if the value is not present in the specified section, it will be\n",
    "#   read from \"DEFAULT\" section.\n",
    "config_section_name = \"transform\"\n",
    "process_name = config.get(config_section_name, \"process_name\")\n",
    "parking_ws = config.get(config_section_name, \"workspace_name\")\n",
    "parking_ws_id = config.get(config_section_name, \"workspace_id\")\n",
    "parking_lakehouse = config.get(config_section_name, \"lakehouse_name\")\n",
    "\n",
    "# Add any other parameters that need to be read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631249bb-689d-4c69-ba5f-9ea139042ad1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Internal (derived) parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31711b30-031d-4790-9585-1833f6088217",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# default is micro-seconds, changing to milli-seconds\n",
    "current_ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S%f\")[:-3]\n",
    "job_exec_instance = job_exec_instance if job_exec_instance else f\"{process_name}#{current_ts}\"\n",
    "execution_user_name = runtime_context[\"userName\"]\n",
    "\n",
    "# Add any other parameters needed by the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef283ac8-dd33-4568-9a56-e9a8aad5c4b2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Monitoring and observability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34deea-70bb-4316-85dc-08e38681da76",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### AppInsights connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ff164-80cc-44e3-b4af-f4809a57e77c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "connection_string = notebookutils.credentials.getSecret(\n",
    "    config.get(\"keyvault\", \"uri\"), config.get(\"otel\", \"appinsights_connection_name\")\n",
    ")\n",
    "otlp_exporter = otel.OpenTelemetryAppInsightsExporter(conn_string=connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8bc908-8c78-4335-ab06-13f545245374",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Populate resource information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4e588-f46f-40dd-806e-078c7c994e0a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Resource references\n",
    "# - Naming conventions: https://opentelemetry.io/docs/specs/semconv/general/attribute-naming/\n",
    "# - For a complete list of reserved ones: https://opentelemetry.io/docs/concepts/semantic-conventions/\n",
    "#  NOTE: service.namespace,service.name,service.instance.id triplet MUST be globally unique.\n",
    "#     The ID helps to distinguish instances of the same service that exist at the same time\n",
    "#     (e.g. instances of a horizontally scaled service)\n",
    "resource_attributes = {\n",
    "    # ---------- Reserved attribute names\n",
    "    \"service.name\": config.get(config_section_name, \"service_name\"),\n",
    "    \"service.version\": config.get(config_section_name, \"service_version\"),\n",
    "    \"service.namespace\": \"parking-sensor\",\n",
    "    \"service.instance.id\": notebookutils.runtime.context[\"activityId\"],\n",
    "    \"process.executable.name\": process_name,\n",
    "    \"deployment.environment\": env_stage,\n",
    "    # ---------- custom attributes - we can also add common attributes like appid, domain id etc\n",
    "    #     here or get them from process reference data using processname as the key.\n",
    "    # runtime context has a lot if useful info - adding it as is.\n",
    "    \"jobexec.context\": f\"{notebookutils.runtime.context}\",  # convert to string otherwise it will fail\n",
    "    \"jobexec.cluster.region\": spark.sparkContext.getConf().get(\"spark.cluster.region\"),\n",
    "    \"jobexec.app.name\": spark.sparkContext.getConf().get(\"spark.app.name\"),\n",
    "    \"jobexec.instance.name\": job_exec_instance,\n",
    "}\n",
    "\n",
    "# Typically, logging is performed within the context of a span.\n",
    "#   This allows log messages to be associated with trace information through the use of trace IDs and span IDs.\n",
    "#   As a result, it's generally not necessary to include resource information in log messages.\n",
    "# Note that trace IDs and span IDs will be null when logging is performed outside of a span context.\n",
    "log_attributes = {\"jobexec.instance.name\": job_exec_instance}\n",
    "trace_attributes = resource_attributes\n",
    "\n",
    "tracer = otlp_exporter.get_otel_tracer(trace_resource_attributes=trace_attributes, tracer_name=f\"tracer-{process_name}\")\n",
    "logger = otlp_exporter.get_otel_logger(\n",
    "    log_resource_attributes=log_attributes,\n",
    "    logger_name=f\"logger-{process_name}\",\n",
    "    add_console_handler=False,\n",
    ")\n",
    "logger.setLevel(\"INFO\")  # deafult is WARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e216da-6f45-4f0d-83fd-68fce17eb9ec",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b860f7-3ab4-40c4-a277-41e15c7d8a52",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Code functions\n",
    "\n",
    "- When using %run we can expose these functions to the calling notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd12d8-92cc-4f36-a1d5-097aab9ba693",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_lakehouse_details(lakehouse_name: str) -> dict:\n",
    "    logger.info(\"Performing lakehouse existence check.\")\n",
    "    try:\n",
    "        details = notebookutils.lakehouse.get(name=lakehouse_name)\n",
    "    except Exception:\n",
    "        logger.exception(f\"Specified lakehouse - {lakehouse_name} doesn't exist. Aborting..\")\n",
    "        raise\n",
    "    return details\n",
    "\n",
    "\n",
    "# create functions as needed for standardization process\n",
    "# (https://github.com/Azure-Samples/modern-data-warehouse-dataops/blob/feat/e2e-fabric-dataops-sample/e2e_samples/parking_sensors/databricks/notebooks/03_transform.py)\n",
    "def dummy_function() -> None:\n",
    "\n",
    "    # your code goes here\n",
    "    pass\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b82efa2-4e54-46d6-bcb5-a96219dffa9f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Template for main function which calls all other functions\n",
    "# Note that, there is a root span using OTEL library\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    root_span_name = f\"root#{process_name}#{current_ts}\"\n",
    "\n",
    "    with tracer.start_as_current_span(root_span_name, kind=SpanKind.INTERNAL) as root_span:\n",
    "        try:\n",
    "            root_span.add_event(\n",
    "                name=\"010-verify-lakehouse\",\n",
    "                attributes={\"lakehouse_name\": parking_lakehouse},\n",
    "            )\n",
    "            lh_details = get_lakehouse_details(parking_lakehouse)\n",
    "            # Always use absolute paths when referring to Onelake locations\n",
    "            lh_table_path = f'{lh_details[\"properties\"][\"abfsPath\"]}/Tables'\n",
    "            lh_file_path = f'{lh_details[\"properties\"][\"abfsPath\"]}/Files'\n",
    "            print(lh_table_path, lh_file_path)\n",
    "\n",
    "            # root_span.add_event(\n",
    "            #     name=\"<<your second event>>\", attributes={attributes in dictonary format for second event}\n",
    "            # )\n",
    "            # code function for second event\n",
    "\n",
    "            # root_span.add_event(\n",
    "            #     name=\"<<your nth event>>\", attributes={attributes in dictonary format for nth event}\n",
    "            # )\n",
    "            # code for nth event\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"{process_name} process failed with error {e}\"\n",
    "            logger.exception(error_message)\n",
    "            root_span.set_status(StatusCode.ERROR, error_message)\n",
    "            root_span.record_exception(e)\n",
    "            raise\n",
    "        else:\n",
    "            root_span.set_status(StatusCode.OK)\n",
    "            logger.info(f\"{process_name} process is successful.\")\n",
    "        finally:\n",
    "            logger.info(f\"\\n** {process_name} process is complete. Check the logs for execution status. **\\n\\n\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae060f3-e2d6-4440-95cd-72d2e08205f3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a30277-8696-4731-859a-c1b61615ea94",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Apply logic here incase this notebook need to be used as a library\n",
    "\n",
    "# Log into AppInsights and verify the following for telemetry events produced by OpenTelemetry.\n",
    "# dependencies\n",
    "# | where name hasprefix \"root#nb-030\"\n",
    "# //\n",
    "# exceptions\n",
    "# //\n",
    "# traces\n",
    "\n",
    "\n",
    "if execution_mode == \"all\":\n",
    "    print(f\"{execution_mode = }. Proceeding with the code execution.\")\n",
    "    main()\n",
    "else:\n",
    "    print(f\"Skipping the main funciton execution as {execution_mode = } and running it like a code module.\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "{{ .environment_id }}",
    "workspaceId": "{{ .workspace_id }}"
   },
   "lakehouse": {
    "default_lakehouse": "{{ .lakehouse_id }}",
    "default_lakehouse_name": "{{ .lakehouse_name }}",
    "default_lakehouse_workspace_id": "{{ .workspace_id }}"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
