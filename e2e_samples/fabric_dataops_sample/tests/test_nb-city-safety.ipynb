{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing data focused unit testing\n",
    "\n",
    "This notebook shows steps involved for performing data focused unit testing on ETL which is part of [nb-city-safety.ipynb](../notebooks/nb-city-safety.ipynb). A sample code centic unit test case is already of that notebook.\n",
    "\n",
    "See [DataTesting.md](../docs/DataTesting.md) for different ways to organize and run unit testcases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting the Notebook - Run only when developing the Notebook\n",
    "\n",
    "- Reference: https://learn.microsoft.com/en-us/fabric/data-engineering/author-notebook-format-code#extend-fabric-notebooks]\n",
    "- **WARNING**: When using formatting using `jupyter_black` it will remove any *cell magic commands* present. You should add them back.\n",
    "\n",
    "```python\n",
    "import jupyter_black\n",
    "jupyter_black.load()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for testing - Test environement setup\n",
    "\n",
    "Test environment is often a one time activity for setup and needs regular updates to test data to account for new business scenarios. Other things to make a note are:\n",
    "\n",
    "- It is very important to account for all business scenarios. Data should cover all business scenarios.\n",
    "- Volume should be low so that run times are low.\n",
    "- Maintain data relations as much as possible, so that the same data set can be used for e2e runs (controlled data set).\n",
    "- Data env cleanup steps should also be identified incase of reruns.\n",
    "- These can also be part of the code deployments using repo.\n",
    "\n",
    "In this example, Using the same source as the Production code (OpenData sets for city safety data), we are creating a controlled volume datasets and storing them in a storage account created for this purpose. During testing, we leverage the configurable settings of the application to use these controlled data sets as the source. This way we are testing the entire notebook (our componennt/unit under testing) with minimal/no code pathcing. This also tests the integrations with other components and configuration settings. More importanntly, we can test the outputs to make sure they are inline with business requirements.\n",
    "\n",
    "\n",
    "```python\n",
    "import random\n",
    "import json\n",
    "\n",
    "target_adls = \"abfss://citydatacontainer@azureopendatastoragedev.dfs.core.windows.net/Safety/Release/\"\n",
    "wasbs_path = \"wasbs://citydatacontainer@azureopendatastorage.blob.core.windows.net/Safety/Release/\"\n",
    "source_counts = {}\n",
    "\n",
    "for city in (\"Boston\", \"Chicago\", \"NewYorkCity\", \"Seattle\", \"SanFrancisco\"):\n",
    "    data_df = spark.read.parquet(f\"{wasbs_path}/city={city}\")\n",
    "    num_records = random.randint(50, 500)\n",
    "    sample_df = data_df.sample(False, num_records / data_df.count())\n",
    "    sample_df.repartition(1).write.format(\"parquet\").mode(\"overwrite\").save(\n",
    "        f\"{target_adls}/city={city}\"\n",
    "    )\n",
    "    source_counts[city] = sample_df.count()\n",
    "\n",
    "count_df = spark.createDataFrame(list(source_counts.items()), [\"city\", \"count\"])\n",
    "count_df.repartition(1).write.format(\"json\").mode(\"overwrite\").save(\n",
    "    f\"{target_adls}/source_counts\"\n",
    ")  # need - `.option(\"multiLine\", true)` during read\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit test - Data focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import MagicMock, patch, call\n",
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "# this makes the ipytest magic available and raise_on_error causes notebook failure incase of errors\n",
    "ipytest.autoconfig(raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# ------------------- Keep Only External parameters in this cell. --------------------------------------------\n",
    "execution_mode = \"normal\"\n",
    "onelake_name = \"onelake\"\n",
    "env_stage = \"dev\"\n",
    "log_level = \"WARN\"\n",
    "config_file_path = f\"{notebookutils.nbResPath}/builtin/city_safety.cfg\" # TO DO: Where are we reading this file from?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load function definitions that need to be tested into current context\n",
    "\n",
    "Things to note:\n",
    "\n",
    "- `%run ` currently doesn't take variable values. \n",
    "- For unit testcases which are part of the nb-city-safety when called from outside (`%run nb-city-safety { \"execution_mode\": \"testing\",....}`) test fixtures like `caplog` will differ. So, either move the testcases into to this notebook or run the nb-city-safety notepbook all by itself when testing for these.\n",
    "- The called notebook built in references are used by the execution (even if the notebook that was callled has its own builtin resources). See [run a notebook](https://learn.microsoft.com/fabric/data-engineering/author-execute-notebook#spark-session-configuration-magic-command) for details about `%run`.\n",
    "- Only function definitions are loaded by using `%run` as we have code in place to skip the execution portion. The actual execution will be done in this notebook as of part of our testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run nb-city-safety { \"execution_mode\": \"module\", \"job_exec_instance\": \"110_city_safety#20240808124161\", \"common_execution_mode\": \"normal\", \"env_stage\": \"dev\", \"config_file_path\": \"/synfs/nb_resource/builtin/city_safety.cfg\", \"param_override\": \"True\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create unit tests \n",
    "\n",
    "- Code-based unit tests are added in [tests/test_nb-city-safety-common.ipynb](../../tests/test_nb-city-safety-common.ipynb). This example demonstrates unit tests in another notebook scenario.\n",
    "- Data-based unit tests are shown below. This example demonstrates unit tests in the same notebook scnario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_utc_timestamp\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"cleanup_mode\", [(True), (False)])\n",
    "def test_city_safety_main(cleanup_mode):\n",
    "    # Validate data counts in target table based on the execution ids\n",
    "    # start with no table first - use custom target table\n",
    "    # append in second run\n",
    "\n",
    "    # These are outside of main function in the script - so we are resetting them so that they accept our values\n",
    "    #   Good practice is to make these as function parameters (unlike what we are doing here).\n",
    "    global cleanup_flag, current_ts, job_exec_instance\n",
    "\n",
    "    cleanup_flag = cleanup_mode  # Start with Overwrite mode and then test append mode\n",
    "    current_ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S%f\")\n",
    "    job_exec_instance = f\"110_city_safety#{current_ts}\"  # make runs unique\n",
    "\n",
    "    source_table_path = \"abfss://citydatacontainer@azureopendatastoragedev.dfs.core.windows.net/Safety/Release/city=Boston\"\n",
    "    exp_df = spark.read.format(\"parquet\").load(source_table_path)\n",
    "    exp_job_exec_instance = job_exec_instance\n",
    "    exp_user = mssparkutils.env.getUserName()\n",
    "    target_table_path = \"abfss://a046e3e0-d007-4d72-9c9f-53da44ba8c58@onelake.dfs.fabric.microsoft.com/a8df3bac-4b1c-4c29-a76c-93840de9831a/Tables/tbl_city_safety_data_test\"\n",
    "    columns = [\n",
    "        \"address\",\n",
    "        \"category\",\n",
    "        \"dataSubtype\",\n",
    "        \"dataType\",\n",
    "        \"dateTime\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"source\",\n",
    "        \"status\",\n",
    "        \"subcategory\",\n",
    "        \"extendedProperties\",\n",
    "    ]\n",
    "\n",
    "    main()  # <--- code execution with no code patching\n",
    "\n",
    "    act_df = spark.read.format(\"delta\").load(target_table_path)\n",
    "    act_df = act_df.filter(act_df.jobExecId == exp_job_exec_instance)\n",
    "\n",
    "    # ----------- Validate unchanged columns ------------------------------\n",
    "    exp_df = exp_df.select(columns)\n",
    "    act_subset_df = act_df.select(columns)\n",
    "\n",
    "    # if spark_major_version >= 3.5:\n",
    "    #   assertDataFrameEqual(act_df, exp_df)\n",
    "    assert exp_df.count() == act_df.count()\n",
    "    assert exp_df.schema == act_subset_df.schema\n",
    "    assert exp_df.exceptAll(act_subset_df).count() == 0\n",
    "\n",
    "    # -------- Additional columns validation ------------------------------\n",
    "    # 'lastUpdateUTC'  is hard to validate as this evaluated during runtime.\n",
    "    #    Code based unit test is the best for validating this one.\n",
    "    #    Another option is store the runtime value from the code somewhere and\n",
    "    #    then reading it from there.\n",
    "    exp_additional_col_values = {\n",
    "        \"City\": \"Boston\",\n",
    "        \"jobExecId\": exp_job_exec_instance,\n",
    "        \"lastUpdateUser\": exp_user,\n",
    "    }\n",
    "    act_additional_col_values = (\n",
    "        act_df.select(\"City\", \"jobExecId\", \"lastUpdateUser\")\n",
    "        .distinct()\n",
    "        .collect()[0]\n",
    "        .asDict()\n",
    "    )\n",
    "\n",
    "    assert exp_additional_col_values == act_additional_col_values\n",
    "\n",
    "    # ----------- Validate transformed column ------------------------------\n",
    "    # - Can be done here or in the code based unit tests\n",
    "    #     as there are no run time values involved similar to 'lastUpdateUTC'\n",
    "    exp_date_df = exp_df.select(\"address\", \"dateTime\")\n",
    "    # using a different transformation instead of using the same logic in the main code\n",
    "    act_date_df = (\n",
    "        act_df.select(\"address\", \"dateTimeUTC\")\n",
    "        .withColumn(\n",
    "            \"dateTime\", from_utc_timestamp(act_df.dateTimeUTC, \"America/New_York\")\n",
    "        )\n",
    "        .select(\"address\", \"dateTime\")\n",
    "    )\n",
    "    assert exp_date_df.exceptAll(act_date_df).count() == 0\n",
    "\n",
    "    # ------------- OTEL SDK for monitor- validate App Insights as the target -----------\n",
    "    # This is part of the OTEL SDK sample as well as OTEL Collector implementation\n",
    "    # Ref: https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/monitor/azure-monitor-query/samples/sample_single_log_query_without_pandas.py\n",
    "\n",
    "    # ****NOTE: Queries use Log Anlaytics table names (not appInsights)\n",
    "\n",
    "    from azure.identity import ClientSecretCredential\n",
    "    from azure.core.exceptions import HttpResponseError\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "    from azure.monitor.query import LogsQueryClient, LogsQueryStatus\n",
    "    from datetime import timedelta\n",
    "\n",
    "    credential = ClientSecretCredential(\n",
    "        tenant_id=\"ab6f7679-cdcd-4084-b327-54012e772f32\",\n",
    "        client_id=\"c6f27d64-fdb4-4fac-b269-13dfd003e609\",\n",
    "        client_secret=\"TODO:<<read_from_key_vault>>\",  # app-fabric-sguda-clientsecret\n",
    "        resource=\"api.loganalytics.azure.com\",\n",
    "    )\n",
    "\n",
    "    client = LogsQueryClient(credential)\n",
    "    log_workspace_id = \"80b89bbf-25de-4af9-99ea-07d424b1a21a\"\n",
    "\n",
    "    logs_query = f\"AppTraces | where TimeGenerated > todatetime('{current_ts}') | where Message == 'City safety processing is complete.'|count\"\n",
    "    traces_query = f\"AppDependencies|where TimeGenerated > todatetime('{current_ts}') |where Target == 'root#nb-safety#{current_ts}'|count\"\n",
    "    exceptions_query = f\"AppExceptions | where TimeGenerated > todatetime('{current_ts}') |where OuterType == 'Exception' | where OuterMessage == 'ETL step failed with error Dummy failure on metrics gathering.'|count\"\n",
    "    metrics_query = (\n",
    "        f\"AppMetrics| where TimeGenerated > todatetime('{current_ts}')|count\"\n",
    "    )\n",
    "\n",
    "    for query in (logs_query, traces_query, exceptions_query, metrics_query):\n",
    "        assert query_app_insights(log_workspace_id, query, 3000) >= 1\n",
    "\n",
    "    # ------------- OTEL using - Collectro validate Opentelemetry data in Fabric -------------------------------\n",
    "    if otel_setup_type == \"collector\":\n",
    "\n",
    "        # TO DO - Expand the example to include detailed validation of all the attributes (resource, trace etc.,)\n",
    "        traces_query = f\"OTELTraces | where SpanName == 'root#nb-safety#{current_ts}'\"\n",
    "        kusto_uri = \"https://trd-25w48grmsgkdja2nn4.z7.kusto.fabric.microsoft.com\"\n",
    "        kusto_token = mssparkutils.credentials.getToken(kusto_uri)\n",
    "        database = \"oteldb\"\n",
    "        traces_df = (\n",
    "            spark.read.format(\"com.microsoft.kusto.spark.synapse.datasource\")\n",
    "            .option(\"accessToken\", kusto_token)\n",
    "            .option(\"kustoCluster\", kusto_uri)\n",
    "            .option(\"kustoDatabase\", database)\n",
    "            .option(\"kustoQuery\", traces_query)\n",
    "            .load()\n",
    "        )\n",
    "\n",
    "        assert traces_df.count() == 1\n",
    "        assert traces_df.select(\"SpanStatus\").collect()[0][0] == \"STATUS_CODE_OK\"\n",
    "\n",
    "        logs_query = f\"\"\"OTELLogs\n",
    "            | where TraceID in (\n",
    "                OTELTraces\n",
    "                | where SpanName == \"root#nb-safety#{current_ts}\"\n",
    "                | project TraceID\n",
    "            )\"\"\"\n",
    "        logs_df = (\n",
    "            spark.read.format(\"com.microsoft.kusto.spark.synapse.datasource\")\n",
    "            .option(\"accessToken\", kusto_token)\n",
    "            .option(\"kustoCluster\", kusto_uri)\n",
    "            .option(\"kustoDatabase\", database)\n",
    "            .option(\"kustoQuery\", logs_query)\n",
    "            .load()\n",
    "        )\n",
    "\n",
    "        assert logs_df.count() >= 9\n",
    "        assert logs_df.select(\"ResourceAttributes\").distinct().collect()[\n",
    "            0\n",
    "        ].asDict() == {\"ResourceAttributes\": '{\"service.name\":\"otel-poc-vm-based\"}'}\n",
    "\n",
    "        metrics_query = \"\"\"OTELMetrics\n",
    "            | where MetricType == \"Sum\"\n",
    "            | where MetricName == \"city-level-metrics\"\n",
    "            | where MetricAttributes[\"record_count_total\"] == 267\n",
    "            \"\"\"\n",
    "        metrics_df = (\n",
    "            spark.read.format(\"com.microsoft.kusto.spark.synapse.datasource\")\n",
    "            .option(\"accessToken\", kusto_token)\n",
    "            .option(\"kustoCluster\", kusto_uri)\n",
    "            .option(\"kustoDatabase\", database)\n",
    "            .option(\"kustoQuery\", metrics_query)\n",
    "            .load()\n",
    "        )\n",
    "\n",
    "        assert metrics_df.count() >= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the unit tests and capture the results\n",
    "\n",
    "- As `ipytest.autoconfig(raise_on_error=True)` was used in the begining of this notebook, any errors from the testcases will not result in notebook failure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture data_unit_tests_results\n",
    "ipytest.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the test rsults\n",
    "\n",
    "- These can be stored somewhere or sent for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_unit_test_results(data_unit_tests_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
