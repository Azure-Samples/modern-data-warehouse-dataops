{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 1. Get dynamic pipeline parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Get pipeline name\n",
        "pipelinename = 'pipeline_name'\n",
        "\n",
        "# Get pipeline run id\n",
        "loadid = ''\n",
        "\n",
        "# Get keyvault linked service name\n",
        "keyvaultlsname = 'Ls_KeyVault_01'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 2. Transform and load Dimension tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "from pyspark.sql.functions import col, lit\n",
        "import ddo_transform.transform as t\n",
        "import ddo_transform.util as util\n",
        "\n",
        "load_id = loadid\n",
        "loaded_on = datetime.datetime.now()\n",
        "\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "\n",
        "# Primary storage info \n",
        "account_name = token_library.getSecretWithLS(keyvaultlsname,\"datalakeaccountname\")\n",
        "container_name = 'datalake' # fill in your container name \n",
        "relative_path = 'data/dw/' # fill in your relative folder path \n",
        "\n",
        "base_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
        "\n",
        "# Read interim cleansed data\n",
        "parkingbay_sdf = spark.read.table(\"interim.parking_bay\").filter(col('load_id') == lit(load_id))\n",
        "sensordata_sdf = spark.read.table(\"interim.sensor\").filter(col('load_id') == lit(load_id))\n",
        "\n",
        "# Read existing Dimensions\n",
        "dim_parkingbay_sdf = spark.read.table(\"dw.dim_parking_bay\")\n",
        "dim_location_sdf = spark.read.table(\"dw.dim_location\")\n",
        "dim_st_marker = spark.read.table(\"dw.dim_st_marker\")\n",
        "\n",
        "# Transform\n",
        "new_dim_parkingbay_sdf = t.process_dim_parking_bay(parkingbay_sdf, dim_parkingbay_sdf, load_id, loaded_on).cache()\n",
        "new_dim_location_sdf = t.process_dim_location(sensordata_sdf, dim_location_sdf, load_id, loaded_on).cache()\n",
        "new_dim_st_marker_sdf = t.process_dim_st_marker(sensordata_sdf, dim_st_marker, load_id, loaded_on).cache()\n",
        "\n",
        "# Load\n",
        "util.save_overwrite_unmanaged_table(spark, new_dim_parkingbay_sdf, table_name=\"dw.dim_parking_bay\", path=os.path.join(base_path, \"dim_parking_bay\"))\n",
        "util.save_overwrite_unmanaged_table(spark, new_dim_location_sdf, table_name=\"dw.dim_location\", path=os.path.join(base_path, \"dim_location\"))\n",
        "util.save_overwrite_unmanaged_table(spark, new_dim_st_marker_sdf, table_name=\"dw.dim_st_marker\", path=os.path.join(base_path, \"dim_st_marker\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 3. Transform and load Fact tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Read existing Dimensions\n",
        "dim_parkingbay_sdf = spark.read.table(\"dw.dim_parking_bay\")\n",
        "dim_location_sdf = spark.read.table(\"dw.dim_location\")\n",
        "dim_st_marker = spark.read.table(\"dw.dim_st_marker\")\n",
        "\n",
        "# Process\n",
        "new_fact_parking = t.process_fact_parking(sensordata_sdf, dim_parkingbay_sdf, dim_location_sdf, dim_st_marker, load_id, loaded_on)\n",
        "\n",
        "# Insert new rows\n",
        "new_fact_parking.write.mode(\"append\").insertInto(\"dw.fact_parking\")\n",
        "\n",
        "# Recording record counts for logging purpose\n",
        "new_dim_parkingbay_count = spark.read.table(\"dw.dim_parking_bay\").count()\n",
        "new_dim_location_count = spark.read.table(\"dw.dim_location\").count()\n",
        "new_dim_st_marker_count = spark.read.table(\"dw.dim_st_marker\").count()\n",
        "new_fact_parking_count = new_fact_parking.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 4. Observability: Logging to Azure Application Insights using OpenCensus Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.log_exporter import AzureEventHandler\n",
        "from datetime import datetime\n",
        "\n",
        "# Getting Application Insights instrumentation key\n",
        "appi_key = token_library.getSecretWithLS(keyvaultlsname,\"applicationInsightsKey\")\n",
        "\n",
        "# Enable App Insights\n",
        "aiLogger = logging.getLogger(__name__)\n",
        "aiLogger.addHandler(AzureEventHandler(connection_string = 'InstrumentationKey=' + appi_key))\n",
        "\n",
        "aiLogger.setLevel(logging.INFO)\n",
        "\n",
        "aiLogger.info(\"Transform (ai): Started at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "properties = {\"custom_dimensions\": {\"pipeline\": pipelinename, \"run_id\": loadid, \"new_parkingbay_count\": new_dim_parkingbay_count}}\n",
        "aiLogger.info(\"Transform (ai): Completed at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), extra=properties)\n",
        "\n",
        "# To query this log, go to the Azure Monitor and run the following kusto query (Scope: Application Insights instance):\n",
        "#customEvents\n",
        "#| order by timestamp desc\n",
        "#| project timestamp, appName, name,\n",
        "#    pipelineName             = customDimensions.pipeline,\n",
        "#    pipelineRunId            = customDimensions.run_id,\n",
        "#    parkingbayCount          = customDimensions.parkingbay_count,\n",
        "#    sensordataCount          = customDimensions.sensordata_count,\n",
        "#    parkingbayMalformedCount = customDimensions.parkingbay_malformed_count,\n",
        "#    sensordataMalformedCount = customDimensions.sensordata_malformed_count,\n",
        "#    dimParkingbayCount       = customDimensions.new_parkingbay_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 5. Observability: Logging to Log Analytics workspace using log4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "# Enable Log Analytics using log4j\n",
        "log4jLogger = sc._jvm.org.apache.log4j\n",
        "logger = log4jLogger.LogManager.getLogger(\"ParkingSensorLogs\")\n",
        "\n",
        "def log(msg = ''):\n",
        "    env = mssparkutils.env\n",
        "    formatted_msg = f'Transform (log4j): {msg}~{pipelinename}~{env.getJobId()}~{env.getPoolName()}~{env.getWorkspaceName()}~{env.getUserId()}'\n",
        "    logger.info(formatted_msg)\n",
        "\n",
        "log(\"Started at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "log(f'new_dim_parkingbay_count: {new_dim_parkingbay_count}')\n",
        "log(f'new_dim_location_count: {new_dim_location_count}')\n",
        "log(f'new_dim_st_marker_count: {new_dim_st_marker_count}')\n",
        "log(f'new_fact_parking_count: {new_fact_parking_count}')\n",
        "\n",
        "log(\"Completed at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "# To query this log, go to the log analytics workspace and run the following kusto query (Scope: Log Analytics Workspace):\n",
        "#SparkLoggingEvent_CL\n",
        "#| where logger_name_s == \"ParkingSensorLogs\"\n",
        "#| order by TimeGenerated desc\n",
        "#| project TimeGenerated, workspaceName_s, Level,\n",
        "#    message         = split(Message, '~', 0),\n",
        "#    pipelineName    = split(Message, '~', 1),\n",
        "#    jobId           = split(Message, '~', 2),\n",
        "#    SparkPoolName   = split(Message, '~', 3),\n",
        "#    UserId          = split(Message, '~', 5)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
