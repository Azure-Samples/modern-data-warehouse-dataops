{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# Get pipeline run id\n",
        "pipeline_run_id = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "# Get keyvault linked service name\n",
        "keyvaultlsname = 'Ls_KeyVault_01'\n",
        "\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "\n",
        "# Primary storage info \n",
        "storage_account_url = token_library.getSecretWithLS(keyvaultlsname, \"datalakeurl\").replace(\"https://\", '')\n",
        "container_name = 'saveddata' # fill in your container name \n",
        "relative_path = '' # fill in your relative folder path \n",
        "file_name = f\"{pipeline_run_id}.parquet\"\n",
        "adls_path = 'abfss://%s@%s/%s' % (container_name, storage_account_url, relative_path) \n",
        "print('Primary storage account path: ' + adls_path) \n",
        "loaded_on = datetime.datetime.now()\n",
        "base_path = os.path.join(adls_path, file_name)\n",
        "print('Base path is: ' + base_path)\n",
        "df = spark.read.parquet(base_path)\n",
        "\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "count = df.count()\n",
        "data = [[loaded_on, file_name, pipeline_run_id, df.count()]]\n",
        " \n",
        "# specify column names\n",
        "columns = ['date', 'filename', 'pipeline_run_id', 'count']\n",
        " \n",
        "# creating a dataframe from the lists of data\n",
        "dataframe = spark.createDataFrame(data, columns)\n",
        "\n",
        "dataframe.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "dataframe.write.mode('append').parquet(os.path.join(adls_path, f'interim.count/{pipeline_run_id}'))"
      ]
    }
  ]
}