{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da256cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary modules at the top\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "# Add the parent directory to the path to import utils\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Fabric-specific imports\n",
    "try:\n",
    "    from notebookutils import mssparkutils\n",
    "except ImportError:\n",
    "    # Fallback for environments where notebookutils is not available\n",
    "    mssparkutils = None\n",
    "\n",
    "# Import utility classes from our utils package\n",
    "from utils import DataAsset, DataLineage, MockFeatureStore, PurviewDataCatalog, fetch_logged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb334c-7832-488e-8d08-bd5f66b0e1e7",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Retrieve the feature set from Azure ML managed feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f6d26-911c-4a6e-b78c-35e553e33dd7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%run feature_set_retrieval\n",
    "\n",
    "# Load the NYC taxi dataset from previous feature retrieval\n",
    "# This should be loaded from feature set retrieval results\n",
    "df = spark.sql(\"SELECT * FROM nyctaxi_featureset LIMIT 1000\")  # placeholder\n",
    "nyctaxi_df = df.toPandas()\n",
    "nyctaxi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67543523-4385-4026-a000-cbca6cb3ce30",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load the NYC weather dataset from previous feature retrieval\n",
    "# This should be loaded from feature set retrieval results\n",
    "nycweather_df_spark = spark.sql(\"SELECT * FROM nycweather_featureset LIMIT 1000\")  # placeholder\n",
    "nycweather_df = nycweather_df_spark.toPandas()  # type: ignore\n",
    "nycweather_df = nycweather_df[nycweather_df[\"year\"] == 2022].drop(columns=[\"year\"])\n",
    "nycweather_df.head()\n",
    "\n",
    "nyctaxi_df = df.toPandas()\n",
    "nyctaxi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d3066-0566-46ac-8f44-89a80f208f3c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Assuming nycweather_df is already defined and contains the relevant data\n",
    "\n",
    "# Scaling NYC weather columns with minmax scaler\n",
    "nycweather_scaler = MinMaxScaler()\n",
    "nycweather_scaled_df = pd.DataFrame(nycweather_scaler.fit_transform(nycweather_df), columns=nycweather_df.columns)\n",
    "nycweather_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21d5bc-3081-438b-bc33-020bd0243c00",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Scaling NYC weather columns with minmax scaler\n",
    "nycweather_scaler = MinMaxScaler().fit(\n",
    "    nycweather_df[[\"temperature_2m_c\", \"windspeed_10m_km_per_hour\", \"precipitation_mm\", \"cloudcover_percentage\"]]\n",
    ")\n",
    "\n",
    "nycweather_df[[\"scaled_temperature\", \"scaled_windspeed\", \"scaled_precipitation\", \"scaled_cloudcover\"]] = (\n",
    "    nycweather_scaler.transform(\n",
    "        nycweather_df[[\"temperature_2m_c\", \"windspeed_10m_km_per_hour\", \"precipitation_mm\", \"cloudcover_percentage\"]]\n",
    "    )\n",
    ")\n",
    "\n",
    "nycweather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b291b03-8166-4d61-9350-1a5b6054fd39",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Join NYC taxi and weather data\n",
    "nyctaxi_with_weather_df = nyctaxi_df.join(\n",
    "    nycweather_df.set_index([\"month\", \"day\", \"hour\"]), on=[\"month_pickup\", \"day_pickup\", \"hour_pickup\"], how=\"inner\"\n",
    ")\n",
    "nyctaxi_with_weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e453a94-ed81-4cc9-a9a2-a6ddf851cc39",
   "metadata": {},
   "source": [
    "# Track Machine Learning experiments and models\n",
    "\n",
    "A machine learning model is a file that has been trained to recognize certain types of patterns. You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from those data. Once you have trained the model, you can use it to reason over data that it hasn't seen before, and make predictions about that data.\n",
    "\n",
    "In this notebook, you will learn the basic steps to run an experiment, add a model version to track run metrics and parameters and register a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bedd7f-6ab2-4f8c-b923-62735fe6a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set given experiment as the active experiment.\n",
    "# If an experiment with this name does not exist, a new experiment with this name is created.\n",
    "ml_experiment_name = \"training-experiment\"\n",
    "mlflow.set_experiment(ml_experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef3b18d-e2ad-4a76-875c-a58d52120592",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Combine the datasets for model training\n",
    "# This assumes we have joined datasets from feature engineering\n",
    "nyctaxi_with_weather_df = pd.concat([nyctaxi_df, nycweather_scaled_df], axis=1)\n",
    "\n",
    "final_df = nyctaxi_with_weather_df.drop(columns=[\"pickup_datetime\", \"dropoff_datetime\"])  # Remove datetime columns\n",
    "\n",
    "# Split features and target\n",
    "X = final_df.drop(columns=[\"demand\"])  # Features\n",
    "y = final_df[\"demand\"]  # Target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"importance_type\": \"split\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 3,\n",
    "    \"n_estimators\": 100,\n",
    "    \"n_jobs\": -1,\n",
    "    \"num_leaves\": 31,\n",
    "    \"objective\": \"poisson\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e7a86-2f79-4b7b-8243-c97aa4329276",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Define infrastructure variables - these should be set from environment or previous cells\n",
    "tenant_id = spark.conf.get(\"spark.fsd.tenant_id\", \"\")\n",
    "featurestore_subscription_id = spark.conf.get(\"spark.fsd.subscription_id\", \"\")\n",
    "featurestore_resource_group_name = spark.conf.get(\"spark.fsd.rg_name\", \"\")\n",
    "featurestore_name = spark.conf.get(\"spark.fsd.name\", \"\")\n",
    "fabric_tenant = spark.conf.get(\"spark.fsd.fabric.tenant\", \"\")\n",
    "\n",
    "# Feature set names\n",
    "nyctaxi_featureset_name = \"nyctaxi\"\n",
    "nycweather_featureset_name = \"nycweather\"\n",
    "\n",
    "# Initialize feature store client and data catalog\n",
    "featurestore = MockFeatureStore()\n",
    "\n",
    "# Mock all_featuresets dict\n",
    "all_featuresets = {nyctaxi_featureset_name: \"1\", nycweather_featureset_name: \"1\"}\n",
    "\n",
    "purview_data_catalog = PurviewDataCatalog()\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    model = LGBMRegressor(**params).fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate error metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Assemble the metrics we're going to write into a collection\n",
    "    metrics = {\"mae\": mae, \"mse\": mse, \"rmse\": rmse, \"r2\": r2}\n",
    "\n",
    "    signature = infer_signature(X_test, y_test)\n",
    "\n",
    "    # Log the parameters used for the model fit\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Log the error metrics that were calculated during validation\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # Activate the MLFlow logging API to log your model artifacts\n",
    "    mlflow.sklearn.log_model(model, \"demand_prediction_model\", signature=signature)\n",
    "    print(f\"Model saved in run_id={run.info.run_id} with metrics {metrics}\")\n",
    "\n",
    "    # Register the model produced from your training job.\n",
    "    mv = mlflow.register_model(f\"runs:/{run.info.run_id}/demand_prediction_model\", \"demand_prediction_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9eec0c-c6b9-4f72-bf00-ef22bdec3157",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "run_id = mlflow.last_active_run().info.run_id\n",
    "print(f\"Logged data and model in run: {run_id}\")\n",
    "\n",
    "# show logged data\n",
    "for key, data in fetch_logged_data(run_id).items():\n",
    "    print(f\"\\n---------- logged {key} ----------\")\n",
    "    pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff01f0c3-5caf-4223-8a0f-2c4a4b6b977d",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Register model training lineage to Purview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfab779-72a6-4536-8492-cc17be1f6c27",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "purview_data_catalog = PurviewDataCatalog()\n",
    "\n",
    "# Create features assets\n",
    "nyctaxi_fset = featurestore.feature_sets.get(nyctaxi_featureset_name, all_featuresets[nyctaxi_featureset_name])\n",
    "nycweather_fset = featurestore.feature_sets.get(nycweather_featureset_name, all_featuresets[nycweather_featureset_name])\n",
    "used_features = nyctaxi_with_weather_df.drop(columns=[\"demand\"]).columns\n",
    "infra_info = {\n",
    "    \"tenant_id\": tenant_id,\n",
    "    \"subscription_id\": featurestore_subscription_id,\n",
    "    \"resource_group\": featurestore_resource_group_name,\n",
    "}\n",
    "\n",
    "nyctaxi_feature_assets = purview_data_catalog.prepare_feature_assets(\n",
    "    featurestore_name, nyctaxi_fset, used_features, **infra_info\n",
    ")\n",
    "\n",
    "nycweather_feature_assets = purview_data_catalog.prepare_feature_assets(\n",
    "    featurestore_name, nycweather_fset, used_features, **infra_info\n",
    ")\n",
    "\n",
    "# Prepare AML custom types\n",
    "purview_data_catalog.prepare_aml_custom_types()\n",
    "\n",
    "# Create model training notebook process asset\n",
    "current_notebook_context = mssparkutils.notebook.nb.context\n",
    "workspace_id = current_notebook_context[\"currentWorkspaceId\"]\n",
    "notebook_id = current_notebook_context[\"currentNotebookId\"]\n",
    "notebook_name = \"model_training\"\n",
    "process_data_asset = DataAsset(\n",
    "    f\"{notebook_name} (Fabric notebook)\",\n",
    "    \"process\",\n",
    "    f\"https://{fabric_tenant}.powerbi.com/groups/{workspace_id}/synapsenotebooks/{notebook_id}\",\n",
    ")\n",
    "\n",
    "# Create Azure ML experiment asset\n",
    "ml_experiment_run = mlflow.get_run(mv.run_id).to_dictionary()\n",
    "ml_artifact_uri = ml_experiment_run[\"info\"][\"artifact_uri\"]\n",
    "ml_experiment_id = ml_artifact_uri.split(f\"{workspace_id}/\")[1].split(\"/\")[0]\n",
    "ml_experiment_fqn = (\n",
    "    f\"https://msit.powerbi.com/groups/{workspace_id}/mlexperiments/{ml_experiment_id}?experience=data-science\"\n",
    ")\n",
    "\n",
    "ml_experiment_asset = DataAsset(ml_experiment_name, \"ml_experiment\", ml_experiment_fqn)\n",
    "\n",
    "# Register lineage like features -> model training notebook -> ML experiment\n",
    "training_model_lineage = DataLineage(\n",
    "    input_data_assets=nyctaxi_feature_assets + nycweather_feature_assets,\n",
    "    output_data_assets=[ml_experiment_asset],\n",
    "    process_asset=process_data_asset,\n",
    ")\n",
    "purview_data_catalog.register_lineage(training_model_lineage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543fff7-8ec3-4f50-9356-3a1f6e77fd3c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create Azure ML model asset\n",
    "ml_model_id = mv.source.split(\".dfs.core.windows.net/\")[1].split(\"/\")[0]\n",
    "ml_model_name = mv.name\n",
    "ml_experiment_run = mlflow.get_run(mv.run_id).to_dictionary()\n",
    "ml_experiment_run_name = ml_experiment_run[\"data\"][\"tags\"][\"mlflow.runName\"]\n",
    "\n",
    "ml_model_fqn = f\"https://msit.powerbi.com/groups/{workspace_id}/mlmodels/{ml_model_id}?experience=data-science\"\n",
    "ml_model_asset = DataAsset(\n",
    "    ml_model_name,\n",
    "    \"ml_model\",\n",
    "    ml_model_fqn,\n",
    "    custom_properties={\"version\": mv.version, \"experimentRunName\": ml_experiment_run_name},\n",
    "    relationship_attributes=[{\"type\": \"sources\", \"qualified_name\": ml_experiment_fqn}],\n",
    ")\n",
    "\n",
    "# Register ML model entity connecting to ML experiment, without process node\n",
    "purview_data_catalog.register_entity(ml_model_asset)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "host": {},
   "language": "python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "orig_nbformat": 4,
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {},
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
