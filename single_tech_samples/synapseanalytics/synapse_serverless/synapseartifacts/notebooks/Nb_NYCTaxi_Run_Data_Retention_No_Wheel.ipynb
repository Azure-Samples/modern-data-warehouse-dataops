{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Parameters passed from Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "dynamicyear = \"\"\n",
        "dynamicmonth = \"\"\n",
        "stgAccountName = \"\"\n",
        "retentionDate = \"\"\n",
        "retentionTimeInYears = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Delete data no longer needed on the retention policy in the Data Lake (2 years in our example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from delta.tables import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "adls2_account_name = stgAccountName\n",
        "adls2_container_name = \"datalake\"\n",
        "adls2_folderyear = dynamicyear\n",
        "adls2_foldermonth = dynamicmonth\n",
        "\n",
        "spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")  \n",
        "delta_table_path = 'abfss://{0}@{1}.dfs.core.windows.net/{2}/{3}/'.format(adls2_container_name, adls2_account_name, adls2_folderyear, adls2_foldermonth)\n",
        "\n",
        "# Create Delta Table Object\n",
        "deltaTableLake = DeltaTable.forPath(spark, delta_table_path )\n",
        "\n",
        "# Delete data according to the company police (2 years on our example)\n",
        "deleteStatement = '{0}'.format(retentionDate)\n",
        "deltaTableLake.delete(\"tpep_pickup_datetime < '{0}'\".format(deleteStatement)) \n",
        "# Vacuum files that will no longer be referenced\n",
        "spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\") \n",
        "# zero refers to no retention time for this files, so it means they will be deleted immediately\n",
        "deltaTableLake.vacuum(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Update configuration file (deleting data no covered by retention, so is not re-ingested)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Install azure storage library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pip install azure-storage-file-datalake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Define functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-01-27T12:16:53.9996695Z",
              "execution_start_time": "2023-01-27T12:16:52.1186875Z",
              "livy_statement_state": "available",
              "queued_time": "2023-01-27T12:16:51.952506Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sysparkpooly5",
              "state": "finished",
              "statement_id": 4
            },
            "text/plain": [
              "StatementMeta(sysparkpooly5, 39, 4, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from azure.storage.filedatalake import DataLakeServiceClient\n",
        "\n",
        "## Functions to read data from ADLS and update ACLS\n",
        "\n",
        "# https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-directory-file-acl-python\n",
        "def initialize_storage_account(storage_account_name, storage_account_key):\n",
        "    \n",
        "    try:  \n",
        "        service_client = DataLakeServiceClient(account_url=f\"https://{storage_account_name}.dfs.core.windows.net\", credential=storage_account_key)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    return service_client\n",
        "\n",
        "\n",
        "def download_file_from_directory(service_client, container, directory, file_name):\n",
        "    try:\n",
        "        file_system_client = service_client.get_file_system_client(file_system=container)\n",
        "\n",
        "        directory_client = file_system_client.get_directory_client(directory)\n",
        "     \n",
        "        file_client = directory_client.get_file_client(file_name)\n",
        "\n",
        "        download = file_client.download_file()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "    else:\n",
        "        configuration = json.loads(download.readall())\n",
        "        return configuration\n",
        "\n",
        "def update_datalake_with_retention(config):\n",
        "    configDeleteFinal={}\n",
        "    configDeleteFinal[\"datalakeProperties\"] = '{\"datalakeProperties\":[]}'\n",
        "    configDelete=[]\n",
        "\n",
        "    for p_info in config[\"datalakeProperties\"]:\n",
        "        if (int(retention_year) > int(p_info[\"year\"])) and (int(current_month) < int(p_info[\"month\"])):\n",
        "            # if the conditions met, this items are out of the new array\n",
        "            print(\"Skip entry\")\n",
        "        else:\n",
        "            configDelete.append(p_info)\n",
        "    configDeleteFinal[\"datalakeProperties\"]=configDelete    \n",
        "    return configDeleteFinal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Define constants, variables and fetch secret values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-01-27T12:17:02.4539762Z",
              "execution_start_time": "2023-01-27T12:16:59.6169719Z",
              "livy_statement_state": "available",
              "queued_time": "2023-01-27T12:16:59.4528712Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sysparkpooly5",
              "state": "finished",
              "statement_id": 5
            },
            "text/plain": [
              "StatementMeta(sysparkpooly5, 39, 5, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Constants\n",
        "keyvault_ls_name = \"Ls_NYCTaxi_KeyVault\"\n",
        "storage_key_name = \"datalakeKey\"\n",
        "data_container = \"datalake\"\n",
        "config_container = \"config\"\n",
        "config_file_name = \"datalake_config.json\"\n",
        "config_file_path = \"/\"\n",
        "\n",
        "# Variables\n",
        "current_year = datetime.utcnow().strftime(\"%Y\")\n",
        "current_month = int(datetime.utcnow().strftime(\"%m\"))\n",
        "retention_year = int(current_year)-int(retentionTimeInYears)\n",
        "current_ts = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "\n",
        "# Secrets based values\n",
        "storage_access_key = mssparkutils.credentials.getSecretWithLS(keyvault_ls_name, storage_key_name)\n",
        "storage_acct_connection = f\"DefaultEndpointsProtocol=https;AccountName={stgAccountName};AccountKey={storage_access_key};EndpointSuffix=core.windows.net\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get configuration file, and delete entries that are no longer in the retention period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from notebookutils import mssparkutils\n",
        "\n",
        "service_client = initialize_storage_account(stgAccountName, storage_access_key)\n",
        "config = download_file_from_directory(service_client, config_container, config_file_path, config_file_name)\n",
        "configDeleteFinal=update_datalake_with_retention(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# mssparkutils.fs.help()\n",
        "source_config = f\"abfss://{config_container}@{stgAccountName}.dfs.core.windows.net{config_file_path}{config_file_name}\"\n",
        "backup_config = f\"abfss://{config_container}@{stgAccountName}.dfs.core.windows.net{config_file_path}{config_file_name}_{current_ts}\"\n",
        "mssparkutils.fs.mv(source_config, backup_config, overwrite=True)\n",
        "jsonResult=json.dumps(configDeleteFinal,indent=2, default=str)   \n",
        "mssparkutils.fs.put(source_config, jsonResult, overwrite=True)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "13a507f1b7e58130c01c2759096d339f2133b6331e760f59296e0922521e11c8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
