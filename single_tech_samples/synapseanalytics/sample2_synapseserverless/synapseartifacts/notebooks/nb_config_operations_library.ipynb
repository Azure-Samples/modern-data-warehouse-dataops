{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Install Required Python Modules\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This module installation can be moved to package installation using \"requirements.txt\"*\r\n",
        "\r\n",
        "*`az synapse spark pool update` with --library-requirements requirements.txt*\r\n",
        "\r\n",
        "*Concern: Long time(~ 20 mins) for the statement to return status*"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install azure-storage-file-datalake"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from azure.storage.filedatalake import DataLakeServiceClient\n",
        "\n",
        "## Functions to read data from ADLS and update ACLS\n",
        "\n",
        "# https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-directory-file-acl-python\n",
        "def initialize_storage_account(storage_account_name, storage_account_key):\n",
        "    \n",
        "    try:  \n",
        "        service_client = DataLakeServiceClient(account_url=f\"https://{storage_acct}.dfs.core.windows.net\", credential=storage_account_key)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    return service_client\n",
        "\n",
        "\n",
        "def download_file_from_directory(service_client, container, directory, file_name):\n",
        "    try:\n",
        "        file_system_client = service_client.get_file_system_client(file_system=container)\n",
        "\n",
        "        directory_client = file_system_client.get_directory_client(directory)\n",
        "     \n",
        "        file_client = directory_client.get_file_client(file_name)\n",
        "\n",
        "        download = file_client.download_file()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "    else:\n",
        "        configuration = json.loads(download.readall())\n",
        "        return configuration\n",
        "\n",
        "\n",
        "# https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-acl-python\n",
        "def update_permission_recursively(service_client, container_name, directory_path, is_default_scope, user_type, user_id, permissions):\n",
        "    \n",
        "    try:\n",
        "        file_system_client = service_client.get_file_system_client(file_system=container_name)\n",
        "\n",
        "        directory_client = file_system_client.get_directory_client(directory_path)\n",
        "              \n",
        "        acl = f\"{user_type}:{user_id}:{permissions}\"\n",
        "\n",
        "        if is_default_scope:\n",
        "           acl = f'default:{user_type}:{user_id}:{permissions}'\n",
        "\n",
        "        directory_client.update_access_control_recursive(acl=acl)\n",
        "\n",
        "        acl_props = directory_client.get_access_control()\n",
        "        \n",
        "        print(f\"Permissions for {directory_path} - {user_type}:{user_id} are:\\n{acl_props['acl']}\") \n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "# Assumption: Config contains all the perms needed for a given location. Incremental changes are not allowed.\n",
        "# Evalauate effective permissions requested.\n",
        "def evaluate_ad_acl_perms():\n",
        "    ad_perms = defaultdict(int)\n",
        "    for p_info in config[\"datalakeProperties\"]:\n",
        "        p_info[\"lastUpdatedDatalake\"] = current_ts\n",
        "        partition = f\"{p_info['year']}/{p_info['month']}\"\n",
        "        partition_path = f\"{data_path_prefix}{partition}/\"\n",
        "        \n",
        "        for perm in p_info[\"aclPermissions\"]:\n",
        "            for grp in perm[\"groups\"]:\n",
        "                ad_set.add(grp)\n",
        "                a_type = perm[\"type\"]\n",
        "                if a_type == \"read\":\n",
        "                    ad_perms[(partition_path, grp)] += 4\n",
        "                elif a_type == \"write\":\n",
        "                    ad_perms[(partition_path, grp)] += 2\n",
        "                elif a_type == \"execute\":\n",
        "                    ad_perms[(partition_path, grp)] += 1\n",
        "                else:\n",
        "                    config_check_errors.append(f\"Invalid acl type value :'{a_type}' specifed for partition '{partition}' . Acl Type must be one among ['read', 'write', 'execute']\")\n",
        "    return ad_perms\n",
        "\n",
        "\n",
        "# Assumption: ACL Grant statements are run after data copy step is complete. Otherwise we will run into `The specified path does not exist` errors.\n",
        "# We are granting \"r-x\" on all folders (recusively from root) so that anyone can \"read and list the *Folders*\" . \n",
        "# We follow this statement with another recursive update this time including the \"datafiles\" path which will overwrite any extra permissions granted in the previous step.\n",
        "# Otherwise, unless we create the parent folders seperately and grant default permissions, we will not have access to parent folders and avoid access denied errors.\n",
        "def update_parent_folder_acls(ad_perms, ad_map):\n",
        "    parent_dirs = set()\n",
        "    for path, ad in ad_perms:\n",
        "        parent_dirs.add((path.lstrip('/').split('/',1)[0], ad))\n",
        "\n",
        "    for parentdir, ad in parent_dirs:\n",
        "        if ad in ad_map:\n",
        "            update_permission_recursively(service_client, data_container, parentdir, 0, 'group', ad_map[ad], 'r-x')\n",
        "        else:\n",
        "            config_check_errors.append(f\"{ad} is not a valid ActiveDirectory Group.\")\n",
        "\n",
        "\n",
        "def update_ad_acls(ad_perms, ad_map):\n",
        "    for k, v in ad_perms.items():\n",
        "        (part_path, ad_name) = k\n",
        "        if ad_name in ad_map:\n",
        "            update_permission_recursively(service_client, data_container, part_path, 0, 'group', ad_map[ad_name], permissions_map[ad_perms[k]])\n",
        "        else:\n",
        "            config_check_errors.append(f\"{ad_name} is not a valid ActiveDirectory Group.\")\n",
        "\n",
        "\n",
        "def check_config_errors():    \n",
        "    if len(config_check_errors) > 0:\n",
        "        raise ValueError(f\"Config file check failed. Errors are: {config_check_errors}\")\n",
        "    print(\"ACL Statements generation and Active Directory Check Complete.\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from notebookutils import mssparkutils"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Parameters to Enable connection to Storage"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read from pipeline\n",
        "storage_acct = \"syngudast114\"\n",
        "\n",
        "# Mostly constant\n",
        "permissions_map = {0: \"---\", 1: \"--x\", 2: \"-w-\", 3: \"-wx\", 4: \"r--\", 5: \"r-x\", 6: \"rw-\", 7: \"rwx\" }\n",
        "current_ts = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "keyvault_ls_name = \"Ls_NYCTaxi_KeyVault\"\n",
        "storage_key_name = \"datalakeKey\"\n",
        "data_container = \"datalake\"\n",
        "config_container = \"config\"\n",
        "config_file_name = \"datalake_config.json\"\n",
        "config_file_path = \"/\"\n",
        "data_path_prefix = \"\"\n",
        "\n",
        "# Secrets based values\n",
        "storage_access_key = mssparkutils.credentials.getSecretWithLS(keyvault_ls_name, storage_key_name)\n",
        "storage_acct_connection = f\"DefaultEndpointsProtocol=https;AccountName={storage_acct};AccountKey={storage_access_key};EndpointSuffix=core.windows.net\"\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Config from ADLS"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service_client = initialize_storage_account(storage_acct, storage_access_key)\n",
        "config = download_file_from_directory(service_client, config_container, config_file_path, config_file_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate overall ACLs needed in short form"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_check_errors = []\n",
        "ad_set = set()\n",
        "ad_perms = evaluate_ad_acl_perms()\n",
        "print(ad_perms)\n",
        "print(ad_set)\n",
        "# Gather list of ADs and their ids - ids are needed for granting ACLs\n",
        "# One Option - Requires APP ID - https://github.com/AzureAD/microsoft-authentication-library-for-python\n",
        "# for now reading from Vault\n",
        "try:\n",
        "    ad_map = { ad: mssparkutils.credentials.getSecretWithLS(keyvault_ls_name, ad) for ad in ad_set}\n",
        "except Exceptiion as e:\n",
        "    config_check_errors.append(f\"No ID returned for given Active directory name. error is {e}\")\n",
        "    \n",
        "print(ad_map)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grant ACLs Recursively"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "update_parent_folder_acls(ad_perms, ad_map)\n",
        "update_ad_acls(ad_perms, ad_map)\n",
        "check_config_errors()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update Config file with latest run time\n",
        "- rename with timestamp and create a new file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mssparkutils.fs.help()\n",
        "source_config = f\"abfss://{config_container}@{storage_acct}.dfs.core.windows.net{config_file_path}{config_file_name}\"\n",
        "backup_config = f\"abfss://{config_container}@{storage_acct}.dfs.core.windows.net{config_file_path}{config_file_name}_{current_ts}\"\n",
        "mssparkutils.fs.mv(source_config, backup_config, overwrite=True)\n",
        "mssparkutils.fs.put(source_config, json.dumps(config, indent=2, default=str), overwrite=True)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}