{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool1",
              "session_id": 13,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-29T06:44:29.1893377Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-29T06:44:29.2939124Z",
              "execution_finish_time": "2021-11-29T06:44:29.2940207Z"
            },
            "text/plain": "StatementMeta(SparkPool1, 13, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "base_path = 'source'\r\n",
        "filename = 'country_list.csv'\r\n",
        "storage_account_name = 'synapsesharingstorage'\r\n",
        "container_name = 'root'\r\n",
        "database = 'default'\r\n",
        "target_table = '{\"name\":\"country_list\",\"path\":\"/data\"}'\r\n",
        "module_name = 'md5'\r\n",
        "module_config = '{}'\r\n",
        "pipeline_run_id = '123'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool1",
              "session_id": 13,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-29T06:44:29.4639432Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-29T06:44:29.5672342Z",
              "execution_finish_time": "2021-11-29T06:45:15.0386624Z"
            },
            "text/plain": "StatementMeta(SparkPool1, 13, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExitValue: country_list.csv"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import importlib\r\n",
        "import json\r\n",
        "import logging \r\n",
        "\r\n",
        "logger = sc._jvm.org.apache.log4j.LogManager.getLogger(\"edm-log4j-logger\") \r\n",
        "\r\n",
        "tranformer_module = importlib.import_module(module_name)\r\n",
        "\r\n",
        "# log input parameters\r\n",
        "logger.info(f\"Pipeline Run ID : {pipeline_run_id}\")\r\n",
        "logger.debug(f\"Base path parameter value {base_path}\")\r\n",
        "logger.debug(f\"File name parameter value {filename}\")\r\n",
        "logger.debug(f\"Storage account name parameter value {storage_account_name}\")\r\n",
        "logger.debug(f\"Container Name parameter value {container_name}\")\r\n",
        "logger.debug(f\"Database name parameter value {database}\")\r\n",
        "logger.debug(f\"Target table parameter value {target_table}\")\r\n",
        "logger.debug(f\"Module name parameter value {module_name}\")\r\n",
        "logger.debug(f\"Target table parameter value {module_config}\")\r\n",
        "\r\n",
        "target_table_config = json.loads(target_table)\r\n",
        "tranformation_config = json.loads(module_config)\r\n",
        "# Parse\r\n",
        "destination_table = target_table_config[\"name\"]\r\n",
        "external_table_path = target_table_config[\"path\"]\r\n",
        "\r\n",
        "\r\n",
        "# Read data from source, filename empty means data will be read by transformers internally directly from tables.  # noqa: E501\r\n",
        "if (filename):\r\n",
        "    adlsUri = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\"  # noqa: E501\r\n",
        "    full_path = adlsUri + base_path\r\n",
        "    # MVO-0 Works with Parquet files ONLY\r\n",
        "    sourcedf = spark.read.option(\"header\",\"true\").csv(full_path + \"/\" + filename)\r\n",
        "else:\r\n",
        "    sourcedf = []\r\n",
        "\r\n",
        "# Tranform the data\r\n",
        "finaldf = tranformer_module.transform(spark, sourcedf, tranformation_config)\r\n",
        "# Check for table existence\r\n",
        "spark_tables = spark.catalog.listTables()\r\n",
        "is_table_exists = any(x.name == destination_table for x in spark_tables)\r\n",
        "if is_table_exists:\r\n",
        "    finaldf.write.mode(\"append\").insertInto(database + '.' + destination_table)\r\n",
        "    logger.info(f\"Data appended to table: {destination_table} after {module_name} transformation\")\r\n",
        "else:\r\n",
        "    finaldf.write.mode(\"overwrite\").option(\"path\",external_table_path).saveAsTable(database + '.' + destination_table);\r\n",
        "    logger.info(f\"New table {destination_table} created after {module_name} transformation!\")\r\n",
        "\r\n",
        "mssparkutils.notebook.exit(filename)"
      ]
    }
  ]
}