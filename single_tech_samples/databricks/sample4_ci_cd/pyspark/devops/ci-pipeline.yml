# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- main
- single-tech/databricks-ops
- single-tech/databricks-ops-thurstonchen

pool:
  vmImage: ubuntu-latest

stages:
- stage: UT
  displayName: Unit Test
  jobs:
  - job: RunSparkUT
    pool:
      vmImage: ubuntu-latest
    displayName: Run Spark Unit Tests

    steps:
    - script: |
        wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz
        tar -xf ./spark-3.1.1-bin-hadoop2.7.tgz
        export SPARK_HOME=$(Build.Repository.LocalPath)/spark-3.1.1-bin-hadoop2.7
        
        mkdir -p Hadoop/bin
        wget -P ./Hadoop/bin -q https://github.com/4ttty/winutils/raw/master/hadoop-2.7.1/bin/winutils.exe 
        export HADOOP_HOME=$(Build.Repository.LocalPath)/Hadoop
        
        deploymentJsonPath=$(find ./ -path "*/conf/deployment.json")
        testsPath=$(sed "s|deployment.json|../tests|g" <<< $deploymentJsonPath)
        projectRootPath=$(sed "s|deployment.json|../|g" <<< $deploymentJsonPath)
        
        export PYTHONPATH=$PYTHONPATH:$projectRootPath
        export PYSPARK_PYTHON=python
        python -m pip install --upgrade pip
        pip install pytest
        pip install pytest-cov
        pip install pyspark
        python -m pytest $testsPath --doctest-modules --ignore $testsPath/integration/ --junitxml=$projectRootPath/junit/test-results.xml --cov=. --cov-report=xml --cov-report=html
      displayName: Run UT with pytest

    - task: PublishTestResults@2
      displayName: Publish UT results
      inputs:
        testResultsFormat: 'JUnit'
        testResultsFiles: '**/test-*.xml'
        failTaskOnFailedTests: true
        testRunTitle: 'Publish UT results (on Python $(python.version))'

    - task: PublishCodeCoverageResults@1
      displayName: Publish code coverage results
      inputs:
        codeCoverageTool: 'Cobertura'
        summaryFileLocation: '**/coverage.xml'
        reportDirectory: '**/htmlcov'

- stage: PublishArtifact
  condition: succeeded('UT')
  displayName: Publish Artifacts
  jobs:
  - job: PublishArtifactJob
    displayName: Publish artifacts
    steps:
    - task: PublishPipelineArtifact@1
      displayName: Publish artifacts
      inputs:
        targetPath: '$(Pipeline.Workspace)'
        artifact: 'mlProjectSampleArtifactTest'
        publishLocation: 'pipeline'
